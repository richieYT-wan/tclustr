{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46897fbc-4baa-45c3-bcdb-f761c7994346",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACoAAAAuCAYAAABeUotNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACXElEQVR4nO2WQUrrUBSG/4ZCoW1iSLVIkdBBAx1kCQEX0ImgoMsQHAbp3IkrcJCZIELpuIXuQ0vNSKOI3CSVSMXj5DWQUmrPI6m8Rz7I4B6Sky83Nzd/gYgI/wDSbwusSy6aNrlo2uSiaZOLps1fi768vKDVamE0GqWoswJi4nkeWZZFkiQRADo8PKTZbMZtw4Yt2m63qVwuk+M4BICazSZdXFxk4ZagQLR+KLm/v4dhGHBdF7quo1Ao4Pz8HI7jwHXdxLm+78P3/Xj89fWF6XQKWZbRaDQgScxVx3mqXq9HmqbFYwB0dXVFAOjt7S1x7v7+PgFYeozHY/aMFjkPFQQBKpVKolYqlQAAYRhCVdW43u/3EzMqhIBpmgAATdN4swmAJVqpVPD+/p6ofXx8AABkWU7UFUWBoiiJ8Rz2awdzezJNE6+vr/A8L649PDxgb28PW1tb7Juz4K4Vy7Lo5OSEfN+Pv/put/vjdUKIeI0KIdhrlC369PRER0dHVKvVaGdnh87Ozujz8/PH66IoItu2ybZtiqKILcrann6T//9fv2ly0bTZiOjz8zMODg6gqio0TYOqqhgOh6weGxE9Pj5GtVrF7e0tZFmGEAI3Nze8JuwNjcnd3R0BoMvLS9J1na6vrwkA1et1Vp/MReeJ6/HxMQ7Y+POHWkxcq8j81c8T1+7uLorFZAYKw3DtPpmLLktccxYT1yoyF12WuABge3ublbgyFzUMA5Zl4fT0FEEQYDKZAAA6nQ6vUUbfUILFxAWABoMBq0eentImF02bXDRtctG0yUXTJhdNm2/tMzRBDgOZWQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1x1 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "f,a = plt.subplots(1,1,figsize=(1e-2, 1e-2))\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "sns.set_style('darkgrid')\n",
    "import os,sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Load models together\n",
    "from src.torch_utils import load_model_full\n",
    "from src.utils import get_class_initcode_keys\n",
    "from torch.utils.data import SequentialSampler\n",
    "from src.datasets import *\n",
    "from src.models import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "from src.utils import mkdirs, convert_path, pkl_dump, pkl_load, add_median_labels, get_palette\n",
    "from src.data_processing import BL62_VALUES, BL62FREQ_VALUES, HLAS, AA_KEYS\n",
    "from src.utils import pkl_load, pkl_dump, get_palette\n",
    "from src.sim_utils import make_dist_matrix\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from src.torch_utils import save_checkpoint, load_checkpoint\n",
    "from src.train_eval import predict_model, train_eval_loops\n",
    "from src.models import FullTCRVAE, TwoStageVAECLF\n",
    "from src.metrics import get_metrics, reconstruction_accuracy, compute_cosine_distance\n",
    "from src.datasets import TCRSpecificDataset, FullTCRDataset\n",
    "import torch\n",
    "import glob\n",
    "from torch import optim\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "mpl.rcParams['figure.dpi'] = 180\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa1723da-73a4-43b3-944e-bbc98c93c2eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# For reference, NetTCR architecture : \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define network architecture \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(in_channels=24, out_channels=16, kernel_size=1, padding='same')\n",
    "        self.c3 = nn.Conv1d(in_channels=24, out_channels=16, kernel_size=3, padding='same')\n",
    "        self.c5 = nn.Conv1d(in_channels=24, out_channels=16, kernel_size=5, padding='same')\n",
    "        self.c7 = nn.Conv1d(in_channels=24, out_channels=16, kernel_size=7, padding='same')\n",
    "        self.c9 = nn.Conv1d(in_channels=24, out_channels=16, kernel_size=9, padding='same')\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = torch.max(self.activation(self.c1(x)), 2)[0]\n",
    "        c3 = torch.max(self.activation(self.c3(x)), 2)[0]\n",
    "        c5 = torch.max(self.activation(self.c5(x)), 2)[0]\n",
    "        c7 = torch.max(self.activation(self.c7(x)), 2)[0]\n",
    "        c9 = torch.max(self.activation(self.c9(x)), 2)[0]\n",
    "\n",
    "        out = torch.cat((c1, c3, c5, c7, c9), 1)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class NetTCR_CDR3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvBlock()\n",
    "\n",
    "        self.linear = nn.Linear(in_features=80*3, out_features=32)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, pep, a3, b3):\n",
    "        # Transpose tensors\n",
    "        pep = torch.permute(pep, (0, 2, 1))\n",
    "        a3 = torch.permute(a3, (0, 2, 1))\n",
    "        b3 = torch.permute(b3, (0, 2, 1))\n",
    "        \n",
    "        pep_cnn = self.cnn(pep)\n",
    "        a3_cnn = self.cnn(a3)\n",
    "        b3_cnn = self.cnn(b3)\n",
    "\n",
    "        cat = torch.cat((pep_cnn, a3_cnn, b3_cnn),1)\n",
    "\n",
    "        hid = self.activation(self.linear(cat))\n",
    "        out = self.activation(self.out(hid))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class NetTCR_CDR3_singlechain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvBlock()\n",
    "\n",
    "        self.linear = nn.Linear(in_features=80*2, out_features=32)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, pep, cdr):\n",
    "        # Transpose tensors\n",
    "        pep = torch.permute(pep, (0, 2, 1))\n",
    "        cdr = torch.permute(cdr, (0, 2, 1))\n",
    "        \n",
    "        pep_cnn = self.cnn(pep)\n",
    "        cdr_cnn = self.cnn(cdr)\n",
    "\n",
    "        cat = torch.cat((pep_cnn, cdr_cnn),1)\n",
    "\n",
    "        hid = self.activation(self.linear(cat))\n",
    "        out = self.activation(self.out(hid))\n",
    "\n",
    "        return out\n",
    "    \n",
    "class NetTCR_CDR123(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvBlock()\n",
    "\n",
    "        self.linear = nn.Linear(in_features=80*7, out_features=32)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, pep, a1, a2, a3, b1, b2, b3):\n",
    "        # Transpose tensors\n",
    "        pep = torch.permute(pep, (0, 2, 1))\n",
    "        a1 = torch.permute(a1, (0, 2, 1))\n",
    "        a2 = torch.permute(a2, (0, 2, 1))\n",
    "        a3 = torch.permute(a3, (0, 2, 1))\n",
    "        b1 = torch.permute(b1, (0, 2, 1))\n",
    "        b2 = torch.permute(b2, (0, 2, 1))\n",
    "        b3 = torch.permute(b3, (0, 2, 1))\n",
    "        \n",
    "        pep_cnn = self.cnn(pep)\n",
    "        a1_cnn = self.cnn(a1)\n",
    "        a2_cnn = self.cnn(a2)\n",
    "        a3_cnn = self.cnn(a3)\n",
    "        b1_cnn = self.cnn(b1)\n",
    "        b2_cnn = self.cnn(b2)\n",
    "        b3_cnn = self.cnn(b3)\n",
    "\n",
    "        cat = torch.cat((pep_cnn, a1_cnn, a2_cnn, a3_cnn, b1_cnn, b2_cnn, b3_cnn), 1)\n",
    "\n",
    "        hid = self.activation(self.linear(cat))\n",
    "        out = self.activation(self.out(hid))\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class NetTCR_CDR123_singlechain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvBlock()\n",
    "\n",
    "        self.linear = nn.Linear(in_features=80*4, out_features=32)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, pep, cdr1, cdr2, cdr3):\n",
    "        # Transpose tensors\n",
    "        pep = torch.permute(pep, (0, 2, 1))\n",
    "        cdr1 = torch.permute(cdr1, (0, 2, 1))\n",
    "        cdr2 = torch.permute(cdr2, (0, 2, 1))\n",
    "        cdr3 = torch.permute(cdr3, (0, 2, 1))\n",
    "        \n",
    "        pep_cnn = self.cnn(pep)\n",
    "        cdr1_cnn = self.cnn(cdr1)\n",
    "        cdr2_cnn = self.cnn(cdr2)\n",
    "        cdr3_cnn = self.cnn(cdr3)\n",
    "\n",
    "        cat = torch.cat((pep_cnn, cdr1_cnn, cdr2_cnn, cdr3_cnn), 1)\n",
    "\n",
    "        hid = self.activation(self.linear(cat))\n",
    "        out = self.activation(self.out(hid))\n",
    "\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd85c9b3-b389-425a-bf44-dd37853f855e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7339.000000\n",
       "mean        5.988146\n",
       "std         0.571654\n",
       "min         5.000000\n",
       "25%         6.000000\n",
       "50%         6.000000\n",
       "75%         6.000000\n",
       "max         7.000000\n",
       "Name: A1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7339.000000\n",
       "mean        6.675705\n",
       "std         1.010586\n",
       "min         4.000000\n",
       "25%         6.000000\n",
       "50%         7.000000\n",
       "75%         7.000000\n",
       "max         8.000000\n",
       "Name: A2, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7339.000000\n",
       "mean       11.434528\n",
       "std         2.100023\n",
       "min         3.000000\n",
       "25%        10.000000\n",
       "50%        11.000000\n",
       "75%        13.000000\n",
       "max        22.000000\n",
       "Name: A3, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7339.000000\n",
       "mean        5.080256\n",
       "std         0.271708\n",
       "min         5.000000\n",
       "25%         5.000000\n",
       "50%         5.000000\n",
       "75%         5.000000\n",
       "max         6.000000\n",
       "Name: B1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7339.000000\n",
       "mean        6.089113\n",
       "std         0.322610\n",
       "min         5.000000\n",
       "25%         6.000000\n",
       "50%         6.000000\n",
       "75%         6.000000\n",
       "max         7.000000\n",
       "Name: B2, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    7339.000000\n",
       "mean       12.426080\n",
       "std         1.761179\n",
       "min         5.000000\n",
       "25%        11.000000\n",
       "50%        12.000000\n",
       "75%        13.000000\n",
       "max        23.000000\n",
       "Name: B3, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/filtered/240418_nettcr_expanded_20binders_17pep_POSONLY.csv')\n",
    "cs = ['A1','A2','A3','B1','B2','B3']\n",
    "for c in cs:\n",
    "    l = df[c].apply(len)\n",
    "    print(c)\n",
    "    display(l.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b59d5-0350-442c-9a9a-d5c3d87eafad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# triplet error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f04a88c3-3f38-4f16-a609-885225c0b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "60539bf1-8fca-4ad9-931f-4a86826b79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pkl_load('/Users/riwa/Documents/code/total_dis.pkl')\n",
    "pos_dis = pkl_load('/Users/riwa/Documents/code/pos_dis.pkl')\n",
    "pos_mask = pkl_load('/Users/riwa/Documents/code/pos_mask.pkl')\n",
    "neg_dis = pkl_load('/Users/riwa/Documents/code/neg_dis.pkl')\n",
    "neg_mask = pkl_load('/Users/riwa/Documents/code/neg_mask.pkl')\n",
    "labels = pkl_load('/Users/riwa/Documents/code/labels.pkl')\n",
    "loss = pkl_load('/Users/riwa/Documents/code/loss.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "eb8acbe5-36d2-448a-a36b-5a9e5625133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_mask = torch.ones_like(pos_mask).fill_diagonal_(0)\n",
    "mask_loss = torch.mul(loss, diag_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa86d60-0e5f-4acc-8abd-bcb04cba46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/riwa/Documents/code/pos_dis.pkl', 'wb') as f:\n",
    "    pickle.dump(positive_distances, f)\n",
    "\n",
    "with open('/Users/riwa/Documents/code/neg_dis.pkl', 'wb') as f:\n",
    "    pickle.dump(negative_distances, f)\n",
    "    \n",
    "with open('/Users/riwa/Documents/code/pos_mask.pkl', 'wb') as f:\n",
    "    pickle.dump(mask_positive, f)\n",
    "    \n",
    "with open('/Users/riwa/Documents/code/neg_mask.pkl', 'wb') as f:\n",
    "    pickle.dump(mask_negative, f)\n",
    "\n",
    "with open('/Users/riwa/Documents/code/labels.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)\n",
    "\n",
    "with open('/Users/riwa/Documents/code/loss.pkl', 'wb') as f:\n",
    "    pickle.dump(loss, f)\n",
    "\n",
    "with open('/Users/riwa/Documents/code/total_dis.pkl', 'wb') as f:\n",
    "    pickle.dump(pairwise_distances, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "012d8445-ba0b-4ecd-b3d7-0944671e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = F.relu(pos - neg + 0.1)\n",
    "def get_vector(loss, pos, neg, i, margin=0.1):\n",
    "    where = torch.where(loss[i]>0)[0]\n",
    "    pp = pos[i][where]\n",
    "    nn = neg[i][where]\n",
    "    ll = loss[i][where]\n",
    "    print('pos\\n\\t', pp)\n",
    "    print('neg\\n\\t', nn)\n",
    "    print('loss\\n\\t', ll)\n",
    "    print('minus\\n\\t', pp-nn+margin)\n",
    "    print('final loss\\n\\t', F.relu(pp-nn+margin))\n",
    "    print('where\\n\\t', where)\n",
    "    return pp, nn, ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "77a59705-419d-4fbe-a6a2-e34fd3304d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels==11).float().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8df325e1-9e41-4e5b-8e63-f9bf02d2b8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "\t tensor([0.4451, 1.2796, 0.0000, 1.3656, 0.0000, 0.8558],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "neg\n",
      "\t tensor([0.0000, 0.0000, 0.0897, 0.0000, 0.0905, 0.0000],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "loss\n",
      "\t tensor([0.5451, 1.3796, 0.0103, 1.4656, 0.0095, 0.9558],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "minus\n",
      "\t tensor([1.0451, 1.8796, 0.5103, 1.9656, 0.5095, 1.4558],\n",
      "       grad_fn=<AddBackward0>)\n",
      "final loss\n",
      "\t tensor([1.0451, 1.8796, 0.5103, 1.9656, 0.5095, 1.4558],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "where\n",
      "\t tensor([ 68, 273, 338, 392, 422, 510])\n"
     ]
    }
   ],
   "source": [
    "pp,nn,ll = get_vector(mask_loss, pos_dis, neg_dis, 251, margin = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "84dcb287-441f-4a8f-94bf-374a2838ed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8903, 1.7248, 0.3555, 1.8108, 0.3547, 1.3010],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([0.8903, 1.7248, 0.3555, 1.8108, 0.3547, 1.3010],\n",
       "        grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin=0.4452\n",
    "pp-nn+margin, F.relu(pp - nn + margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d59dc254-810a-43ba-8367-4986015b566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 0., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 0.]]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_matrix = labels.view(-1,1)\n",
    "positive_mask = (class_matrix == class_matrix.t()).float()\n",
    "negative_mask = (class_matrix != class_matrix.t()).float()\n",
    "\n",
    "# Make diagonal elements zero in positive mask since an anchor shouldn't select itself\n",
    "positive_mask.fill_diagonal_(0)\n",
    "positive_mask, negative_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "94aaf810-03bb-4120-819c-1d90cfcc597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_distances = distances * positive_mask\n",
    "hardest_positive_distances = positive_distances.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7afb9917-d70c-4991-969b-d95c206d13e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1594, 0.9830, 1.0951], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_distances[0, [129,245,341]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b4ec3de5-79fd-4840-b539-4f7c098e63a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0,   0,   0,  ..., 470, 470, 470]),\n",
       " tensor([129, 245, 341,  ..., 354, 379, 428]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(positive_distances>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7dbdaf71-c124-4bf1-a20b-ecf047020331",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_distances = distances * negative_mask\n",
    "# Replace zero with large number where negative_mask is zero\n",
    "max_distance = distances.max().item() + 1\n",
    "negative_distances += max_distance * (1 - negative_mask)\n",
    "hardest_negative_distances = negative_distances.min(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "19fec599-7f4e-442d-9e99-d43c22426be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = hardest_positive_distances - hardest_negative_distances + 0.1\n",
    "triplet_loss = F.relu(triplet_loss)\n",
    "valid_triplets = (positive_mask.sum(dim=1) > 0) & (negative_mask.sum(dim=1) > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5075e95c-6468-42e4-b23f-ba11978849d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_mask[253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2452d51e-6d45-4a78-b2d1-91b48748655f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([253]),)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(valid_triplets==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f6830bb3-f42d-4bae-b814-97d195313e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5964, 1.1194, 1.2056,  ..., 0.5992, 0.7681, 1.0439],\n",
       "        [1.1194, 2.5964, 1.2990,  ..., 1.3319, 1.0508, 0.8547],\n",
       "        [1.2056, 1.2990, 2.5964,  ..., 1.1263, 1.1149, 0.8849],\n",
       "        ...,\n",
       "        [0.5992, 1.3319, 1.1263,  ..., 2.5964, 0.9528, 0.9223],\n",
       "        [0.7681, 1.0508, 1.1149,  ..., 0.9528, 2.5964, 1.1386],\n",
       "        [1.0439, 0.8547, 0.8849,  ..., 0.9223, 1.1386, 2.5964]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d8c45251-6fae-400c-a4dd-fa1852d61deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5963759422302246"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249af312-e989-4eef-a1fc-1f96fd9f967d",
   "metadata": {},
   "source": [
    "# tests architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d54a6e-2a05-41a5-940e-92b0f17205ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5k datapoints, len 50, 20 aa dim\n",
    "x = torch.rand(1000, 50, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9162f6b-917c-4590-9b6e-42da0da1bdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 20, 50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b79d54-4e40-42ec-8611-a96e0c0a8cb0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10, 46])\n",
      "torch.Size([1000, 10, 22])\n",
      "torch.Size([1000, 20, 18])\n"
     ]
    }
   ],
   "source": [
    "# Taking 5 n_hidden\n",
    "conv1 = nn.Conv1d(20, 10, kernel_size=5, stride=1)\n",
    "mp = nn.MaxPool1d(kernel_size=4, stride=2)\n",
    "conv2 = nn.Conv1d(10, 20, kernel_size=5, stride=1)\n",
    "\n",
    "z = conv1(x.permute(0,2,1))\n",
    "print(z.shape)\n",
    "z = mp(z)\n",
    "print(z.shape)\n",
    "z = conv2(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6410eb4-6a47-47c5-a012-2dc622905d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim, latent_dim, kernel_size):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a19fcaf-413a-43e4-90eb-0fade5665500",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 60, 20]),\n",
       " torch.Size([1000, 66, 20]),\n",
       " torch.Size([1000, 64]),\n",
       " torch.Size([1000, 64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, L, num_features, hidden_dims, latent_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(num_features, hidden_dims, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            nn.Conv1d(hidden_dims, hidden_dims * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(hidden_dims * 2 * (L // 4), latent_dim)  # Adjust size accordingly\n",
    "        self.fc_var = nn.Linear(hidden_dims * 2 * (L // 4), latent_dim)  # Adjust size accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Adjusting dimensions to (N, C, L)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, L, num_features, hidden_dims, latent_dim):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dims * 2 * (L // 4))  # Adjust size accordingly\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dims * 2, hidden_dims, kernel_size=6, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(hidden_dims, num_features, kernel_size=6, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, self.hidden_dims * 2, self.L // 4)  # Reshape to (N, C, L) for ConvTranspose\n",
    "        x = self.conv_transpose_layers(x)\n",
    "        x = x.permute(0, 2, 1)  # Reshaping back to (N, L, C)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, L=60, num_features=20, hidden_dims=128, latent_dim=64):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = CNNEncoder(L, num_features, hidden_dims, latent_dim)\n",
    "        self.decoder = CNNDecoder(L, num_features, hidden_dims, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, log_var\n",
    "\n",
    "L = 60\n",
    "aa_dim = 20\n",
    "x = torch.rand(1000, L, aa_dim)\n",
    "v = VAE(L, aa_dim, 128, 64)\n",
    "x_hat, mu, logvar = v(x)\n",
    "\n",
    "x.shape, x_hat.shape, mu.shape, logvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfcfe426-ad80-46b9-9bb6-31bc5288f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After in conv torch.Size([100, 128, 18])\n",
      "After out_fcz torch.Size([100, 128, 18])\n",
      "after out_conv1 torch.Size([100, 80, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 73, 20]), torch.Size([100, 80, 20]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_dim, L = 20, 7+8+22+6+7+23\n",
    "\n",
    "ks_in = 6\n",
    "stride_in = 2\n",
    "pad_in = 2\n",
    "\n",
    "hidden = 64\n",
    "latent = 32\n",
    "\n",
    "ks_out = 3\n",
    "stride_out = 5\n",
    "pad_out = 4\n",
    "\n",
    "in_conv = nn.Sequential(nn.Conv1d(aa_dim, hidden, kernel_size=ks_in, stride=stride_in, padding=pad_in),\n",
    "                        nn.Conv1d(hidden, hidden * 2, ks_in, stride_in, pad_in))\n",
    "x = torch.randn([100, L, aa_dim])\n",
    "z = torch.randn([100, latent])\n",
    "\n",
    "out_fc = nn.Linear(latent, hidden * 2 * (L // 4))\n",
    "out_conv1 = nn.ConvTranspose1d(hidden * 2, hidden, kernel_size=ks_out, stride=stride_out, padding=pad_out)\n",
    "out_conv2 = nn.ConvTranspose1d(hidden, aa_dim, kernel_size=ks_out, stride=1, padding=1)\n",
    "\n",
    "after_in = in_conv(x.permute(0,2,1))\n",
    "print('After in conv', after_in.shape)\n",
    "\n",
    "after_out = out_fc(z)\n",
    "after_out = after_out.view(-1, hidden * 2, L //4)\n",
    "print('After out_fcz', after_out.shape)\n",
    "\n",
    "after_out = out_conv1(after_out)\n",
    "print('after out_conv1', after_out.permute(0, 2, 1).shape)\n",
    "after_out = out_conv2(after_out).permute(0, 2, 1)\n",
    "\n",
    "x.shape, after_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "710ca303-3fa7-4ee4-a988-c4b7783415d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 12\n",
    "((input_size-1)*stride_out)+ks_out-2*pad_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f707e22-7c49-42f9-9f52-990eff550d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1536])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(latent, hidden * 2 * (50//4))(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "434809e4-d473-41af-af82-394476e13a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1536])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_in.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10278b-5201-4b95-89af-49289eb671cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae6301cd-c6f9-4664-8af1-2baf5b80acd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (1959391978.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[63], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, L=50, num_features=20, hidden_dims, latent_dim):\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, L, num_features, hidden_dims, latent_dim):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(num_features, hidden_dims, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dims, hidden_dims * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(hidden_dims * 2 * (L // 4), latent_dim)  # Adjust size accordingly\n",
    "        self.fc_var = nn.Linear(hidden_dims * 2 * (L // 4), latent_dim)  # Adjust size accordingly\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Adjusting dimensions to (N, C, L)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "        return x\n",
    "        \n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, L, num_features, hidden_dims, latent_dim):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dims * 2 * (L // 4))  # Adjust size accordingly\n",
    "        self.conv_transpose_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(hidden_dims * 2, hidden_dims, kernel_size=6, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(hidden_dims, num_features, kernel_size=6, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, self.hidden_dims * 2, self.L // 4)  # Reshape to (N, C, L) for ConvTranspose\n",
    "        x = self.conv_transpose_layers(x)\n",
    "        x = x.permute(0, 2, 1)  # Reshaping back to (N, L, C)\n",
    "\n",
    "\n",
    "x = torch.rand(1000, 50, 20)\n",
    "v = VAE(50, 20, 128, 64)\n",
    "x_hat, mu, logvar = v(x)\n",
    "\n",
    "x.shape, x_hat.shape, mu.shape, logvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d14af093-3512-40ff-a4b0-249250f126aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4, 256])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_in.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5200e363-7a32-42d2-b333-549bccffa926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L out after in, after flat\n",
      "\t 18.0 4 \n",
      "\t torch.Size([100, 256, 4]) torch.Size([100, 1024])\n",
      "z shape\n",
      "\t torch.Size([100, 128])\n",
      "reconv reshaped\n",
      "\t torch.Size([100, 256, 4])\n",
      "transOut, out, x\n",
      "\t 73 \n",
      "\t torch.Size([100, 73, 20]) torch.Size([100, 73, 20])\n"
     ]
    }
   ],
   "source": [
    "L = 7+8+22+6+7+23\n",
    "aa = 20\n",
    "latent = 128\n",
    "hidden = 128\n",
    "\n",
    "x = torch.rand(100, L, aa)\n",
    "\n",
    "kernel_in = 9\n",
    "stride_in = 4\n",
    "pad_in = 2\n",
    "in_conv = nn.Sequential(nn.Conv1d(aa, hidden, kernel_in, stride_in, pad_in), # no activation here just for the dimensions\n",
    "                        nn.Conv1d(hidden, hidden*2, kernel_in, stride_in, pad_in))\n",
    "# simplified with dilation term==1\n",
    "L_int = 1+((L + 2*pad_in - (kernel_in))/stride_in)\n",
    "L_out = int(1+((L_int + 2*pad_in - (kernel_in))/stride_in))\n",
    "\n",
    "# Conv layer and flatten\n",
    "after_in = in_conv(x.permute(0,2,1))\n",
    "after_flat = after_in.flatten(start_dim=1)\n",
    "print('L out', 'after in, after flat\\n\\t', L_int, L_out, '\\n\\t', after_in.shape, after_flat.shape)\n",
    "# Z layer\n",
    "fc_mu = nn.Linear(L_out*2*hidden, latent)\n",
    "z = fc_mu(after_flat)\n",
    "print('z shape\\n\\t', z.shape)\n",
    "# fc out before convtranspose\n",
    "fc_out = nn.Linear(latent, L_out*2*hidden)\n",
    "reconv = fc_out(z)\n",
    "reconv_reshaped = reconv.view(-1, hidden*2, L_out)\n",
    "print('reconv reshaped\\n\\t',reconv_reshaped.shape)\n",
    "\n",
    "kernel_out_1 = 9\n",
    "kernel_out_2 = 9\n",
    "stride_out = 4\n",
    "pad_out = 2\n",
    "output_padding_1 = 1\n",
    "output_padding_2 = 0\n",
    "transconv = nn.Sequential(nn.ConvTranspose1d(hidden * 2, hidden, kernel_out_1, stride_out, pad_out, output_padding_1),\n",
    "                          nn.ConvTranspose1d(hidden, aa, kernel_out_2, stride_out, pad_out, output_padding_2))\n",
    "L_int_trans = stride_out * (L_out -1) + kernel_out_1 - (2*pad_out) + output_padding_1\n",
    "L_out_trans = stride_out * (L_int_trans -1) + kernel_out_2 - (2*pad_out) + output_padding_2\n",
    "\n",
    "after_out = transconv(reconv_reshaped).permute(0,2,1)\n",
    "print('transOut, out, x\\n\\t', L_out_trans, '\\n\\t', after_out.shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d71cd4-e920-4b48-bb27-323bd9e3a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(NetParent):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb5698-dd1c-459b-a79f-2f2c2c1eb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will have to re-figure out the dimensions but try this one ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173a7ee-6c94-4ffd-bb92-a520358eff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_int_trans = stride_out * (L_out -1) + kernel_out - (2*pad_out) + 0\n",
    "L_out_trans = stride_out * (L_int_trans -1) + kernel_out - (2*pad_out) + 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f597767-459b-4f74-b6d9-cfdaeface880",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7515847-c285-4059-9f32-33d8c6c798a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mock DF to have more data for training\n",
    "mock_df = pd.concat([pd.read_csv('../data/multimodal/240311_immrepnegs_tcrs.csv'),\n",
    "                     pd.read_csv('../data/multimodal/240311_nettcr_pairedAB_expanded_noswap.csv')])\n",
    "mock_df.reset_index(inplace=True, drop=True)\n",
    "rand_idx = torch.randperm(len(mock_df))\n",
    "train_idx = rand_idx[:int(.8*len(mock_df))]\n",
    "valid_idx = rand_idx[len(train_idx):]\n",
    "mock_df.loc[train_idx]['partition'] = 1\n",
    "mock_df.loc[valid_idx]['partition'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45431293-6bba-4676-97b6-91f6bc5045db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "22\n",
      "6\n",
      "7\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for q in ['A1','A2','A3','B1','B2','B3']:\n",
    "    print(mock_df[q].apply(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "591a4945-d0d7-4388-9845-798f83b1efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_df.to_csv('../data/filtered/240515_mock_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dcf20-7007-460d-b83f-4dfb2aadd698",
   "metadata": {},
   "source": [
    "# class tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "54be4a8b-ff01-4a1c-a8a2-861f39d2a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing import encoding_matrix_dict\n",
    "from src.models import NetParent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cd9aa7a1-fce3-4050-a1b8-fdec94934118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNEncoder(NetParent):\n",
    "    def __init__(self, kernel_size, stride, pad, max_len, features_dim,\n",
    "                 activation=nn.SELU(), hidden_dim=128, latent_dim=128, batchnorm=True):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.features_dim = features_dim\n",
    "        self.max_len = max_len\n",
    "        self.len_in = 1 + ((1 + ((max_len + 2 * pad - kernel_size) // stride) + 2 * pad - kernel_size) // stride)\n",
    "        # Neural network params\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        bn = nn.BatchNorm1d if batchnorm else nn.Identity\n",
    "        self.conv_layers = nn.Sequential(nn.Conv1d(features_dim, hidden_dim, kernel_size, stride, pad),\n",
    "                                         activation, bn(hidden_dim),\n",
    "                                         nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size, stride, pad),\n",
    "                                         activation, bn(hidden_dim*2))\n",
    "        self.fc_mu = nn.Linear(self.len_in * 2 * hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.len_in * 2 * hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        mu_logvar = self.conv_layers(x)\n",
    "        mu_logvar = mu_logvar.flatten(start_dim=1)\n",
    "        mu = self.fc_mu(mu_logvar)\n",
    "        logvar = self.fc_logvar(mu_logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class CNNDecoder(NetParent):\n",
    "    def __init__(self, kernel_size, stride, pad, len_in, features_dim, output_padding_1, output_padding_2,\n",
    "                 activation=nn.SELU(), hidden_dim=128, latent_dim=128, batchnorm=True):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.features_dim = features_dim\n",
    "        self.len_in = len_in  # Should be the len_out of CNNEncoder\n",
    "        self.len_out_trans = stride * (stride * (len_in - 1) + kernel_size - 2 * pad + output_padding_1 - 1) + kernel_size - 2 * pad + output_padding_2\n",
    "        # Neural network params\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        bn = nn.BatchNorm1d if batchnorm else nn.Identity\n",
    "        print(bn, hidden_dim, kernel_size, stride, pad, output_padding_1, output_padding_2, activation)\n",
    "        self.conv_transpose_layers = nn.Sequential(nn.ConvTranspose1d(hidden*2, hidden, kernel_out, stride_out, pad_out, output_padding_1), nn.SELU(), bn(hidden),\n",
    "                                                   nn.ConvTranspose1d(hidden, features_dim, kernel_out, stride_out, pad_out, output_padding_2), nn.SELU(), bn(features_dim))\n",
    "        self.fc_z = nn.Linear(latent_dim, len_in * 2 * hidden_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x_hat = self.fc_z(z)\n",
    "        x_hat = x_hat.view(-1, self.hidden_dim * 2, self.len_in)\n",
    "        x_hat = self.conv_transpose_layers(x_hat)\n",
    "        x_hat = x_hat.permute(0, 2, 1)\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "class CNNVAE(NetParent):\n",
    "    def __init__(self,\n",
    "                 kernel_size_in, stride_in, pad_in,\n",
    "                 kernel_size_trans, stride_trans, pad_trans, output_padding_trans_1, output_padding_trans_2,\n",
    "                 max_len_a1=7, max_len_a2=8, max_len_a3=22, max_len_b1=6, max_len_b2=7, max_len_b3=23,\n",
    "                 encoding='BL50LO', pad_scale=-20, aa_dim=20, add_positional_encoding=True,\n",
    "                 activation=nn.SELU(), hidden_dim=128, latent_dim=128, batchnorm=True):\n",
    "        super(CNNVAE, self).__init__()\n",
    "        # Init params that will be needed at some point for reconstruction\n",
    "        # Here, define aa_dim and pos_dim ; pos_dim is the dimension of a positional encoding.\n",
    "        # pos_dim should be given by how many sequences are used, i.e. how many max_len_x > 0\n",
    "        # But also use a flag `add_positional_encoding` to make it more explicit that it's active or not\n",
    "        max_len = sum([max_len_a1, max_len_a2, max_len_a3, max_len_b1, max_len_b2, max_len_b3])\n",
    "        pos_dim = sum([int(mlx) > 0 for mlx in\n",
    "                       [max_len_a1, max_len_a2, max_len_a3, max_len_b1, max_len_b2, max_len_b3]]) \\\n",
    "            if add_positional_encoding else 0\n",
    "        self.aa_dim = aa_dim\n",
    "        self.pos_dim = pos_dim\n",
    "        self.add_positional_encoding = add_positional_encoding\n",
    "        features_dim = aa_dim + pos_dim\n",
    "        input_dim = max_len * features_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.features_dim = features_dim\n",
    "        self.max_len = max_len\n",
    "        self.encoding = encoding\n",
    "        if pad_scale is None:\n",
    "            self.pad_scale = -20 if encoding in ['BL50LO', 'BL62LO'] else 0\n",
    "        else:\n",
    "            self.pad_scale = pad_scale\n",
    "\n",
    "        # TODO : Maybe should use -1 instead of -20 for pad values since that's the value for X in BL50LO ?\n",
    "        # Create the encoding matrix to recover / rebuild sequences\n",
    "        MATRIX_VALUES = deepcopy(encoding_matrix_dict[encoding])\n",
    "        MATRIX_VALUES['X'] = np.array([self.pad_scale]).repeat(20)\n",
    "        self.MATRIX_VALUES = torch.from_numpy(np.stack(list(MATRIX_VALUES.values()), axis=0))\n",
    "        # Neural network params\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batchnorm = batchnorm\n",
    "        self.len_in = 1 + ((1 + (\n",
    "                (max_len + 2 * pad_in - kernel_size_in) // stride_in) + 2 * pad_in - kernel_size_in) // stride_in)\n",
    "        self.len_out = stride_trans * (stride_trans * (\n",
    "                self.len_in - 1) + kernel_size_trans - 2 * pad_trans + output_padding_trans_1 - 1) + kernel_size_trans - 2 * pad_trans + output_padding_trans_2\n",
    "\n",
    "        self.encoder = CNNEncoder(kernel_size_in, stride_in, pad_in, max_len, features_dim, activation,\n",
    "                                  hidden_dim, latent_dim, batchnorm)\n",
    "        self.decoder = CNNDecoder(kernel_size_trans, stride_trans, pad_trans, self.len_in, features_dim,\n",
    "                                  output_padding_trans_1, output_padding_trans_2, activation, hidden_dim, latent_dim,\n",
    "                                  batchnorm)\n",
    "\n",
    "    # VAE functions (fwd, embed, reparam, etc)\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "    def embed(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        return z\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        # During training, the reparameterisation leads to z = mu + std * eps\n",
    "        # During evaluation, the trick is disabled and z = mu\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            epsilon = torch.empty_like(mu).normal_(mean=0, std=1)\n",
    "            return (epsilon * std) + mu\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def sample_latent(self, n_samples):\n",
    "        z = torch.randn((n_samples, self.latent_dim)).to(device=self.encoder[0].weight.device)\n",
    "        return z\n",
    "\n",
    "    # Reshaping / reconstruction functions\n",
    "    def slice_x(self, x):\n",
    "        \"\"\"\n",
    "        Slices and extracts // reshapes the sequence vector\n",
    "        Also extracts the positional encoding vector if used (?)\n",
    "        Args:\n",
    "            x:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # Here this function exists for compatibility reason\n",
    "        # In theory the CNN should reconstruct the sequence in the right dimension and require no slicing - reshaping\n",
    "        # We only slice to extract the sequence and positional encoding if it exists\n",
    "        sequence = x\n",
    "        positional_encoding = None\n",
    "        # Then, if self.pos_dim is not 0, further slice the tensor to recover each part\n",
    "        if self.add_positional_encoding:\n",
    "            positional_encoding = sequence[:, :, self.aa_dim:]\n",
    "            sequence = sequence[:, :, :self.aa_dim]\n",
    "        return sequence, positional_encoding\n",
    "\n",
    "    def recover_indices(self, seq_tensor):\n",
    "        N, max_len = seq_tensor.shape[0], seq_tensor.shape[1]\n",
    "\n",
    "        # Expand MATRIX_VALUES to have the same shape as x_seq for broadcasting\n",
    "        expanded_MATRIX_VALUES = self.MATRIX_VALUES.unsqueeze(0).expand(N, -1, -1, -1)\n",
    "        # Compute the absolute differences\n",
    "        abs_diff = torch.abs(seq_tensor.unsqueeze(2) - expanded_MATRIX_VALUES)\n",
    "        # Sum along the last dimension (20) to get the absolute differences for each character\n",
    "        abs_diff_sum = abs_diff.sum(dim=-1)\n",
    "\n",
    "        # Find the argmin along the character dimension (21)\n",
    "        argmin_indices = torch.argmin(abs_diff_sum, dim=-1)\n",
    "        return argmin_indices\n",
    "\n",
    "    def recover_sequences_blosum(self, seq_tensor, AA_KEYS='ARNDCQEGHILKMFPSTWYVX'):\n",
    "        return [''.join([AA_KEYS[y] for y in x]) for x in self.recover_indices(seq_tensor)]\n",
    "\n",
    "    def reconstruct_hat(self, x_hat):\n",
    "        sequence, positional_encoding = self.slice_x(x_hat)\n",
    "        seq_idx = self.recover_indices(sequence)\n",
    "        return seq_idx, positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c7b13433-0076-4191-971e-f1337ee0537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.batchnorm.BatchNorm1d'> 128 9 4 2 1 0 SELU()\n"
     ]
    }
   ],
   "source": [
    "cnnvae = CNNVAE(kernel_in, stride_in, pad_in, kernel_out, stride_out, pad_out, output_padding_1, output_padding_2,\n",
    "                7,8,22,6,7,23, 'BL50LO', -20, 20, True, nn.SELU(), 128, 128, True)\n",
    "x = torch.randn(100, 7+8+22+6+7+23, 26)\n",
    "x_out, mu, logvar = cnnvae(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "eaf3c36b-d3af-4412-91bc-aa0dda1878d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del CNNVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7c81cd9e-c2d9-4177-aa32-f5772903507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.batchnorm.BatchNorm1d'> 128 9 4 2 1 0 SELU()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 73, 26]),\n",
       " torch.Size([100, 73, 26]),\n",
       " torch.Size([100, 128]),\n",
       " torch.Size([100, 128]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.conv_models import CNNVAE\n",
    "\n",
    "cnnvae = CNNVAE(kernel_in, stride_in, pad_in, kernel_out, stride_out, pad_out, output_padding_1, output_padding_2,\n",
    "                7,8,22,6,7,23, 'BL50LO', -20, 20, True, nn.SELU(), 128, 128, True)\n",
    "x = torch.randn(100, 7+8+22+6+7+23, 26)\n",
    "x_out, mu, logvar = cnnvae(x)\n",
    "x.shape, x_out.shape, mu.shape, logvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b2294583-d2b0-49d6-bc09-a8777e5d78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import TCRSpecificDataset\n",
    "dataset = TCRSpecificDataset(mock_df, 7,8,22,6,7,23, add_positional_encoding=True, conv=True)\n",
    "loader = dataset.get_dataloader(1024, sampler=SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fe9c03dd-b7b2-4ea3-b54b-53cd04c37697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24887, 73, 26])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "98e792f7-f956-4c2d-ba56-a102e1c5951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, labels in loader:\n",
    "    x_out, mu, logvar = cnnvae(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "217c587a-6a95-4e24-a484-75306651b22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 73, 26]), torch.Size([1024, 128]), torch.Size([1024, 128]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_out.shape, mu.shape, logvar.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pynn] *",
   "language": "python",
   "name": "conda-env-pynn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
