{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0c502d-15be-4606-bd89-b9ee102196bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACoAAAAuCAYAAABeUotNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACXElEQVR4nO2WQUrrUBSG/4ZCoW1iSLVIkdBBAx1kCQEX0ImgoMsQHAbp3IkrcJCZIELpuIXuQ0vNSKOI3CSVSMXj5DWQUmrPI6m8Rz7I4B6Sky83Nzd/gYgI/wDSbwusSy6aNrlo2uSiaZOLps1fi768vKDVamE0GqWoswJi4nkeWZZFkiQRADo8PKTZbMZtw4Yt2m63qVwuk+M4BICazSZdXFxk4ZagQLR+KLm/v4dhGHBdF7quo1Ao4Pz8HI7jwHXdxLm+78P3/Xj89fWF6XQKWZbRaDQgScxVx3mqXq9HmqbFYwB0dXVFAOjt7S1x7v7+PgFYeozHY/aMFjkPFQQBKpVKolYqlQAAYRhCVdW43u/3EzMqhIBpmgAATdN4swmAJVqpVPD+/p6ofXx8AABkWU7UFUWBoiiJ8Rz2awdzezJNE6+vr/A8L649PDxgb28PW1tb7Juz4K4Vy7Lo5OSEfN+Pv/put/vjdUKIeI0KIdhrlC369PRER0dHVKvVaGdnh87Ozujz8/PH66IoItu2ybZtiqKILcrann6T//9fv2ly0bTZiOjz8zMODg6gqio0TYOqqhgOh6weGxE9Pj5GtVrF7e0tZFmGEAI3Nze8JuwNjcnd3R0BoMvLS9J1na6vrwkA1et1Vp/MReeJ6/HxMQ7Y+POHWkxcq8j81c8T1+7uLorFZAYKw3DtPpmLLktccxYT1yoyF12WuABge3ublbgyFzUMA5Zl4fT0FEEQYDKZAAA6nQ6vUUbfUILFxAWABoMBq0eentImF02bXDRtctG0yUXTJhdNm2/tMzRBDgOZWQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1x1 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "f,a = plt.subplots(1,1,figsize=(1e-2, 1e-2))\n",
    "mpl.rcParams['figure.dpi'] = 180\n",
    "sns.set_style('darkgrid')\n",
    "import os,sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime as dt\n",
    "from src.utils import mkdirs, convert_path, pkl_dump, pkl_load, display_side, add_median_labels, get_palette\n",
    "from src.data_processing import BL62_VALUES, BL62FREQ_VALUES, HLAS, AA_KEYS\n",
    "from src.metrics import get_nested_feature_importance, plot_feature_importance, get_metrics\n",
    "from src.utils import pkl_load, pkl_dump, get_palette\n",
    "from src.bootstrap import bootstrap_eval, get_pval, plot_pval\n",
    "from src.bootstrap import get_pval, plot_pval\n",
    "from src.datasets import NNAlignDataset, get_NNAlign_dataloader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "mpl.rcParams['figure.dpi'] = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d9cf1-fec7-4518-acc6-208386782c84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# small dim tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6215b60d-07bb-4def-a2ba-936ff4d86596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=483, out_features=241, bias=True), ReLU(), Linear(in_features=241, out_features=120, bias=True), ReLU()]\n"
     ]
    }
   ],
   "source": [
    "in_dim=21*23\n",
    "hidden_dim=128\n",
    "n_layers=2\n",
    "\n",
    "layers = [nn.Linear(in_dim, in_dim//2), nn.ReLU()]\n",
    "for n in range(n_layers-1):\n",
    "    in_dim=in_dim//2\n",
    "    layers.extend([nn.Linear(in_dim, in_dim//2), nn.ReLU()])\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe7a9a-49b8-4e11-9bea-6f75a5842770",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gliph first because it's shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79943032-05ea-44eb-afe9-82f98ad9dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are Mtb specific TCRs\n",
    "gliph_bulk = pd.read_excel('../data/GLIPH/GLIPH2_Mtb_TCRs_41587_2020_505_MOESM3_ESM.xlsx', sheet_name = 'bulk TCR')\n",
    "gliph_sc =  pd.read_excel('../data/GLIPH/GLIPH2_Mtb_TCRs_41587_2020_505_MOESM3_ESM.xlsx', sheet_name = 'single cell TCR')\n",
    "gliph_sc.drop(columns=[x for x in gliph_sc.columns if 'Unnamed' in x and not '40' in x], inplace=True)\n",
    "\n",
    "gliph_bulk['len_beta'] = gliph_bulk['CDR3b'].apply(len)\n",
    "gliph_sc['len_beta'] = gliph_sc['CDR3beta'].apply(lambda x: len(x) if x is not np.nan else np.nan)\n",
    "gliph_sc['len_alpha'] = gliph_sc['CDR3alpha'].apply(lambda x: len(x) if x is not np.nan else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3787323b-fc41-4066-a863-a0246f380e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>V</th>\n",
       "      <th>J</th>\n",
       "      <th>peptide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASTGSYGYTF</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASSLTYGYTF</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASSSRTGGYGYTF</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CATSDRMDNEQFF</td>\n",
       "      <td>TRBV24-1*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASGGEFYGYTF</td>\n",
       "      <td>TRBV7-9*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CDR3b            V           J      peptide\n",
       "0     CASTGSYGYTF    TRBV19*01  TRBJ1-2*01  KAFSPEVIPMF\n",
       "1     CASSLTYGYTF    TRBV19*01  TRBJ1-2*01  KAFSPEVIPMF\n",
       "2  CASSSRTGGYGYTF    TRBV19*01  TRBJ1-2*01  KAFSPEVIPMF\n",
       "3   CATSDRMDNEQFF  TRBV24-1*01  TRBJ2-1*01  KAFSPEVIPMF\n",
       "4    CASGGEFYGYTF   TRBV7-9*01  TRBJ1-2*01  KAFSPEVIPMF"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3264 TCRs in the GLIPH VDJdb\n",
    "gliph_vdjdb = pd.read_excel('../data/GLIPH/GLIPH2_VDJdb_41587_2020_505_MOESM4_ESM.xlsx', header=None, names=['CDR3b', 'V', 'J', 'peptide'])\n",
    "display(gliph_vdjdb.head()), print(len(gliph_vdjdb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86263cf5-35ed-4983-ad42-4c307c21861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10501\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>Vb</th>\n",
       "      <th>Jb</th>\n",
       "      <th>CDR3a</th>\n",
       "      <th>Va</th>\n",
       "      <th>Ja</th>\n",
       "      <th>Individual</th>\n",
       "      <th>Counts</th>\n",
       "      <th>len_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASRDRGLSYEQYF</td>\n",
       "      <td>TRBV10-3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASRLGLAGLTQETQYF</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASRPGLWGEQFF</td>\n",
       "      <td>TRBV28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASRQAGRDEQFF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASRSRGGRTYNEQFF</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CDR3b        Vb  Jb  CDR3a  Va  Ja Individual  Counts  len_beta\n",
       "0     CASRDRGLSYEQYF  TRBV10-3 NaN    NaN NaN NaN    01/1016       1        14\n",
       "1  CASRLGLAGLTQETQYF   TRBV5-1 NaN    NaN NaN NaN    01/1016       3        17\n",
       "2      CASRPGLWGEQFF    TRBV28 NaN    NaN NaN NaN    01/1016       2        13\n",
       "3      CASRQAGRDEQFF   TRBV6-1 NaN    NaN NaN NaN    01/1016       1        13\n",
       "4   CASRSRGGRTYNEQFF   TRBV6-2 NaN    NaN NaN NaN    01/1016       2        16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(gliph_bulk))\n",
    "display(gliph_bulk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e73335b3-555e-45f8-9644-cf1bd924e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Individual</th>\n",
       "      <th>Stim</th>\n",
       "      <th>Vbeta</th>\n",
       "      <th>Jbeta</th>\n",
       "      <th>CDR3beta</th>\n",
       "      <th>BetaConfi</th>\n",
       "      <th>BetaReads</th>\n",
       "      <th>Valpha</th>\n",
       "      <th>Jalpha</th>\n",
       "      <th>CDR3alpha</th>\n",
       "      <th>AlphaConfi</th>\n",
       "      <th>AlphaReads</th>\n",
       "      <th>2ndValpha</th>\n",
       "      <th>2ndJalpha</th>\n",
       "      <th>CDR3alpha2nd</th>\n",
       "      <th>Confi</th>\n",
       "      <th>Reads</th>\n",
       "      <th>Unnamed: 40</th>\n",
       "      <th>len_beta</th>\n",
       "      <th>len_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/1013</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBV28</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>CASMGQIYEQYF</td>\n",
       "      <td>0.928</td>\n",
       "      <td>5434.0</td>\n",
       "      <td>TRAV9-1</td>\n",
       "      <td>TRAJ57</td>\n",
       "      <td>CALSAQGGSEKLVF</td>\n",
       "      <td>0.924</td>\n",
       "      <td>4636.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GCCCAGGGTTTTCCCAGTCACGACGGGGTACAGTGTCTCTAGAGAG...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/1013</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>TRBJ1-3</td>\n",
       "      <td>CASSLESVSGNTIYF</td>\n",
       "      <td>0.932</td>\n",
       "      <td>4931.0</td>\n",
       "      <td>TRAV26-1</td>\n",
       "      <td>TRAJ56</td>\n",
       "      <td>CIVRVSGAGANSKLTF</td>\n",
       "      <td>0.619</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>TRAV26-2</td>\n",
       "      <td>TRAJ43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.273</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>GCCCAGGGTTTTCCCAGTCACGACCGATTCTCAGGGCGCCAGTTCT...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/1013</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-6</td>\n",
       "      <td>CASKRDSQAYNSPLHF</td>\n",
       "      <td>0.928</td>\n",
       "      <td>4841.0</td>\n",
       "      <td>TRAV8-3</td>\n",
       "      <td>TRAJ27</td>\n",
       "      <td>CAANTNAGKSTF</td>\n",
       "      <td>0.910</td>\n",
       "      <td>5573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GCCCAGGGTTTTCCCAGTCACGACGGTACAGCGTCTCTCGGGAGAA...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/1013</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>CASSALQAYEQYF</td>\n",
       "      <td>0.925</td>\n",
       "      <td>5296.0</td>\n",
       "      <td>TRAV17</td>\n",
       "      <td>TRAJ52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.996</td>\n",
       "      <td>3589.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GCCCAGGGTTTTCCCAGTCACGACTGGCTACAATGTCTCCAGATTA...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/1013</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>CSARSPPDTQYF</td>\n",
       "      <td>0.941</td>\n",
       "      <td>5606.0</td>\n",
       "      <td>TRAV9-1</td>\n",
       "      <td>TRAJ39</td>\n",
       "      <td>CALSDRYNNAGNMLTF</td>\n",
       "      <td>0.896</td>\n",
       "      <td>3608.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GCCCAGGGTTTTCCCAGTCACGACGGACAAGTTTCTCATCAACCAT...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Individual    Stim     Vbeta    Jbeta          CDR3beta  BetaConfi  \\\n",
       "0    01/1013  MtbLys    TRBV28  TRBJ2-7      CASMGQIYEQYF      0.928   \n",
       "1    01/1013  MtbLys   TRBV5-1  TRBJ1-3   CASSLESVSGNTIYF      0.932   \n",
       "2    01/1013  MtbLys    TRBV19  TRBJ1-6  CASKRDSQAYNSPLHF      0.928   \n",
       "3    01/1013  MtbLys   TRBV6-1  TRBJ2-7     CASSALQAYEQYF      0.925   \n",
       "4    01/1013  MtbLys  TRBV20-1  TRBJ2-3      CSARSPPDTQYF      0.941   \n",
       "\n",
       "   BetaReads    Valpha  Jalpha         CDR3alpha  AlphaConfi  AlphaReads  \\\n",
       "0     5434.0   TRAV9-1  TRAJ57    CALSAQGGSEKLVF       0.924      4636.0   \n",
       "1     4931.0  TRAV26-1  TRAJ56  CIVRVSGAGANSKLTF       0.619      2358.0   \n",
       "2     4841.0   TRAV8-3  TRAJ27      CAANTNAGKSTF       0.910      5573.0   \n",
       "3     5296.0    TRAV17  TRAJ52               NaN       0.996      3589.0   \n",
       "4     5606.0   TRAV9-1  TRAJ39  CALSDRYNNAGNMLTF       0.896      3608.0   \n",
       "\n",
       "  2ndValpha 2ndJalpha CDR3alpha2nd  Confi   Reads  \\\n",
       "0       NaN       NaN          NaN    NaN     NaN   \n",
       "1  TRAV26-2    TRAJ43          NaN  0.273  1041.0   \n",
       "2       NaN       NaN          NaN    NaN     NaN   \n",
       "3       NaN       NaN          NaN    NaN     NaN   \n",
       "4       NaN       NaN          NaN    NaN     NaN   \n",
       "\n",
       "                                         Unnamed: 40  len_beta  len_alpha  \n",
       "0  GCCCAGGGTTTTCCCAGTCACGACGGGGTACAGTGTCTCTAGAGAG...      12.0       14.0  \n",
       "1  GCCCAGGGTTTTCCCAGTCACGACCGATTCTCAGGGCGCCAGTTCT...      15.0       16.0  \n",
       "2  GCCCAGGGTTTTCCCAGTCACGACGGTACAGCGTCTCTCGGGAGAA...      16.0       12.0  \n",
       "3  GCCCAGGGTTTTCCCAGTCACGACTGGCTACAATGTCTCCAGATTA...      13.0        NaN  \n",
       "4  GCCCAGGGTTTTCCCAGTCACGACGGACAAGTTTCTCATCAACCAT...      12.0       16.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(gliph_sc))\n",
    "display(gliph_sc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "595fd115-4637-4eec-b12b-a175cd61a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "gliph_bulk['seq_id'] = [f'seq_{i:05}' for i in range(1, len(gliph_bulk)+1)]\n",
    "gliph_sc['seq_id'] = [f'seq_{i:05}' for i in range(1, len(gliph_sc)+1)]\n",
    "gliph_bulk.to_csv('../data/GLIPH/230918_gliph_bulk.csv', index=False)\n",
    "gliph_sc.to_csv('../data/GLIPH/230918_gliph_sc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1238010-c5c6-4f0b-88ca-841b63ab61e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3beta</th>\n",
       "      <th>Vbeta</th>\n",
       "      <th>Individual</th>\n",
       "      <th>Counts</th>\n",
       "      <th>len_beta</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>db</th>\n",
       "      <th>Stim</th>\n",
       "      <th>Jbeta</th>\n",
       "      <th>BetaConfi</th>\n",
       "      <th>BetaReads</th>\n",
       "      <th>Confi</th>\n",
       "      <th>Reads</th>\n",
       "      <th>Unnamed: 40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASRDRGLSYEQYF</td>\n",
       "      <td>TRBV10-3</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>seq_00001</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASRLGLAGLTQETQYF</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>seq_00002</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASRPGLWGEQFF</td>\n",
       "      <td>TRBV28</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>seq_00003</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASRQAGRDEQFF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>seq_00004</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASRSRGGRTYNEQFF</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>seq_00005</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CDR3beta     Vbeta Individual  Counts  len_beta     seq_id    db  \\\n",
       "0     CASRDRGLSYEQYF  TRBV10-3    01/1016     1.0      14.0  seq_00001  bulk   \n",
       "1  CASRLGLAGLTQETQYF   TRBV5-1    01/1016     3.0      17.0  seq_00002  bulk   \n",
       "2      CASRPGLWGEQFF    TRBV28    01/1016     2.0      13.0  seq_00003  bulk   \n",
       "3      CASRQAGRDEQFF   TRBV6-1    01/1016     1.0      13.0  seq_00004  bulk   \n",
       "4   CASRSRGGRTYNEQFF   TRBV6-2    01/1016     2.0      16.0  seq_00005  bulk   \n",
       "\n",
       "  Stim Jbeta  BetaConfi  BetaReads  Confi  Reads Unnamed: 40  \n",
       "0  NaN   NaN        NaN        NaN    NaN    NaN         NaN  \n",
       "1  NaN   NaN        NaN        NaN    NaN    NaN         NaN  \n",
       "2  NaN   NaN        NaN        NaN    NaN    NaN         NaN  \n",
       "3  NaN   NaN        NaN        NaN    NaN    NaN         NaN  \n",
       "4  NaN   NaN        NaN        NaN    NaN    NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gliph_merged_cdr3=pd.concat([gliph_bulk.assign(db='bulk').drop(columns=['Jb', 'CDR3a', 'Va', 'Ja']).rename(columns={'CDR3b':'CDR3beta', 'Vb':'Vbeta'}), \n",
    "           gliph_sc.assign(db='sc').drop(columns=[x for x in gliph_sc.columns if 'alpha' in x.lower()])]).dropna(subset='CDR3beta').reset_index(drop=True)\n",
    "display(gliph_merged_cdr3.head()), print(len(gliph_merged_cdr3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9ab8a41-c7c0-4aa9-b781-1b300017795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gliph_merged_cdr3['flag'] = gliph_merged_cdr3['CDR3beta'].apply(lambda x: 'X' in x or '-' in x)\n",
    "gliph_merged_cdr3.query('flag')\n",
    "# Only keep the datapoints that don't have X\n",
    "gliph_merged_cdr3.query('not flag').drop(columns='flag').reset_index(drop=True).to_csv('../data/GLIPH/230918_gliph_merged_cdr3beta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "db7241fe-b0e8-4f22-b93b-a99fac7b3136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16652, 15048)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gliph_merged_cdr3 = pd.read_csv('../data/GLIPH/230918_gliph_merged_cdr3beta.csv')\n",
    "len(gliph_merged_cdr3), len(gliph_merged_cdr3['CDR3beta'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "73092977-01fd-4942-bb95-ab7b6f4c6a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3beta</th>\n",
       "      <th>Vbeta</th>\n",
       "      <th>Individual</th>\n",
       "      <th>Counts</th>\n",
       "      <th>len_beta</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>db</th>\n",
       "      <th>Stim</th>\n",
       "      <th>Jbeta</th>\n",
       "      <th>BetaConfi</th>\n",
       "      <th>BetaReads</th>\n",
       "      <th>Confi</th>\n",
       "      <th>Reads</th>\n",
       "      <th>Unnamed: 40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASRDRGLSYEQYF</td>\n",
       "      <td>TRBV10-3</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>seq_00001</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASRLGLAGLTQETQYF</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>seq_00002</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASRPGLWGEQFF</td>\n",
       "      <td>TRBV28</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>seq_00003</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASRQAGRDEQFF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>seq_00004</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASRSRGGRTYNEQFF</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>01/1016</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>seq_00005</td>\n",
       "      <td>bulk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16647</th>\n",
       "      <td>CASSELEADTQYF</td>\n",
       "      <td>TRBV2</td>\n",
       "      <td>09/0334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>seq_08249</td>\n",
       "      <td>sc</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>0.934</td>\n",
       "      <td>7306.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CTCCAGGGTTTTCCCAGTCACGACGCCTGATGGATCAAATTTCACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>CSVVEGFGSFEKRNTQYF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>09/0334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>seq_08250</td>\n",
       "      <td>sc</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>0.904</td>\n",
       "      <td>5390.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CTCCAGGGTTTTCCCAGTCACGACGTTTCCCATCAGCCGCCCAAAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>CASSSSPGSNSPLHF</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>09/0334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>seq_08251</td>\n",
       "      <td>sc</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBJ1-6</td>\n",
       "      <td>0.925</td>\n",
       "      <td>6794.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CTCCAGGGTTTTCCCAGTCACGACCTCCGCACAACAGTTCCCTGAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>CASSPGQPGGYTF</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>09/0334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>seq_08252</td>\n",
       "      <td>sc</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>0.937</td>\n",
       "      <td>7678.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CTCCAGGGTTTTCCCAGTCACGACTACAAAGTCTCTCGAAAAGAGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>CATSRNQGANTEAFF</td>\n",
       "      <td>TRBV15</td>\n",
       "      <td>09/0334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>seq_08253</td>\n",
       "      <td>sc</td>\n",
       "      <td>MtbLys</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>0.918</td>\n",
       "      <td>6276.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CTCCAGGGTTTTCCCAGTCACGACGATAACTTCCAATCCAGGAGGC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16652 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 CDR3beta     Vbeta Individual  Counts  len_beta     seq_id  \\\n",
       "0          CASRDRGLSYEQYF  TRBV10-3    01/1016     1.0      14.0  seq_00001   \n",
       "1       CASRLGLAGLTQETQYF   TRBV5-1    01/1016     3.0      17.0  seq_00002   \n",
       "2           CASRPGLWGEQFF    TRBV28    01/1016     2.0      13.0  seq_00003   \n",
       "3           CASRQAGRDEQFF   TRBV6-1    01/1016     1.0      13.0  seq_00004   \n",
       "4        CASRSRGGRTYNEQFF   TRBV6-2    01/1016     2.0      16.0  seq_00005   \n",
       "...                   ...       ...        ...     ...       ...        ...   \n",
       "16647       CASSELEADTQYF     TRBV2    09/0334     NaN      13.0  seq_08249   \n",
       "16648  CSVVEGFGSFEKRNTQYF  TRBV29-1    09/0334     NaN      18.0  seq_08250   \n",
       "16649     CASSSSPGSNSPLHF     TRBV9    09/0334     NaN      15.0  seq_08251   \n",
       "16650       CASSPGQPGGYTF    TRBV27    09/0334     NaN      13.0  seq_08252   \n",
       "16651     CATSRNQGANTEAFF    TRBV15    09/0334     NaN      15.0  seq_08253   \n",
       "\n",
       "         db    Stim    Jbeta  BetaConfi  BetaReads  Confi  Reads  \\\n",
       "0      bulk     NaN      NaN        NaN        NaN    NaN    NaN   \n",
       "1      bulk     NaN      NaN        NaN        NaN    NaN    NaN   \n",
       "2      bulk     NaN      NaN        NaN        NaN    NaN    NaN   \n",
       "3      bulk     NaN      NaN        NaN        NaN    NaN    NaN   \n",
       "4      bulk     NaN      NaN        NaN        NaN    NaN    NaN   \n",
       "...     ...     ...      ...        ...        ...    ...    ...   \n",
       "16647    sc  MtbLys  TRBJ2-3      0.934     7306.0    NaN    NaN   \n",
       "16648    sc  MtbLys  TRBJ2-3      0.904     5390.0    NaN    NaN   \n",
       "16649    sc  MtbLys  TRBJ1-6      0.925     6794.0    NaN    NaN   \n",
       "16650    sc  MtbLys  TRBJ1-2      0.937     7678.0    NaN    NaN   \n",
       "16651    sc  MtbLys  TRBJ1-1      0.918     6276.0    NaN    NaN   \n",
       "\n",
       "                                             Unnamed: 40  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "16647  CTCCAGGGTTTTCCCAGTCACGACGCCTGATGGATCAAATTTCACT...  \n",
       "16648  CTCCAGGGTTTTCCCAGTCACGACGTTTCCCATCAGCCGCCCAAAC...  \n",
       "16649  CTCCAGGGTTTTCCCAGTCACGACCTCCGCACAACAGTTCCCTGAC...  \n",
       "16650  CTCCAGGGTTTTCCCAGTCACGACTACAAAGTCTCTCGAAAAGAGA...  \n",
       "16651  CTCCAGGGTTTTCCCAGTCACGACGATAACTTCCAATCCAGGAGGC...  \n",
       "\n",
       "[16652 rows x 14 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gliph_merged_cdr3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88460363-eecf-4d71-9971-1eb1ee19ce29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# DeepTCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d97403c2-76e3-40a6-8406-50df709da19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m10x_Data\u001b[m\u001b[m/             \u001b[34mHuman_Antigens\u001b[m\u001b[m/       \u001b[34mSidhom\u001b[m\u001b[m/\n",
      "\u001b[34mDash_Human\u001b[m\u001b[m/           \u001b[34mHuman_HLA_Tutorial\u001b[m\u001b[m/   VDJ.tsv\n",
      "\u001b[34mDash_Murine\u001b[m\u001b[m/          McPAS-TCR.csv         \u001b[34mZhang\u001b[m\u001b[m/\n",
      "\u001b[34mGlanville\u001b[m\u001b[m/            \u001b[34mMurine_Antigens\u001b[m\u001b[m/      combine_vdj_mcpas.py\n",
      "\u001b[34mHIV\u001b[m\u001b[m/                  \u001b[34mRudqvist\u001b[m\u001b[m/             vdj_mcpas.csv\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/DeepTCR/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4bab55a6-4f40-4afb-b9f5-d4d8acfdfa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>A0101_VTEHDTLLY_IE-1_CMV</th>\n",
       "      <th>A0201_KTWGQYWQV_gp100_Cancer</th>\n",
       "      <th>A0201_ELAGIGILTV_MART-1_Cancer</th>\n",
       "      <th>A0201_CLLWSFQTSA_Tyrosinase_Cancer</th>\n",
       "      <th>A0201_IMDQVPFSV_gp100_Cancer</th>\n",
       "      <th>A0201_SLLMWITQV_NY-ESO-1_Cancer</th>\n",
       "      <th>A0201_KVAELVHFL_MAGE-A3_Cancer</th>\n",
       "      <th>A0201_KVLEYVIKV_MAGE-A1_Cancer</th>\n",
       "      <th>A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase</th>\n",
       "      <th>A0201_LLDFVRFMGV_EBNA-3B_EBV</th>\n",
       "      <th>A0201_LLMGTLGIVC_HPV-16E7_82-91</th>\n",
       "      <th>A0201_CLGGLLTMV_LMP-2A_EBV</th>\n",
       "      <th>A0201_YLLEMLWRL_LMP1_EBV</th>\n",
       "      <th>A0201_FLYALALLL_LMP2A_EBV</th>\n",
       "      <th>A0201_GILGFVFTL_Flu-MP_Influenza</th>\n",
       "      <th>A0201_GLCTLVAML_BMLF1_EBV</th>\n",
       "      <th>A0201_NLVPMVATV_pp65_CMV</th>\n",
       "      <th>A0201_ILKEPVHGV_RT_HIV</th>\n",
       "      <th>A0201_FLASKIGRLV_Ca2-indepen-Plip-A2</th>\n",
       "      <th>A2402_CYTWNQMNL_WT1-(235-243)236M_Y</th>\n",
       "      <th>A0201_RTLNAWVKV_Gag-protein_HIV</th>\n",
       "      <th>A0201_KLQCVDLHV_PSA146-154</th>\n",
       "      <th>A0201_LLFGYPVYV_HTLV-1</th>\n",
       "      <th>A0201_SLFNTVATL_Gag-protein_HIV</th>\n",
       "      <th>A0201_SLYNTVATLY_Gag-protein_HIV</th>\n",
       "      <th>A0201_SLFNTVATLY_Gag-protein_HIV</th>\n",
       "      <th>A0201_RMFPNAPYL_WT-1</th>\n",
       "      <th>A0201_YLNDHLEPWI_BCL-X_Cancer</th>\n",
       "      <th>A0201_MLDLQPETT_16E7_HPV</th>\n",
       "      <th>A0301_KLGGALQAK_IE-1_CMV</th>\n",
       "      <th>A0301_RLRAEAQVK_EMNA-3A_EBV</th>\n",
       "      <th>A0301_RIAAWMATY_BCL-2L1_Cancer</th>\n",
       "      <th>A1101_IVTDFSVIK_EBNA-3B_EBV</th>\n",
       "      <th>A1101_AVFDRKSDAK_EBNA-3B_EBV</th>\n",
       "      <th>B3501_IPSINVHHY_pp65_CMV</th>\n",
       "      <th>A2402_AYAQKIFKI_IE-1_CMV</th>\n",
       "      <th>A2402_QYDPVAALF_pp65_CMV</th>\n",
       "      <th>B0702_QPRAPIRPI_EBNA-6_EBV</th>\n",
       "      <th>B0702_TPRVTGGGAM_pp65_CMV</th>\n",
       "      <th>B0702_RPPIFIRRL_EBNA-3A_EBV</th>\n",
       "      <th>B0702_RPHERNGFTVL_pp65_CMV</th>\n",
       "      <th>B0801_RAKFKQLL_BZLF1_EBV</th>\n",
       "      <th>B0801_ELRRKMMYM_IE-1_CMV</th>\n",
       "      <th>B0801_FLRGRAYGL_EBNA-3A_EBV</th>\n",
       "      <th>A0101_SLEGGGLGY_NC</th>\n",
       "      <th>A0101_STEGGGLAY_NC</th>\n",
       "      <th>A0201_ALIAPVHAV_NC</th>\n",
       "      <th>A2402_AYSSAGASI_NC</th>\n",
       "      <th>B0702_GPAESAAGL_NC</th>\n",
       "      <th>NR(B0801)_AAKGRGAAL_NC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAAAAGEAGTYKYIF</td>\n",
       "      <td>CASSLEEGYSPLHF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAAAANQAGTALIF</td>\n",
       "      <td>CASSFFVGGAEAFF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAAAASDGQKLLF</td>\n",
       "      <td>CASTLPGQKQFF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAAAAYNQGGKLIF</td>\n",
       "      <td>CATSDPAGMTGGWHGYTF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAAADNYGQNFVF</td>\n",
       "      <td>CAWSSGEGTDTQYF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             alpha                beta  A0101_VTEHDTLLY_IE-1_CMV  \\\n",
       "0  CAAAAGEAGTYKYIF      CASSLEEGYSPLHF                       0.0   \n",
       "1   CAAAANQAGTALIF      CASSFFVGGAEAFF                       0.0   \n",
       "2    CAAAASDGQKLLF        CASTLPGQKQFF                       0.0   \n",
       "3   CAAAAYNQGGKLIF  CATSDPAGMTGGWHGYTF                       0.0   \n",
       "4    CAAADNYGQNFVF      CAWSSGEGTDTQYF                       1.0   \n",
       "\n",
       "   A0201_KTWGQYWQV_gp100_Cancer  A0201_ELAGIGILTV_MART-1_Cancer  \\\n",
       "0                           0.0                             0.0   \n",
       "1                           0.0                             0.0   \n",
       "2                           0.0                             0.0   \n",
       "3                           0.0                             0.0   \n",
       "4                           0.0                             0.0   \n",
       "\n",
       "   A0201_CLLWSFQTSA_Tyrosinase_Cancer  A0201_IMDQVPFSV_gp100_Cancer  \\\n",
       "0                                 0.0                           0.0   \n",
       "1                                 0.0                           0.0   \n",
       "2                                 0.0                           0.0   \n",
       "3                                 0.0                           0.0   \n",
       "4                                 0.0                           0.0   \n",
       "\n",
       "   A0201_SLLMWITQV_NY-ESO-1_Cancer  A0201_KVAELVHFL_MAGE-A3_Cancer  \\\n",
       "0                              0.0                             0.0   \n",
       "1                              0.0                             0.0   \n",
       "2                              0.0                             0.0   \n",
       "3                              0.0                             0.0   \n",
       "4                              2.0                             0.0   \n",
       "\n",
       "   A0201_KVLEYVIKV_MAGE-A1_Cancer  A0201_CLLGTYTQDV_Kanamycin-B-dioxygenase  \\\n",
       "0                             0.0                                       0.0   \n",
       "1                             0.0                                       0.0   \n",
       "2                             0.0                                       0.0   \n",
       "3                             1.0                                       0.0   \n",
       "4                             0.0                                       0.0   \n",
       "\n",
       "   A0201_LLDFVRFMGV_EBNA-3B_EBV  A0201_LLMGTLGIVC_HPV-16E7_82-91  \\\n",
       "0                           0.0                              0.0   \n",
       "1                           0.0                              0.0   \n",
       "2                           0.0                              0.0   \n",
       "3                           0.0                              0.0   \n",
       "4                           1.0                              0.0   \n",
       "\n",
       "   A0201_CLGGLLTMV_LMP-2A_EBV  A0201_YLLEMLWRL_LMP1_EBV  \\\n",
       "0                         0.0                       0.0   \n",
       "1                         0.0                       0.0   \n",
       "2                         0.0                       0.0   \n",
       "3                         0.0                       0.0   \n",
       "4                         0.0                       0.0   \n",
       "\n",
       "   A0201_FLYALALLL_LMP2A_EBV  A0201_GILGFVFTL_Flu-MP_Influenza  \\\n",
       "0                        0.0                               0.0   \n",
       "1                        0.0                               0.0   \n",
       "2                        0.0                               0.0   \n",
       "3                        0.0                               0.0   \n",
       "4                        0.0                               0.0   \n",
       "\n",
       "   A0201_GLCTLVAML_BMLF1_EBV  A0201_NLVPMVATV_pp65_CMV  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        0.0                       0.0   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   A0201_ILKEPVHGV_RT_HIV  A0201_FLASKIGRLV_Ca2-indepen-Plip-A2  \\\n",
       "0                     0.0                                   0.0   \n",
       "1                     0.0                                   0.0   \n",
       "2                     0.0                                   0.0   \n",
       "3                     2.0                                   0.0   \n",
       "4                     0.0                                   0.0   \n",
       "\n",
       "   A2402_CYTWNQMNL_WT1-(235-243)236M_Y  A0201_RTLNAWVKV_Gag-protein_HIV  \\\n",
       "0                                  0.0                              0.0   \n",
       "1                                  0.0                              0.0   \n",
       "2                                  0.0                              0.0   \n",
       "3                                  0.0                              0.0   \n",
       "4                                  0.0                              0.0   \n",
       "\n",
       "   A0201_KLQCVDLHV_PSA146-154  A0201_LLFGYPVYV_HTLV-1  \\\n",
       "0                         0.0                     0.0   \n",
       "1                         0.0                     0.0   \n",
       "2                         0.0                     0.0   \n",
       "3                         0.0                     0.0   \n",
       "4                         0.0                     1.0   \n",
       "\n",
       "   A0201_SLFNTVATL_Gag-protein_HIV  A0201_SLYNTVATLY_Gag-protein_HIV  \\\n",
       "0                              0.0                               0.0   \n",
       "1                              0.0                               0.0   \n",
       "2                              0.0                               0.0   \n",
       "3                              0.0                               0.0   \n",
       "4                              0.0                               0.0   \n",
       "\n",
       "   A0201_SLFNTVATLY_Gag-protein_HIV  A0201_RMFPNAPYL_WT-1  \\\n",
       "0                               0.0                   0.0   \n",
       "1                               0.0                   0.0   \n",
       "2                               0.0                   0.0   \n",
       "3                               0.0                   0.0   \n",
       "4                               0.0                   0.0   \n",
       "\n",
       "   A0201_YLNDHLEPWI_BCL-X_Cancer  A0201_MLDLQPETT_16E7_HPV  \\\n",
       "0                            1.0                       0.0   \n",
       "1                            0.0                       0.0   \n",
       "2                            0.0                       0.0   \n",
       "3                            0.0                       0.0   \n",
       "4                            0.0                       0.0   \n",
       "\n",
       "   A0301_KLGGALQAK_IE-1_CMV  A0301_RLRAEAQVK_EMNA-3A_EBV  \\\n",
       "0                       0.0                          0.0   \n",
       "1                       0.0                          1.0   \n",
       "2                       0.0                          0.0   \n",
       "3                      25.0                         11.0   \n",
       "4                      21.0                         17.0   \n",
       "\n",
       "   A0301_RIAAWMATY_BCL-2L1_Cancer  A1101_IVTDFSVIK_EBNA-3B_EBV  \\\n",
       "0                             0.0                          0.0   \n",
       "1                             0.0                          0.0   \n",
       "2                             0.0                          0.0   \n",
       "3                             0.0                          7.0   \n",
       "4                             1.0                          5.0   \n",
       "\n",
       "   A1101_AVFDRKSDAK_EBNA-3B_EBV  B3501_IPSINVHHY_pp65_CMV  \\\n",
       "0                           3.0                       0.0   \n",
       "1                           1.0                       0.0   \n",
       "2                           0.0                       0.0   \n",
       "3                          14.0                       0.0   \n",
       "4                          21.0                       0.0   \n",
       "\n",
       "   A2402_AYAQKIFKI_IE-1_CMV  A2402_QYDPVAALF_pp65_CMV  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   B0702_QPRAPIRPI_EBNA-6_EBV  B0702_TPRVTGGGAM_pp65_CMV  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         0.0                        0.0   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   B0702_RPPIFIRRL_EBNA-3A_EBV  B0702_RPHERNGFTVL_pp65_CMV  \\\n",
       "0                          0.0                         0.0   \n",
       "1                          0.0                         0.0   \n",
       "2                          0.0                         0.0   \n",
       "3                          0.0                         0.0   \n",
       "4                          0.0                         0.0   \n",
       "\n",
       "   B0801_RAKFKQLL_BZLF1_EBV  B0801_ELRRKMMYM_IE-1_CMV  \\\n",
       "0                       4.0                       0.0   \n",
       "1                       1.0                       0.0   \n",
       "2                       0.0                       1.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   B0801_FLRGRAYGL_EBNA-3A_EBV  A0101_SLEGGGLGY_NC  A0101_STEGGGLAY_NC  \\\n",
       "0                          0.0                 0.0                 0.0   \n",
       "1                          0.0                 0.0                 0.0   \n",
       "2                          0.0                 0.0                 0.0   \n",
       "3                          0.0                 0.0                 0.0   \n",
       "4                          0.0                 0.0                 0.0   \n",
       "\n",
       "   A0201_ALIAPVHAV_NC  A2402_AYSSAGASI_NC  B0702_GPAESAAGL_NC  \\\n",
       "0                 0.0                 0.0                 0.0   \n",
       "1                 0.0                 0.0                 0.0   \n",
       "2                 0.0                 0.0                 0.0   \n",
       "3                 1.0                 0.0                 0.0   \n",
       "4                 0.0                 0.0                 0.0   \n",
       "\n",
       "   NR(B0801)_AAKGRGAAL_NC  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57228\n"
     ]
    }
   ],
   "source": [
    "tenx = pd.read_csv('../data/DeepTCR/10x_Data/Data_Regression.csv')\n",
    "display(tenx.head())\n",
    "print(len(tenx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e265d93b-a023-4dd9-a54d-c7d425359904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope</th>\n",
       "      <th>cdr3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLWNGPMAV</td>\n",
       "      <td>CASSYSRTGSYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLWNGPMAV</td>\n",
       "      <td>CASSQGLAYEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLWNGPMAV</td>\n",
       "      <td>CASSVEGPGELFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLWNGPMAV</td>\n",
       "      <td>CASSEATGASYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLWNGPMAV</td>\n",
       "      <td>CASSEYVQYYGYTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47292</th>\n",
       "      <td>SFHSLHLLF</td>\n",
       "      <td>CASSPPRLRDTQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47293</th>\n",
       "      <td>SFHSLHLLF</td>\n",
       "      <td>CISVPAARTGHRTQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47294</th>\n",
       "      <td>SFHSLHLLF</td>\n",
       "      <td>CASRPDRDNNYGFGF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47295</th>\n",
       "      <td>SFHSLHLLF</td>\n",
       "      <td>CASRPERDNNCF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47296</th>\n",
       "      <td>SFHSLHLLF</td>\n",
       "      <td>CASIARVPSPGSCF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47297 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         epitope              cdr3\n",
       "0      LLWNGPMAV   CASSYSRTGSYEQYF\n",
       "1      LLWNGPMAV     CASSQGLAYEQFF\n",
       "2      LLWNGPMAV     CASSVEGPGELFF\n",
       "3      LLWNGPMAV   CASSEATGASYEQYF\n",
       "4      LLWNGPMAV    CASSEYVQYYGYTF\n",
       "...          ...               ...\n",
       "47292  SFHSLHLLF    CASSPPRLRDTQYF\n",
       "47293  SFHSLHLLF  CISVPAARTGHRTQYF\n",
       "47294  SFHSLHLLF   CASRPDRDNNYGFGF\n",
       "47295  SFHSLHLLF      CASRPERDNNCF\n",
       "47296  SFHSLHLLF    CASIARVPSPGSCF\n",
       "\n",
       "[47297 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdj_mcpas = pd.read_csv('../data/DeepTCR/vdj_mcpas.csv')\n",
    "vdj_mcpas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e15003-470b-4273-be30-4047d713ccac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# COVID AdaptiveBiotech dataset\n",
    "\n",
    "_forget this one for now ; let's see what we can do with the covid-TCR-specific MIRA data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4bdd2f77-2ea8-4ab2-aa4d-cab8c514a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mImmuneCODE-MIRA-Release002.1\u001b[m\u001b[m/          \u001b[34mImmuneCODE-Repertoires-002.2\u001b[m\u001b[m/\n",
      "ImmuneCODE-Repertoire-Tags-002.2.tsv   ImmuneCODE-Repertoires-002.tgz\n",
      "ImmuneCODE-Repertoire-Tags-002.2.xlsx\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/covid_ada_002/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c0574332-6290-450c-8395-12c453010ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044BW_20200414_Unenr_immunoSEQ_TCRB.tsv\n",
      "1349BW_unsorted_cc1000000_ImmunRACE_043020_003_gDNA_TCRB.tsv\n",
      "1588BW_20200417_PBMC_unsorted_cc1000000_ImmunRACE_050820_008_gDNA_TCRB.tsv\n",
      "1684BW_20200520_Unenr_gDNA_ImmunoSEQ_TCRB.tsv\n",
      "1811BW_20200520_Unenr_gDNA_ImmunoSEQ_TCRB.tsv\n",
      "1886BW_unsorted_cc1000000_ImmunRACE_043020_004_gDNA_TCRB.tsv\n",
      "1979BW_20200420_PBMC_unsorted_cc1000000_ImmunRACE_052220_005_TCRB.tsv\n",
      "1995BW_20200520_Unenr_gDNA_ImmunoSEQ_TCRB.tsv\n",
      "2513BW_20200417_PBMC_unsorted_cc1000000_ImmunRACE_050820_006_gDNA_TCRB.tsv\n",
      "2742BW_20200420_PBMC_unsorted_cc1000000_ImmunRACE_052220_004_TCRB.tsv\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/covid_ada_002/ImmuneCODE-Repertoires-002.2/ | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "100f5a58-3eda-4ce3-85dc-29da89d459f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minigene-detail.csv\n",
      "minigene-hits.csv\n",
      "orfs.csv\n",
      "peptide-detail-ci.csv\n",
      "peptide-detail-cii.csv\n",
      "peptide-hits-ci.csv\n",
      "peptide-hits-cii.csv\n",
      "readme.txt\n",
      "subject-metadata.csv\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/covid_ada_002/ImmuneCODE-MIRA-Release002.1/ | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fb96a5a6-7ab1-47c5-8636-91f9a711f0fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'Biological Sex',\n",
       " 'Dataset',\n",
       " 'Ethnic Group',\n",
       " 'HLA MHC class I',\n",
       " 'HLA MHC class II',\n",
       " 'ImmuneCODERelease',\n",
       " 'Racial Group',\n",
       " 'Tissue Source',\n",
       " 'Virus Diseases',\n",
       " 'abnormal_test_list',\n",
       " 'ace_inhibitor_type',\n",
       " 'ambulatory_limitation',\n",
       " 'arb_type',\n",
       " 'birth_year',\n",
       " 'blood_oxygenation_percent_average',\n",
       " 'blood_oxygenation_percent_max',\n",
       " 'blood_oxygenation_percent_min',\n",
       " 'blood_type',\n",
       " 'cancer_diagnosed',\n",
       " 'cancer_type',\n",
       " 'chest_xray_ct_results',\n",
       " 'cigarette_pack_years',\n",
       " 'cmv_at_donation',\n",
       " 'cohort',\n",
       " 'collection_region',\n",
       " 'covid_abnormal_imaging',\n",
       " 'covid_abnormal_test_results',\n",
       " 'covid_category',\n",
       " 'covid_diagnosis',\n",
       " 'covid_diagnosis_location',\n",
       " 'covid_diagnosis_method',\n",
       " 'covid_exposed',\n",
       " 'covid_pcr_test_result_positive',\n",
       " 'covid_pcr_test_type',\n",
       " 'covid_pneumonia',\n",
       " 'covid_quarantine',\n",
       " 'covid_recovered',\n",
       " 'covid_symptoms_current',\n",
       " 'covid_symptoms_total',\n",
       " 'covid_unit_admit',\n",
       " 'covid_visit',\n",
       " 'current_medications',\n",
       " 'days_from_cancer_diagnosis_to_sample',\n",
       " 'days_from_diagnosis_to_death',\n",
       " 'days_from_diagnosis_to_recovery',\n",
       " 'days_from_diagnosis_to_sample',\n",
       " 'days_from_exposure_to_symptoms',\n",
       " 'days_from_hospitalization_to_sample',\n",
       " 'days_from_last_symptom_to_sample',\n",
       " 'days_from_recovery_to_sample',\n",
       " 'days_from_sample_to_death',\n",
       " 'days_from_symptom_cessation_to_sample',\n",
       " 'days_from_symptom_onset_to_diagnosis',\n",
       " 'days_from_symptom_onset_to_sample',\n",
       " 'days_from_symptom_onset_to_symptom_cessation',\n",
       " 'days_in_hospital',\n",
       " 'days_since_most_recent_exposure',\n",
       " 'days_ventilator',\n",
       " 'death',\n",
       " 'death_days_since_enrollment',\n",
       " 'describe_abnormal_covid_test_results',\n",
       " 'describe_additional_exposures',\n",
       " 'describe_autoimmune_diagnoses',\n",
       " 'describe_autoimmune_medications',\n",
       " 'describe_cancers',\n",
       " 'describe_diagnosis',\n",
       " 'describe_diagnosis_setting',\n",
       " 'describe_immunosupressants',\n",
       " 'describe_other_abnormal_test_results',\n",
       " 'describe_other_covid_symptoms',\n",
       " 'describe_other_diagnoses',\n",
       " 'describe_other_symptoms',\n",
       " 'describe_other_symptoms_one_month',\n",
       " 'describe_race',\n",
       " 'describe_recovered_reasons',\n",
       " 'diabetes_type',\n",
       " 'diagnosis_source',\n",
       " 'diagnostic_test_days_before_enrollment',\n",
       " 'diastolic_bp_mmHg',\n",
       " 'discharged',\n",
       " 'disease_state',\n",
       " 'diseases',\n",
       " 'drug_treatment_category',\n",
       " 'end_treatment_days_since_enrollment',\n",
       " 'ethnicity',\n",
       " 'experiment',\n",
       " 'fever_resolved',\n",
       " 'fever_resolved_days_since_enrollment',\n",
       " 'had_transplant_for_cancer',\n",
       " 'has_abdominal_pain',\n",
       " 'has_asthma',\n",
       " 'has_cancer',\n",
       " 'has_chronic_hypertension',\n",
       " 'has_chronic_kidney_disease',\n",
       " 'has_chronic_kidney_disease_stage_5',\n",
       " 'has_congestive_heart_failure',\n",
       " 'has_copd',\n",
       " 'has_coronary_artery_disease',\n",
       " 'has_cough',\n",
       " 'has_diarrhea',\n",
       " 'has_dysgeusia',\n",
       " 'has_dyspnea',\n",
       " 'has_fatigue',\n",
       " 'has_hiv',\n",
       " 'has_nausea_vomiting',\n",
       " 'has_new_dialysis_need',\n",
       " 'has_new_vaspressor_need',\n",
       " 'has_sputum',\n",
       " 'has_symptoms',\n",
       " 'height_meters',\n",
       " 'high_risk_travel',\n",
       " 'hospitalized',\n",
       " 'hypoxia_resolved',\n",
       " 'icu_admit',\n",
       " 'is_cancer_treated',\n",
       " 'is_immunocompromised',\n",
       " 'nsaid_type',\n",
       " 'observation_days_since_enrollment',\n",
       " 'onset_of_symptoms_days_before_enrollment',\n",
       " 'patient_location_at_sampling',\n",
       " 'pulse_rate',\n",
       " 'quarantine_days',\n",
       " 'quarantine_reasons',\n",
       " 'received_mechanical_ventilation',\n",
       " 'receiving_ecmo',\n",
       " 'receiving_mechanical_ventilation',\n",
       " 'recent_travel_countries',\n",
       " 'recovered_reasons',\n",
       " 'respiratory_rate',\n",
       " 'respiratory_support_type',\n",
       " 'sample_name',\n",
       " 'selected_autoimmune_diagnoses',\n",
       " 'selected_other_diagnoses',\n",
       " 'sixty_day_abnormal_test_results',\n",
       " 'smoker',\n",
       " 'specimen_source',\n",
       " 'start_treatment_days_since_enrollment',\n",
       " 'status',\n",
       " 'subject_id',\n",
       " 'symptoms',\n",
       " 'systolic_bp_mmHg',\n",
       " 'temperature_C_average',\n",
       " 'temperature_C_max',\n",
       " 'temperature_C_min',\n",
       " 'thirty_days_symptoms',\n",
       " 'travel_start_end_days_ago',\n",
       " 'treatment_type',\n",
       " 'uses_ace_inhibitor',\n",
       " 'uses_arb',\n",
       " 'uses_asthma_quick_relief',\n",
       " 'uses_autoimmune_medications',\n",
       " 'uses_corticosteroids_for_asthma',\n",
       " 'uses_immunosuppressant',\n",
       " 'uses_nsaid',\n",
       " 'visit',\n",
       " 'weight_kg',\n",
       " 'who_ordinal_scale']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pd.read_csv('../data/covid_ada_002/ImmuneCODE-Repertoire-Tags-002.2.tsv', sep = '\\t').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "b73f11d3-620e-4ab3-bd1a-1964bf5ce0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min      0.000012\n",
       "80%      0.000012\n",
       "95%      0.000025\n",
       "99.5%    0.000087\n",
       "max      0.018725\n",
       "Name: productive_frequency, dtype: float64"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "567e163e-62fe-48b4-9c93-003829265a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min      0.000001\n",
      "80%      0.000001\n",
      "95%      0.000002\n",
      "99.5%    0.000016\n",
      "max      0.008509\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000004\n",
      "80%      0.000004\n",
      "95%      0.000013\n",
      "99.5%    0.000053\n",
      "max      0.101399\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000013\n",
      "80%      0.000013\n",
      "95%      0.000025\n",
      "99.5%    0.000151\n",
      "max      0.086329\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000012\n",
      "80%      0.000012\n",
      "95%      0.000023\n",
      "99.5%    0.000070\n",
      "max      0.010198\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000016\n",
      "80%      0.000016\n",
      "95%      0.000032\n",
      "99.5%    0.000127\n",
      "max      0.027664\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000026\n",
      "80%      0.000026\n",
      "95%      0.000077\n",
      "99.5%    0.000950\n",
      "max      0.063559\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000013\n",
      "80%      0.000013\n",
      "95%      0.000027\n",
      "99.5%    0.000082\n",
      "max      0.005034\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000003\n",
      "80%      0.000003\n",
      "95%      0.000010\n",
      "99.5%    0.000051\n",
      "max      0.052195\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000018\n",
      "80%      0.000018\n",
      "95%      0.000036\n",
      "99.5%    0.000107\n",
      "max      0.006077\n",
      "Name: productive_frequency, dtype: float64\n",
      "min      0.000011\n",
      "80%      0.000011\n",
      "95%      0.000022\n",
      "99.5%    0.000097\n",
      "max      0.006987\n",
      "Name: productive_frequency, dtype: float64\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[623], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m), os\u001b[38;5;241m.\u001b[39mlistdir(folder)[:\u001b[38;5;241m15\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      5\u001b[0m            \u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductive_frequency!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mna\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and amino_acid !=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mna\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbio_identity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamino_acid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproductive_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_gene\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj_gene\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_family\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj_family\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_resolved\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj_resolved\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filename)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pynn/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "folder = '../data/covid_ada_002/repertoire/ImmuneCODE-Repertoires-002.2/'\n",
    "for f in filter(lambda x: x.endswith('.tsv'), os.listdir(folder)[:15]):\n",
    "    filename = f'{folder}{f}'\n",
    "    df = pd.read_csv(filename, sep = '\\t')\\\n",
    "           .query('productive_frequency!=\"na\" and amino_acid !=\"na\"')\n",
    "    df = df[['bio_identity', 'amino_acid', 'productive_frequency', 'v_gene', 'j_gene', 'v_family', 'j_family', 'v_resolved', 'j_resolved']]\n",
    "    df['ID'] = os.path.basename(filename).replace('.tsv','')\n",
    "    df['productive_frequency'] = df['productive_frequency'].astype(float)\n",
    "    sorted_df = df.sort_values('productive_frequency',ascending=False)\n",
    "    print(df['productive_frequency'].describe(percentiles=[.8, .9, .95, .99, .9925, .995, .9975, .999]).loc[['min', '80%', '95%', '99.5%', 'max']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548a8e4-0d3e-4d93-b9bc-f26e5d71edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_productive_percentile(df, fcol = 'productive_frequency'):\n",
    "    min_val = df[fcol].min()\n",
    "    tiles=np.arange(.6, .9, 0.005)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "14997c39-f419-41f6-bc91-25d455c8d98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53578"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "percentile=30\n",
    "percentile_threshold = df['productive_frequency'].quantile(percentile / 100)\n",
    "\n",
    "# Step 3: Select rows below the percentile threshold\n",
    "bottom_percentile_df = df[sorted_df['productive_frequency'] <= percentile_threshold]\n",
    "len(bottom_percentile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "20fad9d0-7a1b-4a9b-9aab-a25ff8fcba8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58628"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "dda1863b-f3d4-4796-93fb-240a299d099a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.24998437519531e-05"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "6ba14e43-5616-44eb-ba9d-b9bbadc7410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     58628.000000\n",
       "mean          0.000017\n",
       "std           0.000139\n",
       "min           0.000012\n",
       "50%           0.000012\n",
       "80%           0.000012\n",
       "90%           0.000012\n",
       "95%           0.000025\n",
       "99%           0.000050\n",
       "99.25%        0.000062\n",
       "99.5%         0.000087\n",
       "99.75%        0.000162\n",
       "99.9%         0.000409\n",
       "max           0.018725\n",
       "Name: productive_frequency, dtype: float64"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "2e81c94e-d6b0-48e2-8f89-dc68d745f8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.74989062636717e-05"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['productive_frequency'].quantile(.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "191ea2df-d8e6-476a-8128-f5b35d3bc250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4771     False\n",
       "3270     False\n",
       "1020     False\n",
       "150      False\n",
       "330      False\n",
       "         ...  \n",
       "25877     True\n",
       "25878     True\n",
       "25881     True\n",
       "25883     True\n",
       "70583     True\n",
       "Name: productive_frequency, Length: 58628, dtype: bool"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sorted_df['productive_frequency'] <= percentile_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "620e6c15-86d1-4fb2-9e19-5fdf2d49ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.24998437519531e-05"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentile_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "d4c929b8-5915-49fe-b339-209a12e87abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio_identity</th>\n",
       "      <th>amino_acid</th>\n",
       "      <th>productive_frequency</th>\n",
       "      <th>v_gene</th>\n",
       "      <th>j_gene</th>\n",
       "      <th>v_family</th>\n",
       "      <th>j_family</th>\n",
       "      <th>v_resolved</th>\n",
       "      <th>j_resolved</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [bio_identity, amino_acid, productive_frequency, v_gene, j_gene, v_family, j_family, v_resolved, j_resolved, ID]\n",
       "Index: []"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_percentile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "50b7f551-4d89-4c12-b9f9-8f1da77cb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rep1[['bio_identity', 'amino_acid', 'productive_frequency', 'v_gene', 'j_gene', 'v_family', 'j_family', 'v_resolved', 'j_resolved']]\n",
    "df = df.query('amino_acid != \"na\" and productive_frequency !=\"na\"')\n",
    "df['productive_frequency'] = df['productive_frequency'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da97c8-43fc-43ac-9ba5-08af526d4726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NetTCR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0633bd9d-ac8e-4f4c-83c0-1f1aeca290b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarci_annotated_new_df.csv  new_train_df.csv\n",
      "\u001b[34mcovid_controls\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/NetTCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b05f47b1-f652-4f68-8dae-dddc5046cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']:\n",
    "    nettcr_train[f'len_{c}'] = nettcr_train[c].apply(len)\n",
    "    nettcr_train[f'flag_{c}'] = nettcr_train[c].apply(lambda z:any(['X' in x or '-' in x or '*' in x for x in z]))\n",
    "\n",
    "lens = [f'len_{c}' for c in ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']] \n",
    "flags = [f'flag_{c}' for c in ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bca09-7bb5-43d9-a3f9-6ab6524ac65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90828d28-4e24-4d92-8f1f-074b56b39f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_A1</th>\n",
       "      <th>len_A2</th>\n",
       "      <th>len_A3</th>\n",
       "      <th>len_B1</th>\n",
       "      <th>len_B2</th>\n",
       "      <th>len_B3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28088.000000</td>\n",
       "      <td>28088.000000</td>\n",
       "      <td>28088.000000</td>\n",
       "      <td>28088.00000</td>\n",
       "      <td>28088.000000</td>\n",
       "      <td>28088.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.021967</td>\n",
       "      <td>6.690188</td>\n",
       "      <td>13.598085</td>\n",
       "      <td>9.07124</td>\n",
       "      <td>18.442467</td>\n",
       "      <td>14.394617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.596134</td>\n",
       "      <td>0.904765</td>\n",
       "      <td>1.960316</td>\n",
       "      <td>0.25806</td>\n",
       "      <td>0.669436</td>\n",
       "      <td>1.688912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             len_A1        len_A2        len_A3       len_B1        len_B2  \\\n",
       "count  28088.000000  28088.000000  28088.000000  28088.00000  28088.000000   \n",
       "mean       9.021967      6.690188     13.598085      9.07124     18.442467   \n",
       "std        0.596134      0.904765      1.960316      0.25806      0.669436   \n",
       "min        8.000000      3.000000      6.000000      8.00000     17.000000   \n",
       "25%        9.000000      6.000000     12.000000      9.00000     18.000000   \n",
       "50%        9.000000      7.000000     14.000000      9.00000     18.000000   \n",
       "75%        9.000000      7.000000     15.000000      9.00000     19.000000   \n",
       "max       10.000000      8.000000     20.000000     10.00000     20.000000   \n",
       "\n",
       "             len_B3  \n",
       "count  28088.000000  \n",
       "mean      14.394617  \n",
       "std        1.688912  \n",
       "min        7.000000  \n",
       "25%       13.000000  \n",
       "50%       14.000000  \n",
       "75%       15.000000  \n",
       "max       20.000000  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nettcr_train[[x for x in nettcr_train.columns if 'len' in x]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33239a58-fea4-4248-8f1d-f09d4d9f3b19",
   "metadata": {},
   "source": [
    "## Checking immrep negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "a96ec0d4-3095-418f-ac55-965705053c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TCR_COV1-memCD8.tsv\n",
      "strip_a 263\n",
      "strip_b 815\n",
      "strip_a2 1\n",
      "strip_b2 0\n"
     ]
    }
   ],
   "source": [
    "dir = '../data/immrep/'\n",
    "test=pd.read_csv(f'{dir}TCR_COV7_memCD8.tsv', sep = '\\t')\n",
    "test['strip_a'] = test['TRA_CDR3'].apply(lambda x: x.lstrip('C').rstrip('F'))\n",
    "test['strip_b'] = test['TRB_CDR3'].apply(lambda x: x.lstrip('C').rstrip('F'))\n",
    "test['strip_a2'] = test['TRA_CDR3'].apply(lambda x: x[1:-1])\n",
    "test['strip_b2'] = test['TRB_CDR3'].apply(lambda x: x[1:-1])\n",
    "print('\\n',f)\n",
    "print('strip_a', len(test.loc[~(test['strip_a']==test['A3'])][['TRA_CDR3','strip_a', 'A3']]))\n",
    "print('strip_b', len(test.loc[~(test['strip_b']==test['B3'])][['TRB_CDR3','strip_b', 'B3']]))\n",
    "print('strip_a2', len(test.loc[~(test['strip_a2']==test['A3'])][['TRA_CDR3','strip_a2', 'A3']]))\n",
    "print('strip_b2', len(test.loc[~(test['strip_b2']==test['B3'])][['TRB_CDR3','strip_b2', 'B3']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "81d7f7d6-53e2-4a43-924a-7ee4d1c773ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRA_CDR3</th>\n",
       "      <th>strip_a2</th>\n",
       "      <th>A3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>CAMRDDKIIFGKGTRLHILPSKSTF</td>\n",
       "      <td>AMRDDKIIFGKGTRLHILPSKST</td>\n",
       "      <td>AMRDDKII</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRA_CDR3                 strip_a2        A3\n",
       "842  CAMRDDKIIFGKGTRLHILPSKSTF  AMRDDKIIFGKGTRLHILPSKST  AMRDDKII"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[~(test['strip_a2']==test['A3'])][['TRA_CDR3','strip_a2', 'A3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "ad33c505-5126-4dda-97d4-52de70264105",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TCR_COV2-memCD8.tsv\n",
      "strip_a 62\n",
      "strip_b 354\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV7_memCD8.tsv\n",
      "strip_a 263\n",
      "strip_b 815\n",
      "strip_a2 1\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV8_memCD8.tsv\n",
      "strip_a 161\n",
      "strip_b 776\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV11_memCD8.tsv\n",
      "strip_a 202\n",
      "strip_b 663\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV5-memCD8.tsv\n",
      "strip_a 222\n",
      "strip_b 742\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV12_memCD8.tsv\n",
      "strip_a 102\n",
      "strip_b 408\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV6_memCD8.tsv\n",
      "strip_a 109\n",
      "strip_b 549\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV3-memCD8.tsv\n",
      "strip_a 61\n",
      "strip_b 240\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV9_memCD8.tsv\n",
      "strip_a 120\n",
      "strip_b 430\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV10_memCD8.tsv\n",
      "strip_a 125\n",
      "strip_b 551\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV4-memCD8.tsv\n",
      "strip_a 253\n",
      "strip_b 602\n",
      "strip_a2 0\n",
      "strip_b2 0\n",
      "\n",
      " TCR_COV1-memCD8.tsv\n",
      "strip_a 29\n",
      "strip_b 249\n",
      "strip_a2 0\n",
      "strip_b2 0\n"
     ]
    }
   ],
   "source": [
    "# Seems that to get from CDR3X -> X3, you do seq[1:-1] and drop the first/last AAs\n",
    "for f in os.listdir(dir):\n",
    "    test = pd.read_csv(f'{dir}{f}', sep = '\\t')\n",
    "    test['strip_a'] = test['TRA_CDR3'].apply(lambda x: x.lstrip('C').rstrip('F'))\n",
    "    test['strip_b'] = test['TRB_CDR3'].apply(lambda x: x.lstrip('C').rstrip('F'))\n",
    "    test['strip_a2'] = test['TRA_CDR3'].apply(lambda x: x[1:-1])\n",
    "    test['strip_b2'] = test['TRB_CDR3'].apply(lambda x: x[1:-1])\n",
    "    print('\\n',f)\n",
    "    print('strip_a', len(test.loc[~(test['strip_a']==test['A3'])][['TRA_CDR3','strip_a', 'A3']]))\n",
    "    print('strip_b', len(test.loc[~(test['strip_b']==test['B3'])][['TRB_CDR3','strip_b', 'B3']]))\n",
    "    print('strip_a2', len(test.loc[~(test['strip_a2']==test['A3'])][['TRA_CDR3','strip_a2', 'A3']]))\n",
    "    print('strip_b2', len(test.loc[~(test['strip_b2']==test['B3'])][['TRB_CDR3','strip_b2', 'B3']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "6ab743f1-4099-4696-9307-dd092c054135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14864\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AERPGGKLI</td>\n",
       "      <td>SGHAT</td>\n",
       "      <td>FQNNGV</td>\n",
       "      <td>ASSYQGNEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>TRBV11-2*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TISGNEY</td>\n",
       "      <td>GLKNN</td>\n",
       "      <td>IVRVGGSQGNLI</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARGPATNEKLF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRDTTYY</td>\n",
       "      <td>RNSFDEQN</td>\n",
       "      <td>ALSDPALKAAGNKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASSFFSGGWNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV19*01</td>\n",
       "      <td>TRAJ17*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMNPNYQLI</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSAPDRPGNEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ33*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YGGTVN</td>\n",
       "      <td>YFSGDPLV</td>\n",
       "      <td>LRGLDTGFQKLV</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSPYRDSQETQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV8-1*01</td>\n",
       "      <td>TRAJ8*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        A1        A2               A3      B1       B2             B3  binder  \\\n",
       "0   NSASDY   IRSNMDK        AERPGGKLI   SGHAT   FQNNGV     ASSYQGNEAF       0   \n",
       "1  TISGNEY     GLKNN     IVRVGGSQGNLI  DFQATT  SNEGSKA   SARGPATNEKLF       0   \n",
       "2  TRDTTYY  RNSFDEQN  ALSDPALKAAGNKLT   MNHEY   SMNVEV  ASSFFSGGWNEQF       0   \n",
       "3  TSENNYY  QEAYKQQN       AFMNPNYQLI   SGDLS   YYNGEE  ASSAPDRPGNEQY       0   \n",
       "4   YGGTVN  YFSGDPLV     LRGLDTGFQKLV   SGDLS   YYNGEE  ASSPYRDSQETQY       0   \n",
       "\n",
       "    peptide original_peptide         TRAV       TRAJ         TRBV        TRBJ  \n",
       "0  true_neg         true_neg  TRAV13-2*01  TRAJ23*01  TRBV11-2*01  TRBJ1-1*01  \n",
       "1  true_neg         true_neg  TRAV26-1*01  TRAJ42*01  TRBV20-1*01  TRBJ1-4*01  \n",
       "2  true_neg         true_neg    TRAV19*01  TRAJ17*01    TRBV27*01  TRBJ2-1*01  \n",
       "3  true_neg         true_neg  TRAV38-1*01  TRAJ33*01     TRBV9*01  TRBJ2-7*01  \n",
       "4  true_neg         true_neg   TRAV8-1*01   TRAJ8*01     TRBV9*01  TRBJ2-5*01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immrep_negs = pd.read_csv('../data/NetTCR/negative_controls_95_redundancy_reduced.csv').assign(peptide='true_neg', original_peptide='true_neg')\n",
    "print(len(immrep_negs))\n",
    "\n",
    "# getting the TRA/B:V/J annotation\n",
    "immrep_annot = pd.concat([pd.read_csv(f'{dir}{f}', sep = '\\t') for f in os.listdir(dir)])[['TRAV', 'TRAJ', 'TRBV', 'TRBJ', 'A1', 'A2', 'A3', 'B1', 'B2', 'B3']]\n",
    "immrep_negs = immrep_negs.merge(immrep_annot[['TRAV', 'TRAJ', 'TRBV', 'TRBJ','A1', 'A2', 'A3', 'B1','B2','B3']], \n",
    "                              left_on=['A1', 'A2', 'A3', 'B1','B2','B3'], right_on=['A1', 'A2', 'A3', 'B1','B2','B3'])\\\n",
    "                       .drop_duplicates(['A1', 'A2', 'A3', 'B1','B2','B3', 'binder', 'original_peptide', 'peptide'])\\\n",
    "                       .reset_index(drop=True)\n",
    "display(immrep_negs.head()), print(len(immrep_negs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a01bcb-dc90-4606-b397-a907001c0f3d",
   "metadata": {},
   "source": [
    "## Getting Mathias' filtered reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "bfdac14b-7864-4257-9704-0231052b4efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37721\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>peptide</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>binder</th>\n",
       "      <th>partition</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KALYS</td>\n",
       "      <td>LLKGGEQ</td>\n",
       "      <td>GTEIGGGTSYGKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASGTETQY</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>peptide_swapped</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "      <td>2627</td>\n",
       "      <td>TRAV30*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2295</td>\n",
       "      <td>KALYS</td>\n",
       "      <td>LLKGGEQ</td>\n",
       "      <td>GTEIGGGTSYGKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASGTETQY</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>peptide_swapped</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "      <td>2627</td>\n",
       "      <td>TRAV30*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3272</td>\n",
       "      <td>KALYS</td>\n",
       "      <td>LLKGGEQ</td>\n",
       "      <td>GTEIGGGTSYGKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASGTETQY</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>peptide_swapped</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "      <td>2627</td>\n",
       "      <td>TRAV30*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10869</td>\n",
       "      <td>KALYS</td>\n",
       "      <td>LLKGGEQ</td>\n",
       "      <td>GTEIGGGTSYGKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASGTETQY</td>\n",
       "      <td>AVFDRKSDAK</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>peptide_swapped</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "      <td>2627</td>\n",
       "      <td>TRAV30*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12576</td>\n",
       "      <td>KALYS</td>\n",
       "      <td>LLKGGEQ</td>\n",
       "      <td>GTEIGGGTSYGKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASGTETQY</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>peptide_swapped</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "      <td>2627</td>\n",
       "      <td>TRAV30*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     A1       A2              A3     B1      B2        B3  \\\n",
       "0           0  KALYS  LLKGGEQ  GTEIGGGTSYGKLT  MNHEY  SMNVEV  ASGTETQY   \n",
       "1        2295  KALYS  LLKGGEQ  GTEIGGGTSYGKLT  MNHEY  SMNVEV  ASGTETQY   \n",
       "2        3272  KALYS  LLKGGEQ  GTEIGGGTSYGKLT  MNHEY  SMNVEV  ASGTETQY   \n",
       "3       10869  KALYS  LLKGGEQ  GTEIGGGTSYGKLT  MNHEY  SMNVEV  ASGTETQY   \n",
       "4       12576  KALYS  LLKGGEQ  GTEIGGGTSYGKLT  MNHEY  SMNVEV  ASGTETQY   \n",
       "\n",
       "      peptide       allele           origin  binder  partition  \\\n",
       "0   SPRWYFYYL  HLA-B*07:02  peptide_swapped       0          2   \n",
       "1   NLVPMVATV  HLA-A*02:01  peptide_swapped       0          2   \n",
       "2   GILGFVFTL  HLA-A*02:01  peptide_swapped       0          2   \n",
       "3  AVFDRKSDAK  HLA-A*11:01  peptide_swapped       0          2   \n",
       "4  ELAGIGILTV  HLA-A*02:01  peptide_swapped       0          2   \n",
       "\n",
       "  original_peptide  original_index       TRAV       TRAJ       TRBV  \\\n",
       "0        KLGGALQAK            2627  TRAV30*01  TRAJ52*01  TRBV27*01   \n",
       "1        KLGGALQAK            2627  TRAV30*01  TRAJ52*01  TRBV27*01   \n",
       "2        KLGGALQAK            2627  TRAV30*01  TRAJ52*01  TRBV27*01   \n",
       "3        KLGGALQAK            2627  TRAV30*01  TRAJ52*01  TRBV27*01   \n",
       "4        KLGGALQAK            2627  TRAV30*01  TRAJ52*01  TRBV27*01   \n",
       "\n",
       "         TRBJ  \n",
       "0  TRBJ2-5*01  \n",
       "1  TRBJ2-5*01  \n",
       "2  TRBJ2-5*01  \n",
       "3  TRBJ2-5*01  \n",
       "4  TRBJ2-5*01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nettcr_new = pd.read_csv('../data/NetTCR/nettcr_train_mathias.csv')\n",
    "print(len(nettcr_new))\n",
    "# Get the TRA/B:V/J annotation\n",
    "annot = pd.read_csv('../data/NetTCR/anarci_output.tsv', sep = '\\t')\n",
    "nettcr_new = nettcr_new.merge(annot[['TRAV', 'TRAJ', 'TRBV', 'TRBJ','A1', 'A2', 'A3', 'B1','B2','B3']], \n",
    "                              left_on=['A1', 'A2', 'A3', 'B1','B2','B3'], right_on=['A1', 'A2', 'A3', 'B1','B2','B3'])\\\n",
    "                       .drop_duplicates(['A1', 'A2', 'A3', 'B1','B2','B3', 'binder', 'partition', 'origin', 'peptide'])\\\n",
    "                       .reset_index(drop=True)\n",
    "display(nettcr_new.head()), print(len(nettcr_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "5f7874d3-a6f7-4c5f-b2ac-bbc68a4251fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def get_kfolds(df, k, xcol, ycol, shuffle=False, random_state=None):\n",
    "    \"\"\" Splits & assigns the fold numbers\n",
    "    Args:\n",
    "        df:\n",
    "        k:\n",
    "        shuffle:\n",
    "        random_state:\n",
    "\n",
    "    Returns:\n",
    "        df: df with column fold according to the Kfolds\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=shuffle, random_state=random_state)\n",
    "    df['partition'] = -1\n",
    "    for i, (train_idx, test_idx) in enumerate(kf.split(df[xcol].values, df[ycol])):\n",
    "        df.iloc[test_idx, df.columns.get_loc('partition')] = i\n",
    "    df.partition = df.partition.astype(int)\n",
    "    return df\n",
    "\n",
    "immrep_negs = get_kfolds(immrep_negs, k=5, xcol='B3', ycol='binder', shuffle=True, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7fc60aef-d452-455c-90e2-1dad52d31273",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_noswap = pd.concat([immrep_negs, nettcr_new.query('origin!=\"peptide_swapped\"')])\n",
    "merged_noswap.to_csv('../data/NetTCR/230919_nettcr_immrepnegs_noswap.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "10969674-28aa-4bc0-aac9-67598bfbdd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_wswap = pd.concat([immrep_negs, nettcr_new]).drop_duplicates(['A1', 'A2', 'A3', 'B1', 'B2', 'B3', 'peptide'])\n",
    "merged_wswap.to_csv('../data/NetTCR/230919_nettcr_immrepnegs_merged_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65536c5c-0177-41e6-8568-fe143104601b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Saving filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "b62018b3-e8fd-407c-abac-157c22246896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AERPGGKLI</td>\n",
       "      <td>SGHAT</td>\n",
       "      <td>FQNNGV</td>\n",
       "      <td>ASSYQGNEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>TRBV11-2*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TISGNEY</td>\n",
       "      <td>GLKNN</td>\n",
       "      <td>IVRVGGSQGNLI</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARGPATNEKLF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRDTTYY</td>\n",
       "      <td>RNSFDEQN</td>\n",
       "      <td>ALSDPALKAAGNKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASSFFSGGWNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV19*01</td>\n",
       "      <td>TRAJ17*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMNPNYQLI</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSAPDRPGNEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ33*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YGGTVN</td>\n",
       "      <td>YFSGDPLV</td>\n",
       "      <td>LRGLDTGFQKLV</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSPYRDSQETQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV8-1*01</td>\n",
       "      <td>TRAJ8*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        A1        A2               A3      B1       B2             B3  binder  \\\n",
       "0   NSASDY   IRSNMDK        AERPGGKLI   SGHAT   FQNNGV     ASSYQGNEAF       0   \n",
       "1  TISGNEY     GLKNN     IVRVGGSQGNLI  DFQATT  SNEGSKA   SARGPATNEKLF       0   \n",
       "2  TRDTTYY  RNSFDEQN  ALSDPALKAAGNKLT   MNHEY   SMNVEV  ASSFFSGGWNEQF       0   \n",
       "3  TSENNYY  QEAYKQQN       AFMNPNYQLI   SGDLS   YYNGEE  ASSAPDRPGNEQY       0   \n",
       "4   YGGTVN  YFSGDPLV     LRGLDTGFQKLV   SGDLS   YYNGEE  ASSPYRDSQETQY       0   \n",
       "\n",
       "    peptide original_peptide         TRAV       TRAJ         TRBV        TRBJ  \\\n",
       "0  true_neg         true_neg  TRAV13-2*01  TRAJ23*01  TRBV11-2*01  TRBJ1-1*01   \n",
       "1  true_neg         true_neg  TRAV26-1*01  TRAJ42*01  TRBV20-1*01  TRBJ1-4*01   \n",
       "2  true_neg         true_neg    TRAV19*01  TRAJ17*01    TRBV27*01  TRBJ2-1*01   \n",
       "3  true_neg         true_neg  TRAV38-1*01  TRAJ33*01     TRBV9*01  TRBJ2-7*01   \n",
       "4  true_neg         true_neg   TRAV8-1*01   TRAJ8*01     TRBV9*01  TRBJ2-5*01   \n",
       "\n",
       "   partition  Unnamed: 0 allele origin  original_index  \n",
       "0          3         NaN    NaN    NaN             NaN  \n",
       "1          2         NaN    NaN    NaN             NaN  \n",
       "2          4         NaN    NaN    NaN             NaN  \n",
       "3          1         NaN    NaN    NaN             NaN  \n",
       "4          3         NaN    NaN    NaN             NaN  "
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_noswap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "9b15b8ed-ef22-43c5-b3ee-c0820e627f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TRBV11-2*01', 'TRBV20-1*01', 'TRBV27*01', 'TRBV9*01', 'TRBV2*01',\n",
       "       'TRBV10-2*01', 'TRBV5-5*01', 'TRBV7-9*01', 'TRBV6-2*01',\n",
       "       'TRBV28*01', 'TRBV6-5*01', 'TRBV4-1*01', 'TRBV11-3*01',\n",
       "       'TRBV30*01', 'TRBV7-2*01', 'TRBV12-4*01', 'TRBV6-6*01',\n",
       "       'TRBV5-4*01', 'TRBV18*01', 'TRBV7-8*01', 'TRBV13*01', 'TRBV19*01',\n",
       "       'TRBV7-3*01', 'TRBV29-1*01', 'TRBV7-6*01', 'TRBV5-1*01',\n",
       "       'TRBV15*01', 'TRBV10-1*01', 'TRBV4-3*01', 'TRBV10-3*01',\n",
       "       'TRBV5-6*01', 'TRBV24-1*01', 'TRBV6-4*01', 'TRBV6-1*01',\n",
       "       'TRBV14*01', 'TRBV25-1*01', 'TRBV5-8*01', 'TRBV12-3*01',\n",
       "       'TRBV11-1*01', 'TRBV4-2*01', 'TRBV3-1*01', 'TRBV12-5*01',\n",
       "       'TRBV7-7*01', 'TRBV16*01', 'TRBV5-7*01', 'TRBV19*02', 'TRBV7-9*03',\n",
       "       'TRBV6-3*01', 'TRBV6-7*01', 'TRBV7-8*02', 'TRBV10-3*02',\n",
       "       'TRBV7-2*02', 'TRBV5-3*01', 'TRBV6-4*02', 'TRBV15*02', 'TRBV9*02',\n",
       "       'TRBV6-8*01', 'TRBV23-1*01', 'TRBV19*03', 'TRBV6-9*01',\n",
       "       'TRBV6-6*02'], dtype=object)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_noswap.TRBV.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99715f3-eae7-49f6-96fc-80fc6442d190",
   "metadata": {},
   "source": [
    "## Actually the gliph/deepTCR vdj and mcpas data is trash so I might as well re-filter it myself with a fresh download (230919)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "e800ee3e-b597-4c3e-bddb-366e9680eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1851 613 19013 24111\n"
     ]
    }
   ],
   "source": [
    "## OLD STUFF\n",
    "gliph_vdjdb['B3'] = gliph_vdjdb['CDR3b'].apply(lambda x: x[1:-1])\n",
    "gliph_vdjdb_outpep = gliph_vdjdb.query('B3 not in @merged_noswap.B3.values and peptide not in @merged_noswap.peptide.values')\n",
    "gliph_vdjdb_inpep = gliph_vdjdb.query('B3 not in @merged_noswap.B3.values and peptide in @merged_noswap.peptide.values')\n",
    "\n",
    "vdj_mcpas['B3'] = vdj_mcpas['cdr3'].apply(lambda x: x[1:-1])\n",
    "deeptcr_vdjmcpas_outpep = vdj_mcpas.query('B3 not in @merged_noswap.B3.values and epitope not in @merged_noswap.peptide.values').rename(columns={'cdr3':'CDR3b','epitope':'peptide'})\n",
    "deeptcr_vdjmcpas_inpep = vdj_mcpas.query('B3 not in @merged_noswap.B3.values and epitope in @merged_noswap.peptide.values').rename(columns={'cdr3':'CDR3b','epitope':'peptide'})\n",
    "\n",
    "gliph_vdjdb_outpep = gliph_vdjdb.query('B3 not in @merged_noswap.B3.values and peptide not in @merged_noswap.peptide.values')\n",
    "gliph_vdjdb_inpep = gliph_vdjdb.query('B3 not in @merged_noswap.B3.values and peptide in @merged_noswap.peptide.values')\n",
    "\n",
    "print(len(gliph_vdjdb_outpep), len(gliph_vdjdb_inpep), len(deeptcr_vdjmcpas_outpep), len(deeptcr_vdjmcpas_inpep))\n",
    "\n",
    "# Saving \"external test sets\"\n",
    "deeptcr_gliph_vmc_concat_inpep = pd.concat([deeptcr_vdjmcpas_inpep, gliph_vdjdb_inpep]).drop_duplicates(['peptide','B3'], keep='last')\n",
    "deeptcr_gliph_vmc_concat_outpep = pd.concat([deeptcr_vdjmcpas_outpep, gliph_vdjdb_outpep]).drop_duplicates(['peptide','B3'], keep='last')\n",
    "\n",
    "deeptcr_gliph_vmc_concat_inpep.to_csv('../data/filtered/230919_compiled_vdjdb_mcpas_seen_peptides.csv', index=False)\n",
    "deeptcr_gliph_vmc_concat_outpep.to_csv('../data/filtered/230919_compiled_vdjdb_mcpas_unseen_peptides.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "80044866-8b86-4e74-8b8c-5eddad81189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>V</th>\n",
       "      <th>J</th>\n",
       "      <th>peptide</th>\n",
       "      <th>B3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASTGSYGYTF</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "      <td>ASTGSYGYT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASSLTYGYTF</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "      <td>ASSLTYGYT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CASSSRTGGYGYTF</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "      <td>ASSSRTGGYGYT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CATSDRMDNEQFF</td>\n",
       "      <td>TRBV24-1*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "      <td>ATSDRMDNEQF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CASGGEFYGYTF</td>\n",
       "      <td>TRBV7-9*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>KAFSPEVIPMF</td>\n",
       "      <td>ASGGEFYGYT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>CASSLYRVGYNEQFF</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>ARMILMTHF</td>\n",
       "      <td>ASSLYRVGYNEQF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>CASCDNTGYEQYF</td>\n",
       "      <td>TRBV10-2*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>ARMILMTHF</td>\n",
       "      <td>ASCDNTGYEQY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>CASSQSSMQDPYGYTF</td>\n",
       "      <td>TRBV4-3*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>ARMILMTHF</td>\n",
       "      <td>ASSQSSMQDPYGYT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>CASNFDKGGYEQYF</td>\n",
       "      <td>TRBV2*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>ARMILMTHF</td>\n",
       "      <td>ASNFDKGGYEQY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>CSVAHTGTEQYF</td>\n",
       "      <td>TRBV29-1*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>ARMILMTHF</td>\n",
       "      <td>SVAHTGTEQY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3264 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 CDR3b            V           J      peptide              B3\n",
       "0          CASTGSYGYTF    TRBV19*01  TRBJ1-2*01  KAFSPEVIPMF       ASTGSYGYT\n",
       "1          CASSLTYGYTF    TRBV19*01  TRBJ1-2*01  KAFSPEVIPMF       ASSLTYGYT\n",
       "2       CASSSRTGGYGYTF    TRBV19*01  TRBJ1-2*01  KAFSPEVIPMF    ASSSRTGGYGYT\n",
       "3        CATSDRMDNEQFF  TRBV24-1*01  TRBJ2-1*01  KAFSPEVIPMF     ATSDRMDNEQF\n",
       "4         CASGGEFYGYTF   TRBV7-9*01  TRBJ1-2*01  KAFSPEVIPMF      ASGGEFYGYT\n",
       "...                ...          ...         ...          ...             ...\n",
       "3259   CASSLYRVGYNEQFF    TRBV27*01  TRBJ2-1*01    ARMILMTHF   ASSLYRVGYNEQF\n",
       "3260     CASCDNTGYEQYF  TRBV10-2*01  TRBJ2-7*01    ARMILMTHF     ASCDNTGYEQY\n",
       "3261  CASSQSSMQDPYGYTF   TRBV4-3*01  TRBJ1-2*01    ARMILMTHF  ASSQSSMQDPYGYT\n",
       "3262    CASNFDKGGYEQYF     TRBV2*01  TRBJ2-7*01    ARMILMTHF    ASNFDKGGYEQY\n",
       "3263      CSVAHTGTEQYF  TRBV29-1*01  TRBJ2-7*01    ARMILMTHF      SVAHTGTEQY\n",
       "\n",
       "[3264 rows x 5 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gliph_vdjdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "2beb7bcc-5f41-44de-a076-e41672d22fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>CDR3</th>\n",
       "      <th>V</th>\n",
       "      <th>J</th>\n",
       "      <th>MHC A</th>\n",
       "      <th>MHC B</th>\n",
       "      <th>MHC class</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>Epitope species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRA</td>\n",
       "      <td>CIVRAPGRADMRF</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ43*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSYLPGQGDHYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSFEAGQGFFSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gene                  CDR3            V           J     MHC A MHC B  \\\n",
       "0  TRA         CIVRAPGRADMRF  TRAV26-1*01   TRAJ43*01  HLA-B*08   B2M   \n",
       "1  TRB  CASSYLPGQGDHYSNQPQHF    TRBV13*01  TRBJ1-5*01  HLA-B*08   B2M   \n",
       "2  TRB   CASSFEAGQGFFSNQPQHF    TRBV13*01  TRBJ1-5*01  HLA-B*08   B2M   \n",
       "\n",
       "  MHC class   Epitope Epitope species  \n",
       "0      MHCI  FLKEKGGL           HIV-1  \n",
       "1      MHCI  FLKEKGGL           HIV-1  \n",
       "2      MHCI  FLKEKGGL           HIV-1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3a</th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>peptide</th>\n",
       "      <th>MHC</th>\n",
       "      <th>Tissue</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RASSPPGETQYF</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>HLA-A2</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RASSSIGGADTQYF</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "      <td>HLA-B7</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>CAASEGGFKTIF</td>\n",
       "      <td>CASSLGTGNNEQFF</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>HLA-B*8</td>\n",
       "      <td>CSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAJ9</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CDR3a           CDR3b     peptide      MHC Tissue TRAV   TRAJ  \\\n",
       "139           NaN    RASSPPGETQYF   GILGFVFTL   HLA-A2   PBMC  NaN    NaN   \n",
       "140           NaN  RASSSIGGADTQYF  LPRRSGAAGA   HLA-B7   PBMC  NaN    NaN   \n",
       "217  CAASEGGFKTIF  CASSLGTGNNEQFF    RAKFKQLL  HLA-B*8    CSF  NaN  TRAJ9   \n",
       "\n",
       "         TRBV     TRBJ  \n",
       "139   TRBV7-8  TRBJ2-5  \n",
       "140   TRBV7-8  TRBJ2-3  \n",
       "217  TRBV11-2  TRBJ2-1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vdjdb = pd.read_csv('../data/redownload/VDJdb_2023_09_19_1500.tsv', sep = '\\t')[['Gene', 'CDR3', 'V', 'J', 'MHC A', 'MHC B', 'MHC class', 'Epitope', 'Epitope species']]\n",
    "mcpas = pd.read_csv('../data/redownload/McPas_2023_09_19_1536_CD8_noallergy.csv').dropna(subset=['Epitope.peptide'])[['CDR3.alpha.aa', 'CDR3.beta.aa', 'Epitope.peptide', 'MHC', 'Tissue', 'TRAV', 'TRAJ', 'TRBV', 'TRBJ']]\\\n",
    "          .rename(columns = {'CDR3.alpha.aa':'CDR3a', 'CDR3.beta.aa':'CDR3b', 'Epitope.peptide':'peptide'})\n",
    "display(vdjdb.head(3))\n",
    "display(mcpas.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "b6f51ecd-0cb3-46db-bec2-3998145257f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7454 38100\n"
     ]
    }
   ],
   "source": [
    "mcpas_filter = mcpas.dropna(subset=['CDR3b']).dropna(subset=['TRBV', 'TRBJ'])\n",
    "vdjdb_filter = vdjdb.query('Gene==\"TRB\"').rename(columns={'CDR3':'CDR3b', 'V':'TRBV', 'J':'TRBJ'}).dropna(subset=['TRBV', 'TRBJ'])\n",
    "# mcpas_filter['A3'] = mcpas_filter['CDR3a'].apply(lambda x: x[1:-1])\n",
    "mcpas_filter['B3'] = mcpas_filter['CDR3b'].apply(lambda x: x[1:-1])\n",
    "vdjdb_filter['B3'] = vdjdb_filter['CDR3b'].apply(lambda x: x[1:-1])\n",
    "mcpas_filter = mcpas_filter.query('B3 not in @merged_wswap.B3.values')\n",
    "vdjdb_filter = vdjdb_filter.query('B3 not in @merged_wswap.B3.values')\n",
    "print(len(mcpas_filter), len(vdjdb_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "1510e140-bccd-4779-9dd9-66974fcb6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcpas_filter.drop(columns= [x for x in mcpas_filter.columns if x.endswith('_family')], inplace=True)\n",
    "vdjdb_filter.drop(columns= [x for x in vdjdb_filter.columns if x.endswith('_family')], inplace=True)\n",
    "merged_noswap.drop(columns= [x for x in merged_noswap.columns if x.endswith('_family')], inplace=True)\n",
    "merged_wswap.drop(columns= [x for x in merged_wswap.columns if x.endswith('_family')], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "55e0325b-a593-4f42-a2cf-b92416a675b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gene family to make it easier to encode...\n",
    "mcpas_filter['TRBV_gene'] = mcpas_filter['TRBV'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "mcpas_filter['TRBJ_gene'] = mcpas_filter['TRBJ'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "vdjdb_filter['TRBV_gene'] = vdjdb_filter['TRBV'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "vdjdb_filter['TRBJ_gene'] = vdjdb_filter['TRBJ'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "merged_noswap['TRBV_gene'] = merged_noswap['TRBV'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "merged_noswap['TRBJ_gene'] = merged_noswap['TRBJ'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "merged_wswap['TRBV_gene'] = merged_wswap['TRBV'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])\n",
    "merged_wswap['TRBJ_gene'] = merged_wswap['TRBJ'].apply(lambda x: x.split('*')[0].split(':')[0].split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "24baf754-31a3-401e-8ef9-cd85ed5ff5a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRBV10-1',\n",
       " 'TRBV10-2',\n",
       " 'TRBV10-3',\n",
       " 'TRBV11-1',\n",
       " 'TRBV11-2',\n",
       " 'TRBV11-3',\n",
       " 'TRBV12-3',\n",
       " 'TRBV12-4',\n",
       " 'TRBV12-5',\n",
       " 'TRBV13',\n",
       " 'TRBV14',\n",
       " 'TRBV15',\n",
       " 'TRBV16',\n",
       " 'TRBV18',\n",
       " 'TRBV19',\n",
       " 'TRBV2',\n",
       " 'TRBV20-1',\n",
       " 'TRBV23-1',\n",
       " 'TRBV24-1',\n",
       " 'TRBV25-1',\n",
       " 'TRBV27',\n",
       " 'TRBV28',\n",
       " 'TRBV29-1',\n",
       " 'TRBV3-1',\n",
       " 'TRBV30',\n",
       " 'TRBV4-1',\n",
       " 'TRBV4-2',\n",
       " 'TRBV4-3',\n",
       " 'TRBV5-1',\n",
       " 'TRBV5-3',\n",
       " 'TRBV5-4',\n",
       " 'TRBV5-5',\n",
       " 'TRBV5-6',\n",
       " 'TRBV5-7',\n",
       " 'TRBV5-8',\n",
       " 'TRBV6-1',\n",
       " 'TRBV6-2',\n",
       " 'TRBV6-3',\n",
       " 'TRBV6-4',\n",
       " 'TRBV6-5',\n",
       " 'TRBV6-6',\n",
       " 'TRBV6-7',\n",
       " 'TRBV6-8',\n",
       " 'TRBV6-9',\n",
       " 'TRBV7-2',\n",
       " 'TRBV7-3',\n",
       " 'TRBV7-6',\n",
       " 'TRBV7-7',\n",
       " 'TRBV7-8',\n",
       " 'TRBV7-9',\n",
       " 'TRBV9']"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(merged_noswap.TRBV_gene.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "f2536662-de06-4154-a6ed-f01df284a358",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRBV10-1',\n",
       " 'TRBV10-2',\n",
       " 'TRBV10-3',\n",
       " 'TRBV11-1',\n",
       " 'TRBV11-2',\n",
       " 'TRBV11-3',\n",
       " 'TRBV12-3',\n",
       " 'TRBV12-4',\n",
       " 'TRBV12-5',\n",
       " 'TRBV13',\n",
       " 'TRBV14',\n",
       " 'TRBV15',\n",
       " 'TRBV16',\n",
       " 'TRBV18',\n",
       " 'TRBV19',\n",
       " 'TRBV2',\n",
       " 'TRBV20-1',\n",
       " 'TRBV24-1',\n",
       " 'TRBV25-1',\n",
       " 'TRBV27',\n",
       " 'TRBV28',\n",
       " 'TRBV29-1',\n",
       " 'TRBV3-1',\n",
       " 'TRBV30',\n",
       " 'TRBV4-1',\n",
       " 'TRBV4-2',\n",
       " 'TRBV4-3',\n",
       " 'TRBV5-1',\n",
       " 'TRBV5-4',\n",
       " 'TRBV5-5',\n",
       " 'TRBV5-6',\n",
       " 'TRBV5-8',\n",
       " 'TRBV6-1',\n",
       " 'TRBV6-2',\n",
       " 'TRBV6-3',\n",
       " 'TRBV6-4',\n",
       " 'TRBV6-5',\n",
       " 'TRBV6-6',\n",
       " 'TRBV6-8',\n",
       " 'TRBV6-9',\n",
       " 'TRBV7-2',\n",
       " 'TRBV7-3',\n",
       " 'TRBV7-4',\n",
       " 'TRBV7-6',\n",
       " 'TRBV7-7',\n",
       " 'TRBV7-8',\n",
       " 'TRBV7-9',\n",
       " 'TRBV9']"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vdjdb_filter.TRBV_gene.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "96956f0b-b17c-4346-b381-4b87cbd6e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuples = [(f'{k:02}', f'{k:0}') for k in range(10)]\n",
    "map_dict = {f'{k:02}':f'{k:0}' for k in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "764dfb04-8c2a-4db7-833a-d7c4b05c12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what a piece of shit database\n",
    "mcpas_filter = mcpas_filter.query(\"TRBJ_gene not in ['Donor 13', 'Negative', 'TRAJ26', 'TRAJ30']\")\n",
    "\n",
    "# further process genes in this mcpas bullshit piece of shit database\n",
    "def rename_stupid_gene(gene):\n",
    "    gene = gene.lstrip(' ').rstrip(' ')\n",
    "    gene = gene.replace('201','20')\n",
    "    for k in range(10):\n",
    "        gene = gene.replace(f'{k:02}', f'{k:0}')\n",
    "    return gene\n",
    "mcpas_filter['TRBV_gene'] = mcpas_filter['TRBV_gene'].apply(rename_stupid_gene)    \n",
    "\n",
    "mcpas_filter['TRBJ_gene'] = mcpas_filter['TRBJ_gene'].apply(rename_stupid_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "5316e268-e3cd-4dac-89a1-e9ae16e47468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3a</th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>peptide</th>\n",
       "      <th>MHC</th>\n",
       "      <th>Tissue</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>B3</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RASSPPGETQYF</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>HLA-A2</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "      <td>ASSPPGETQY</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>NaN</td>\n",
       "      <td>RASSSIGGADTQYF</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "      <td>HLA-B7</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>ASSSIGGADTQY</td>\n",
       "      <td>TRBV7-8</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>CAASEGGFKTIF</td>\n",
       "      <td>CASSLGTGNNEQFF</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>HLA-B*8</td>\n",
       "      <td>CSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAJ9</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>ASSLGTGNNEQF</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CNARGQAITEKLFF</td>\n",
       "      <td>GLCTLVAML</td>\n",
       "      <td>HLA-A2</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "      <td>NARGQAITEKLF</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>NaN</td>\n",
       "      <td>CNVGGTYEQYF</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "      <td>HLA-B7</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>NVGGTYEQY</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CDR3a           CDR3b     peptide      MHC Tissue TRAV   TRAJ  \\\n",
       "139           NaN    RASSPPGETQYF   GILGFVFTL   HLA-A2   PBMC  NaN    NaN   \n",
       "140           NaN  RASSSIGGADTQYF  LPRRSGAAGA   HLA-B7   PBMC  NaN    NaN   \n",
       "217  CAASEGGFKTIF  CASSLGTGNNEQFF    RAKFKQLL  HLA-B*8    CSF  NaN  TRAJ9   \n",
       "249           NaN  CNARGQAITEKLFF   GLCTLVAML   HLA-A2   PBMC  NaN    NaN   \n",
       "250           NaN     CNVGGTYEQYF  LPRRSGAAGA   HLA-B7   PBMC  NaN    NaN   \n",
       "\n",
       "         TRBV     TRBJ            B3 TRBV_gene TRBJ_gene  \n",
       "139   TRBV7-8  TRBJ2-5    ASSPPGETQY   TRBV7-8   TRBJ2-5  \n",
       "140   TRBV7-8  TRBJ2-3  ASSSIGGADTQY   TRBV7-8   TRBJ2-3  \n",
       "217  TRBV11-2  TRBJ2-1  ASSLGTGNNEQF  TRBV11-2   TRBJ2-1  \n",
       "249  TRBV20-1  TRBJ1-4  NARGQAITEKLF  TRBV20-1   TRBJ1-4  \n",
       "250  TRBV29-1  TRBJ2-7     NVGGTYEQY  TRBV29-1   TRBJ2-7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>MHC A</th>\n",
       "      <th>MHC B</th>\n",
       "      <th>MHC class</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>Epitope species</th>\n",
       "      <th>B3</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSYLPGQGDHYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>ASSYLPGQGDHYSNQPQH</td>\n",
       "      <td>TRBV13</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSFEAGQGFFSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>ASSFEAGQGFFSNQPQH</td>\n",
       "      <td>TRBV13</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSFEPGQGFYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>ASSFEPGQGFYSNQPQH</td>\n",
       "      <td>TRBV13</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSYEPGQVSHYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>ASSYEPGQVSHYSNQPQH</td>\n",
       "      <td>TRBV13</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSALASLNEQFF</td>\n",
       "      <td>TRBV14*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>ASSALASLNEQF</td>\n",
       "      <td>TRBV14</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gene                 CDR3b       TRBV        TRBJ     MHC A MHC B MHC class  \\\n",
       "1  TRB  CASSYLPGQGDHYSNQPQHF  TRBV13*01  TRBJ1-5*01  HLA-B*08   B2M      MHCI   \n",
       "2  TRB   CASSFEAGQGFFSNQPQHF  TRBV13*01  TRBJ1-5*01  HLA-B*08   B2M      MHCI   \n",
       "4  TRB   CASSFEPGQGFYSNQPQHF  TRBV13*01  TRBJ1-5*01  HLA-B*08   B2M      MHCI   \n",
       "5  TRB  CASSYEPGQVSHYSNQPQHF  TRBV13*01  TRBJ1-5*01  HLA-B*08   B2M      MHCI   \n",
       "7  TRB        CASSALASLNEQFF  TRBV14*01  TRBJ2-1*01  HLA-B*08   B2M      MHCI   \n",
       "\n",
       "    Epitope Epitope species                  B3 TRBV_gene TRBJ_gene  \n",
       "1  FLKEKGGL           HIV-1  ASSYLPGQGDHYSNQPQH    TRBV13   TRBJ1-5  \n",
       "2  FLKEKGGL           HIV-1   ASSFEAGQGFFSNQPQH    TRBV13   TRBJ1-5  \n",
       "4  FLKEKGGL           HIV-1   ASSFEPGQGFYSNQPQH    TRBV13   TRBJ1-5  \n",
       "5  FLKEKGGL           HIV-1  ASSYEPGQVSHYSNQPQH    TRBV13   TRBJ1-5  \n",
       "7  FLKEKGGL           HIV-1        ASSALASLNEQF    TRBV14   TRBJ2-1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AERPGGKLI</td>\n",
       "      <td>SGHAT</td>\n",
       "      <td>FQNNGV</td>\n",
       "      <td>ASSYQGNEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>TRBV11-2*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TISGNEY</td>\n",
       "      <td>GLKNN</td>\n",
       "      <td>IVRVGGSQGNLI</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARGPATNEKLF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRDTTYY</td>\n",
       "      <td>RNSFDEQN</td>\n",
       "      <td>ALSDPALKAAGNKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASSFFSGGWNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV19*01</td>\n",
       "      <td>TRAJ17*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMNPNYQLI</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSAPDRPGNEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ33*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YGGTVN</td>\n",
       "      <td>YFSGDPLV</td>\n",
       "      <td>LRGLDTGFQKLV</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSPYRDSQETQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV8-1*01</td>\n",
       "      <td>TRAJ8*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        A1        A2               A3      B1       B2             B3  binder  \\\n",
       "0   NSASDY   IRSNMDK        AERPGGKLI   SGHAT   FQNNGV     ASSYQGNEAF       0   \n",
       "1  TISGNEY     GLKNN     IVRVGGSQGNLI  DFQATT  SNEGSKA   SARGPATNEKLF       0   \n",
       "2  TRDTTYY  RNSFDEQN  ALSDPALKAAGNKLT   MNHEY   SMNVEV  ASSFFSGGWNEQF       0   \n",
       "3  TSENNYY  QEAYKQQN       AFMNPNYQLI   SGDLS   YYNGEE  ASSAPDRPGNEQY       0   \n",
       "4   YGGTVN  YFSGDPLV     LRGLDTGFQKLV   SGDLS   YYNGEE  ASSPYRDSQETQY       0   \n",
       "\n",
       "    peptide original_peptide         TRAV       TRAJ         TRBV        TRBJ  \\\n",
       "0  true_neg         true_neg  TRAV13-2*01  TRAJ23*01  TRBV11-2*01  TRBJ1-1*01   \n",
       "1  true_neg         true_neg  TRAV26-1*01  TRAJ42*01  TRBV20-1*01  TRBJ1-4*01   \n",
       "2  true_neg         true_neg    TRAV19*01  TRAJ17*01    TRBV27*01  TRBJ2-1*01   \n",
       "3  true_neg         true_neg  TRAV38-1*01  TRAJ33*01     TRBV9*01  TRBJ2-7*01   \n",
       "4  true_neg         true_neg   TRAV8-1*01   TRAJ8*01     TRBV9*01  TRBJ2-5*01   \n",
       "\n",
       "   partition  Unnamed: 0 allele origin  original_index TRBV_gene TRBJ_gene  \n",
       "0          3         NaN    NaN    NaN             NaN  TRBV11-2   TRBJ1-1  \n",
       "1          2         NaN    NaN    NaN             NaN  TRBV20-1   TRBJ1-4  \n",
       "2          4         NaN    NaN    NaN             NaN    TRBV27   TRBJ2-1  \n",
       "3          1         NaN    NaN    NaN             NaN     TRBV9   TRBJ2-7  \n",
       "4          3         NaN    NaN    NaN             NaN     TRBV9   TRBJ2-5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AERPGGKLI</td>\n",
       "      <td>SGHAT</td>\n",
       "      <td>FQNNGV</td>\n",
       "      <td>ASSYQGNEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>TRBV11-2*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TISGNEY</td>\n",
       "      <td>GLKNN</td>\n",
       "      <td>IVRVGGSQGNLI</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARGPATNEKLF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRDTTYY</td>\n",
       "      <td>RNSFDEQN</td>\n",
       "      <td>ALSDPALKAAGNKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASSFFSGGWNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV19*01</td>\n",
       "      <td>TRAJ17*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMNPNYQLI</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSAPDRPGNEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ33*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YGGTVN</td>\n",
       "      <td>YFSGDPLV</td>\n",
       "      <td>LRGLDTGFQKLV</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSPYRDSQETQY</td>\n",
       "      <td>0</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>true_neg</td>\n",
       "      <td>TRAV8-1*01</td>\n",
       "      <td>TRAJ8*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        A1        A2               A3      B1       B2             B3  binder  \\\n",
       "0   NSASDY   IRSNMDK        AERPGGKLI   SGHAT   FQNNGV     ASSYQGNEAF       0   \n",
       "1  TISGNEY     GLKNN     IVRVGGSQGNLI  DFQATT  SNEGSKA   SARGPATNEKLF       0   \n",
       "2  TRDTTYY  RNSFDEQN  ALSDPALKAAGNKLT   MNHEY   SMNVEV  ASSFFSGGWNEQF       0   \n",
       "3  TSENNYY  QEAYKQQN       AFMNPNYQLI   SGDLS   YYNGEE  ASSAPDRPGNEQY       0   \n",
       "4   YGGTVN  YFSGDPLV     LRGLDTGFQKLV   SGDLS   YYNGEE  ASSPYRDSQETQY       0   \n",
       "\n",
       "    peptide original_peptide         TRAV       TRAJ         TRBV        TRBJ  \\\n",
       "0  true_neg         true_neg  TRAV13-2*01  TRAJ23*01  TRBV11-2*01  TRBJ1-1*01   \n",
       "1  true_neg         true_neg  TRAV26-1*01  TRAJ42*01  TRBV20-1*01  TRBJ1-4*01   \n",
       "2  true_neg         true_neg    TRAV19*01  TRAJ17*01    TRBV27*01  TRBJ2-1*01   \n",
       "3  true_neg         true_neg  TRAV38-1*01  TRAJ33*01     TRBV9*01  TRBJ2-7*01   \n",
       "4  true_neg         true_neg   TRAV8-1*01   TRAJ8*01     TRBV9*01  TRBJ2-5*01   \n",
       "\n",
       "   partition  Unnamed: 0 allele origin  original_index TRBV_gene TRBJ_gene  \n",
       "0          3         NaN    NaN    NaN             NaN  TRBV11-2   TRBJ1-1  \n",
       "1          2         NaN    NaN    NaN             NaN  TRBV20-1   TRBJ1-4  \n",
       "2          4         NaN    NaN    NaN             NaN    TRBV27   TRBJ2-1  \n",
       "3          1         NaN    NaN    NaN             NaN     TRBV9   TRBJ2-7  \n",
       "4          3         NaN    NaN    NaN             NaN     TRBV9   TRBJ2-5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mcpas_filter.head())\n",
    "display(vdjdb_filter.head())\n",
    "display(merged_noswap.head())\n",
    "display(merged_wswap.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "b3a48d33-15b0-4ffd-acdb-336775487d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 48, 51)"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mcpas_filter.TRBV_gene.unique()), len(vdjdb_filter.TRBV_gene.unique()), len(merged_noswap.TRBV_gene.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "1b7011d5-d7a8-47ff-ab48-1038d7ade1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 13, 13)"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mcpas_filter.TRBJ_gene.unique()), len(vdjdb_filter.TRBJ_gene.unique()), len(merged_noswap.TRBJ_gene.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3dc12-a6da-4286-8bcf-feeb93ae0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_productive_percentile(df, fcol = 'productive_frequency'):\n",
    "    min_val = df[fcol].min()\n",
    "    tiles=np.arange(.6, .9, 0.005)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "208dacfc-2920-4b87-9395-fe6a595ef7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2=pd.Index(vdjdb_filter.TRBJ_gene.unique())\n",
    "b2=pd.Index(merged_noswap.TRBJ_gene.unique())\n",
    "c2=pd.Index(mcpas_filter.TRBJ_gene.unique())\n",
    "mcpas_unique_trbj = [x for x in c2 if x not in b2]\n",
    "len(mcpas_unique_trbj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "67ac770d-dc19-42a0-93ed-57ff715b3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230919_compiled_vdjdb_mcpas_seen_peptides.csv\n",
      "230919_compiled_vdjdb_mcpas_unseen_peptides.csv\n",
      "230919_nettcr_immrepnegs_noswap.csv\n",
      "230919_nettcr_immrepnegs_withswap.csv\n",
      "230920_mcpas_filtered_cdr3b_vjgenes.csv\n",
      "230920_vdjdb_filtered_cdr3b_vjgenes.csv\n",
      "\u001b[34mtest\u001b[m\u001b[m/\n",
      "\u001b[34mtrain\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "8cfc9e35-14fc-4891-8bbb-e72618323283",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjdb_filter.rename(columns = {'Epitope':'peptide'}, inplace=True)\n",
    "vdjdb_filter.drop(columns = ['V_class', 'Gene'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "10ad493c-41cf-4bc1-ac99-5f989b6087b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CDR3b', 'TRBV', 'TRBJ', 'peptide', 'B3', 'TRBV_gene', 'TRBJ_gene']"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in vdjdb_filter.columns if x in mcpas_filter.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "26544caa-fa84-4a36-9ba4-ae439075333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.Index(vdjdb_filter.TRBV_gene.unique())\n",
    "b=pd.Index(merged_noswap.TRBV_gene.unique())\n",
    "c=pd.Index(mcpas_filter.TRBV_gene.unique())\n",
    "mcpas_unique_trbv = [x for x in c if x not in b]\n",
    "\n",
    "# Here, filter mcpas and vdjdb to only keep the datapoints with vj genes that exist in train data\n",
    "\n",
    "\n",
    "    \n",
    "mcpas_filter.query('TRBV_gene in @merged_noswap.TRBV_gene.unique() and TRBJ_gene in @merged_noswap.TRBJ_gene.unique()').to_csv('../data/filtered/230920_vdjdb_filtered_cdr3b_vjgenes.csv', index=False)\n",
    "vdjdb_filter.query('TRBV_gene in @merged_noswap.TRBV_gene.unique() and TRBJ_gene in @merged_noswap.TRBJ_gene.unique()').to_csv('../data/filtered/230920_mcpas_filtered_cdr3b_vjgenes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "4ae5f632-8f78-4f8f-9e1b-5c6bd42270c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AERPGGKLI</td>\n",
       "      <td>SGHAT</td>\n",
       "      <td>FQNNGV</td>\n",
       "      <td>ASSYQGNEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>TRBV11-2*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TISGNEY</td>\n",
       "      <td>GLKNN</td>\n",
       "      <td>IVRVGGSQGNLI</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARGPATNEKLF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRDTTYY</td>\n",
       "      <td>RNSFDEQN</td>\n",
       "      <td>ALSDPALKAAGNKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASSFFSGGWNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV19*01</td>\n",
       "      <td>TRAJ17*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMNPNYQLI</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSAPDRPGNEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ33*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YGGTVN</td>\n",
       "      <td>YFSGDPLV</td>\n",
       "      <td>LRGLDTGFQKLV</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSPYRDSQETQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV8-1*01</td>\n",
       "      <td>TRAJ8*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37716</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMLGAGGTSYGKLT</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SHIVND</td>\n",
       "      <td>ASSIGYYGYT</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV19*02</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>4</td>\n",
       "      <td>36260.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37717</th>\n",
       "      <td>DSASNY</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>AYGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSFRSSETQY</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV13-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>2</td>\n",
       "      <td>36817.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>10x</td>\n",
       "      <td>5082.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37718</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AENLGGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSTRATGELF</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-2*01</td>\n",
       "      <td>3</td>\n",
       "      <td>36884.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>10x</td>\n",
       "      <td>6211.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37719</th>\n",
       "      <td>SSVSVY</td>\n",
       "      <td>YLSGSTLV</td>\n",
       "      <td>AVGGDGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSIRASGVEQF</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV8-6*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>2</td>\n",
       "      <td>37310.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37720</th>\n",
       "      <td>SIFNT</td>\n",
       "      <td>LYKAGEL</td>\n",
       "      <td>AGQLAGGSQGNLI</td>\n",
       "      <td>LGHNA</td>\n",
       "      <td>YSLEER</td>\n",
       "      <td>ASSQERGGKWAYEQY</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV35*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV4-3*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>37417.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>TRBV4-3</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21217 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A1        A2               A3      B1       B2               B3  \\\n",
       "0       NSASDY   IRSNMDK        AERPGGKLI   SGHAT   FQNNGV       ASSYQGNEAF   \n",
       "1      TISGNEY     GLKNN     IVRVGGSQGNLI  DFQATT  SNEGSKA     SARGPATNEKLF   \n",
       "2      TRDTTYY  RNSFDEQN  ALSDPALKAAGNKLT   MNHEY   SMNVEV    ASSFFSGGWNEQF   \n",
       "3      TSENNYY  QEAYKQQN       AFMNPNYQLI   SGDLS   YYNGEE    ASSAPDRPGNEQY   \n",
       "4       YGGTVN  YFSGDPLV     LRGLDTGFQKLV   SGDLS   YYNGEE    ASSPYRDSQETQY   \n",
       "...        ...       ...              ...     ...      ...              ...   \n",
       "37716  TSENNYY  QEAYKQQN  AFMLGAGGTSYGKLT   LNHDA   SHIVND       ASSIGYYGYT   \n",
       "37717   DSASNY   IRSNVGE       AYGGSQGNLI   LNHDA   SQIVND      ASSFRSSETQY   \n",
       "37718   NSASDY   IRSNMDK    AENLGGGSQGNLI   LNHDA   SQIVND      ASSTRATGELF   \n",
       "37719   SSVSVY  YLSGSTLV    AVGGDGGSQGNLI   LNHDA   SQIVND     ASSIRASGVEQF   \n",
       "37720    SIFNT   LYKAGEL    AGQLAGGSQGNLI   LGHNA   YSLEER  ASSQERGGKWAYEQY   \n",
       "\n",
       "       binder      peptide original_peptide         TRAV       TRAJ  \\\n",
       "0           0  immrep_negs      immrep_negs  TRAV13-2*01  TRAJ23*01   \n",
       "1           0  immrep_negs      immrep_negs  TRAV26-1*01  TRAJ42*01   \n",
       "2           0  immrep_negs      immrep_negs    TRAV19*01  TRAJ17*01   \n",
       "3           0  immrep_negs      immrep_negs  TRAV38-1*01  TRAJ33*01   \n",
       "4           0  immrep_negs      immrep_negs   TRAV8-1*01   TRAJ8*01   \n",
       "...       ...          ...              ...          ...        ...   \n",
       "37716       1    GILGFVFTL        GILGFVFTL  TRAV38-1*01  TRAJ52*01   \n",
       "37717       1    GILGFVFTL        GILGFVFTL  TRAV13-1*01  TRAJ42*01   \n",
       "37718       1    GILGFVFTL        GILGFVFTL  TRAV13-2*01  TRAJ42*01   \n",
       "37719       1    GILGFVFTL        GILGFVFTL   TRAV8-6*01  TRAJ42*01   \n",
       "37720       1    GILGFVFTL        GILGFVFTL    TRAV35*01  TRAJ42*01   \n",
       "\n",
       "              TRBV        TRBJ  partition  Unnamed: 0       allele  origin  \\\n",
       "0      TRBV11-2*01  TRBJ1-1*01          3         NaN          NaN  immrep   \n",
       "1      TRBV20-1*01  TRBJ1-4*01          2         NaN          NaN  immrep   \n",
       "2        TRBV27*01  TRBJ2-1*01          4         NaN          NaN  immrep   \n",
       "3         TRBV9*01  TRBJ2-7*01          1         NaN          NaN  immrep   \n",
       "4         TRBV9*01  TRBJ2-5*01          3         NaN          NaN  immrep   \n",
       "...            ...         ...        ...         ...          ...     ...   \n",
       "37716    TRBV19*02  TRBJ1-2*01          4     36260.0  HLA-A*02:01   VDJdb   \n",
       "37717    TRBV19*01  TRBJ2-5*01          2     36817.0  HLA-A*02:01     10x   \n",
       "37718    TRBV19*01  TRBJ2-2*01          3     36884.0  HLA-A*02:01     10x   \n",
       "37719    TRBV19*01  TRBJ2-1*01          2     37310.0  HLA-A*02:01   VDJdb   \n",
       "37720   TRBV4-3*01  TRBJ2-7*01          1     37417.0  HLA-A*02:01   VDJdb   \n",
       "\n",
       "       original_index TRBV_gene TRBJ_gene  \n",
       "0                 NaN  TRBV11-2   TRBJ1-1  \n",
       "1                 NaN  TRBV20-1   TRBJ1-4  \n",
       "2                 NaN    TRBV27   TRBJ2-1  \n",
       "3                 NaN     TRBV9   TRBJ2-7  \n",
       "4                 NaN     TRBV9   TRBJ2-5  \n",
       "...               ...       ...       ...  \n",
       "37716          2580.0    TRBV19   TRBJ1-2  \n",
       "37717          5082.0    TRBV19   TRBJ2-5  \n",
       "37718          6211.0    TRBV19   TRBJ2-2  \n",
       "37719          4078.0    TRBV19   TRBJ2-1  \n",
       "37720          5236.0   TRBV4-3   TRBJ2-7  \n",
       "\n",
       "[21217 rows x 20 columns]"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_noswap.drop(columns = ['V_class', 'len'], inplace=True)\n",
    "merged_noswap = merged_noswap.replace('true_neg', 'immrep_negs')\n",
    "merged_noswap.loc[merged_noswap['origin'].isna(), 'origin']= 'immrep'\n",
    "merged_noswap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "c66a2cba-1b01-4754-8c70-1c6b57829b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AERPGGKLI</td>\n",
       "      <td>SGHAT</td>\n",
       "      <td>FQNNGV</td>\n",
       "      <td>ASSYQGNEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>TRBV11-2*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TISGNEY</td>\n",
       "      <td>GLKNN</td>\n",
       "      <td>IVRVGGSQGNLI</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARGPATNEKLF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRDTTYY</td>\n",
       "      <td>RNSFDEQN</td>\n",
       "      <td>ALSDPALKAAGNKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SMNVEV</td>\n",
       "      <td>ASSFFSGGWNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV19*01</td>\n",
       "      <td>TRAJ17*01</td>\n",
       "      <td>TRBV27*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMNPNYQLI</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSAPDRPGNEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ33*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YGGTVN</td>\n",
       "      <td>YFSGDPLV</td>\n",
       "      <td>LRGLDTGFQKLV</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSPYRDSQETQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV8-1*01</td>\n",
       "      <td>TRAJ8*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37716</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMLGAGGTSYGKLT</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SHIVND</td>\n",
       "      <td>ASSIGYYGYT</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV19*02</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>4</td>\n",
       "      <td>36260.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37717</th>\n",
       "      <td>DSASNY</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>AYGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSFRSSETQY</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV13-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>2</td>\n",
       "      <td>36817.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>10x</td>\n",
       "      <td>5082.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37718</th>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AENLGGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSTRATGELF</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-2*01</td>\n",
       "      <td>3</td>\n",
       "      <td>36884.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>10x</td>\n",
       "      <td>6211.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37719</th>\n",
       "      <td>SSVSVY</td>\n",
       "      <td>YLSGSTLV</td>\n",
       "      <td>AVGGDGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSIRASGVEQF</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV8-6*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>2</td>\n",
       "      <td>37310.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37720</th>\n",
       "      <td>SIFNT</td>\n",
       "      <td>LYKAGEL</td>\n",
       "      <td>AGQLAGGSQGNLI</td>\n",
       "      <td>LGHNA</td>\n",
       "      <td>YSLEER</td>\n",
       "      <td>ASSQERGGKWAYEQY</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV35*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV4-3*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>37417.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>TRBV4-3</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21217 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A1        A2               A3      B1       B2               B3  \\\n",
       "0       NSASDY   IRSNMDK        AERPGGKLI   SGHAT   FQNNGV       ASSYQGNEAF   \n",
       "1      TISGNEY     GLKNN     IVRVGGSQGNLI  DFQATT  SNEGSKA     SARGPATNEKLF   \n",
       "2      TRDTTYY  RNSFDEQN  ALSDPALKAAGNKLT   MNHEY   SMNVEV    ASSFFSGGWNEQF   \n",
       "3      TSENNYY  QEAYKQQN       AFMNPNYQLI   SGDLS   YYNGEE    ASSAPDRPGNEQY   \n",
       "4       YGGTVN  YFSGDPLV     LRGLDTGFQKLV   SGDLS   YYNGEE    ASSPYRDSQETQY   \n",
       "...        ...       ...              ...     ...      ...              ...   \n",
       "37716  TSENNYY  QEAYKQQN  AFMLGAGGTSYGKLT   LNHDA   SHIVND       ASSIGYYGYT   \n",
       "37717   DSASNY   IRSNVGE       AYGGSQGNLI   LNHDA   SQIVND      ASSFRSSETQY   \n",
       "37718   NSASDY   IRSNMDK    AENLGGGSQGNLI   LNHDA   SQIVND      ASSTRATGELF   \n",
       "37719   SSVSVY  YLSGSTLV    AVGGDGGSQGNLI   LNHDA   SQIVND     ASSIRASGVEQF   \n",
       "37720    SIFNT   LYKAGEL    AGQLAGGSQGNLI   LGHNA   YSLEER  ASSQERGGKWAYEQY   \n",
       "\n",
       "       binder      peptide original_peptide         TRAV       TRAJ  \\\n",
       "0           0  immrep_negs      immrep_negs  TRAV13-2*01  TRAJ23*01   \n",
       "1           0  immrep_negs      immrep_negs  TRAV26-1*01  TRAJ42*01   \n",
       "2           0  immrep_negs      immrep_negs    TRAV19*01  TRAJ17*01   \n",
       "3           0  immrep_negs      immrep_negs  TRAV38-1*01  TRAJ33*01   \n",
       "4           0  immrep_negs      immrep_negs   TRAV8-1*01   TRAJ8*01   \n",
       "...       ...          ...              ...          ...        ...   \n",
       "37716       1    GILGFVFTL        GILGFVFTL  TRAV38-1*01  TRAJ52*01   \n",
       "37717       1    GILGFVFTL        GILGFVFTL  TRAV13-1*01  TRAJ42*01   \n",
       "37718       1    GILGFVFTL        GILGFVFTL  TRAV13-2*01  TRAJ42*01   \n",
       "37719       1    GILGFVFTL        GILGFVFTL   TRAV8-6*01  TRAJ42*01   \n",
       "37720       1    GILGFVFTL        GILGFVFTL    TRAV35*01  TRAJ42*01   \n",
       "\n",
       "              TRBV        TRBJ  partition  Unnamed: 0       allele  origin  \\\n",
       "0      TRBV11-2*01  TRBJ1-1*01          3         NaN          NaN  immrep   \n",
       "1      TRBV20-1*01  TRBJ1-4*01          2         NaN          NaN  immrep   \n",
       "2        TRBV27*01  TRBJ2-1*01          4         NaN          NaN  immrep   \n",
       "3         TRBV9*01  TRBJ2-7*01          1         NaN          NaN  immrep   \n",
       "4         TRBV9*01  TRBJ2-5*01          3         NaN          NaN  immrep   \n",
       "...            ...         ...        ...         ...          ...     ...   \n",
       "37716    TRBV19*02  TRBJ1-2*01          4     36260.0  HLA-A*02:01   VDJdb   \n",
       "37717    TRBV19*01  TRBJ2-5*01          2     36817.0  HLA-A*02:01     10x   \n",
       "37718    TRBV19*01  TRBJ2-2*01          3     36884.0  HLA-A*02:01     10x   \n",
       "37719    TRBV19*01  TRBJ2-1*01          2     37310.0  HLA-A*02:01   VDJdb   \n",
       "37720   TRBV4-3*01  TRBJ2-7*01          1     37417.0  HLA-A*02:01   VDJdb   \n",
       "\n",
       "       original_index TRBV_gene TRBJ_gene  \n",
       "0                 NaN  TRBV11-2   TRBJ1-1  \n",
       "1                 NaN  TRBV20-1   TRBJ1-4  \n",
       "2                 NaN    TRBV27   TRBJ2-1  \n",
       "3                 NaN     TRBV9   TRBJ2-7  \n",
       "4                 NaN     TRBV9   TRBJ2-5  \n",
       "...               ...       ...       ...  \n",
       "37716          2580.0    TRBV19   TRBJ1-2  \n",
       "37717          5082.0    TRBV19   TRBJ2-5  \n",
       "37718          6211.0    TRBV19   TRBJ2-2  \n",
       "37719          4078.0    TRBV19   TRBJ2-1  \n",
       "37720          5236.0   TRBV4-3   TRBJ2-7  \n",
       "\n",
       "[21217 rows x 20 columns]"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_wswap.drop(columns = ['V_class', 'len'], inplace=True)\n",
    "merged_wswap = merged_noswap.replace('true_neg', 'immrep_negs')\n",
    "merged_wswap.loc[merged_noswap['origin'].isna(), 'origin']= 'immrep'\n",
    "merged_wswap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "0e28b879-2200-4a7a-b974-245523b3d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdjmcpas = pd.concat([mcpas_filter, vdjdb_filter], axis=0).drop_duplicates(['B3', 'TRBV_gene', 'TRBJ_gene', 'peptide'])\n",
    "\n",
    "vdjmcpas.to_csv('../data/filtered/230921_vdjdb_mcpas_filtered_concat_cdr3b_vjgenes', index=False)\n",
    "merged_noswap.to_csv('../data/filtered/230921_nettcr_immrepnegs_noswap.csv', index=False)\n",
    "merged_wswap.to_csv('../data/filtered/230921_nettcr_immrepnegs_withswap.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "08d9b125-8710-4b26-ae4b-3a4a73bc6097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26084, 13039)"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vdjmcpas.query('peptide in @merged_noswap.peptide.values')), len(vdjmcpas.query('peptide not in @merged_noswap.peptide.values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "eeab5284-14b8-42d0-b405-9f4c18e981d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complex.id</th>\n",
       "      <th>Gene</th>\n",
       "      <th>CDR3</th>\n",
       "      <th>V</th>\n",
       "      <th>J</th>\n",
       "      <th>Species</th>\n",
       "      <th>MHC A</th>\n",
       "      <th>MHC B</th>\n",
       "      <th>MHC class</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>Epitope gene</th>\n",
       "      <th>Epitope species</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Method</th>\n",
       "      <th>Meta</th>\n",
       "      <th>CDR3fix</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TRA</td>\n",
       "      <td>CIVRAPGRADMRF</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ43*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>Nef</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>PMID:15596521</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CIVRAPGRADMRF\", \"cdr3_old\": \"CIVRAPG...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSYLPGQGDHYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>Nef</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>PMID:15596521</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CASSYLPGQGDHYSNQPQHF\", \"cdr3_old\": \"...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TRA</td>\n",
       "      <td>CAVPSGAGSYQLTF</td>\n",
       "      <td>TRAV20*01</td>\n",
       "      <td>TRAJ28*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>Nef</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>PMID:15596521</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CAVPSGAGSYQLTF\", \"cdr3_old\": \"CAVPSG...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSFEPGQGFYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>Nef</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>PMID:15596521</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CASSFEPGQGFYSNQPQHF\", \"cdr3_old\": \"C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSYEPGQVSHYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>Nef</td>\n",
       "      <td>HIV-1</td>\n",
       "      <td>PMID:15596521</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CASSYEPGQVSHYSNQPQHF\", \"cdr3_old\": \"...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61151</th>\n",
       "      <td>30371</td>\n",
       "      <td>TRA</td>\n",
       "      <td>CVVRELFSDGQKLLF</td>\n",
       "      <td>TRAV8-2*01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-DPA*01:03</td>\n",
       "      <td>HLA-DPB*04:01</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>TFEYVSQPFLMDLE</td>\n",
       "      <td>Spike</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "      <td>PMID:35750048</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD4+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CVVRELFSDGQKLLF\", \"cdr3_old\": \"CVVRE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61152</th>\n",
       "      <td>30373</td>\n",
       "      <td>TRA</td>\n",
       "      <td>CARTDSWGKLQF</td>\n",
       "      <td>TRAV21*01</td>\n",
       "      <td>TRAJ24*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "      <td>Nucleocapsid</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "      <td>PMID:35750048</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CARTDSWGKLQF\", \"cdr3_old\": \"CARTDSWG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61153</th>\n",
       "      <td>30450</td>\n",
       "      <td>TRA</td>\n",
       "      <td>LRDNAGNMLTF</td>\n",
       "      <td>TRAV12-3*01</td>\n",
       "      <td>TRAJ39*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-A*03:01</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>KTFPPTEPK</td>\n",
       "      <td>Nucleocapsid</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "      <td>PMID:35750048</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"LRDNAGNMLTF\", \"cdr3_old\": \"LRDNAGNML...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61154</th>\n",
       "      <td>30591</td>\n",
       "      <td>TRA</td>\n",
       "      <td>CSLYNNNDMRF</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ43*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-DQA1*05:01</td>\n",
       "      <td>HLA-DQB1*02:01</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>PQPELPYPQPQL</td>\n",
       "      <td>Gluten</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>PMID:33927715</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD4+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CSLYNNNDMRF\", \"cdr3_old\": \"CSLYNNNDM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61155</th>\n",
       "      <td>30592</td>\n",
       "      <td>TRA</td>\n",
       "      <td>CALSTDSWGKLQF</td>\n",
       "      <td>TRAV6*01</td>\n",
       "      <td>TRAJ24*01</td>\n",
       "      <td>HomoSapiens</td>\n",
       "      <td>HLA-DQA1*05:01</td>\n",
       "      <td>HLA-DQB1*02:01</td>\n",
       "      <td>MHCII</td>\n",
       "      <td>PQQPFPQPEQPFP</td>\n",
       "      <td>Gluten</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>PMID:33927715</td>\n",
       "      <td>{\"frequency\": \"\", \"identification\": \"tetramer-...</td>\n",
       "      <td>{\"cell.subset\": \"CD4+\", \"clone.id\": \"\", \"donor...</td>\n",
       "      <td>{\"cdr3\": \"CALSTDSWGKLQF\", \"cdr3_old\": \"CALSTDS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56664 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       complex.id Gene                  CDR3            V           J  \\\n",
       "0               1  TRA         CIVRAPGRADMRF  TRAV26-1*01   TRAJ43*01   \n",
       "1               1  TRB  CASSYLPGQGDHYSNQPQHF    TRBV13*01  TRBJ1-5*01   \n",
       "2               2  TRA        CAVPSGAGSYQLTF    TRAV20*01   TRAJ28*01   \n",
       "3               2  TRB   CASSFEPGQGFYSNQPQHF    TRBV13*01  TRBJ1-5*01   \n",
       "4               3  TRB  CASSYEPGQVSHYSNQPQHF    TRBV13*01  TRBJ1-5*01   \n",
       "...           ...  ...                   ...          ...         ...   \n",
       "61151       30371  TRA       CVVRELFSDGQKLLF   TRAV8-2*01         NaN   \n",
       "61152       30373  TRA          CARTDSWGKLQF    TRAV21*01   TRAJ24*01   \n",
       "61153       30450  TRA           LRDNAGNMLTF  TRAV12-3*01   TRAJ39*01   \n",
       "61154       30591  TRA           CSLYNNNDMRF  TRAV26-1*01   TRAJ43*01   \n",
       "61155       30592  TRA         CALSTDSWGKLQF     TRAV6*01   TRAJ24*01   \n",
       "\n",
       "           Species           MHC A           MHC B MHC class         Epitope  \\\n",
       "0      HomoSapiens        HLA-B*08             B2M      MHCI        FLKEKGGL   \n",
       "1      HomoSapiens        HLA-B*08             B2M      MHCI        FLKEKGGL   \n",
       "2      HomoSapiens        HLA-B*08             B2M      MHCI        FLKEKGGL   \n",
       "3      HomoSapiens        HLA-B*08             B2M      MHCI        FLKEKGGL   \n",
       "4      HomoSapiens        HLA-B*08             B2M      MHCI        FLKEKGGL   \n",
       "...            ...             ...             ...       ...             ...   \n",
       "61151  HomoSapiens   HLA-DPA*01:03   HLA-DPB*04:01     MHCII  TFEYVSQPFLMDLE   \n",
       "61152  HomoSapiens     HLA-B*07:02             B2M      MHCI       SPRWYFYYL   \n",
       "61153  HomoSapiens     HLA-A*03:01             B2M      MHCI       KTFPPTEPK   \n",
       "61154  HomoSapiens  HLA-DQA1*05:01  HLA-DQB1*02:01     MHCII    PQPELPYPQPQL   \n",
       "61155  HomoSapiens  HLA-DQA1*05:01  HLA-DQB1*02:01     MHCII   PQQPFPQPEQPFP   \n",
       "\n",
       "       Epitope gene Epitope species      Reference  \\\n",
       "0               Nef           HIV-1  PMID:15596521   \n",
       "1               Nef           HIV-1  PMID:15596521   \n",
       "2               Nef           HIV-1  PMID:15596521   \n",
       "3               Nef           HIV-1  PMID:15596521   \n",
       "4               Nef           HIV-1  PMID:15596521   \n",
       "...             ...             ...            ...   \n",
       "61151         Spike      SARS-CoV-2  PMID:35750048   \n",
       "61152  Nucleocapsid      SARS-CoV-2  PMID:35750048   \n",
       "61153  Nucleocapsid      SARS-CoV-2  PMID:35750048   \n",
       "61154        Gluten           Wheat  PMID:33927715   \n",
       "61155        Gluten           Wheat  PMID:33927715   \n",
       "\n",
       "                                                  Method  \\\n",
       "0      {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "1      {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "2      {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "3      {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "4      {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "...                                                  ...   \n",
       "61151  {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "61152  {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "61153  {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "61154  {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "61155  {\"frequency\": \"\", \"identification\": \"tetramer-...   \n",
       "\n",
       "                                                    Meta  \\\n",
       "0      {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "1      {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "2      {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "3      {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "4      {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "...                                                  ...   \n",
       "61151  {\"cell.subset\": \"CD4+\", \"clone.id\": \"\", \"donor...   \n",
       "61152  {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "61153  {\"cell.subset\": \"CD8+\", \"clone.id\": \"\", \"donor...   \n",
       "61154  {\"cell.subset\": \"CD4+\", \"clone.id\": \"\", \"donor...   \n",
       "61155  {\"cell.subset\": \"CD4+\", \"clone.id\": \"\", \"donor...   \n",
       "\n",
       "                                                 CDR3fix  Score  \n",
       "0      {\"cdr3\": \"CIVRAPGRADMRF\", \"cdr3_old\": \"CIVRAPG...      2  \n",
       "1      {\"cdr3\": \"CASSYLPGQGDHYSNQPQHF\", \"cdr3_old\": \"...      2  \n",
       "2      {\"cdr3\": \"CAVPSGAGSYQLTF\", \"cdr3_old\": \"CAVPSG...      2  \n",
       "3      {\"cdr3\": \"CASSFEPGQGFYSNQPQHF\", \"cdr3_old\": \"C...      2  \n",
       "4      {\"cdr3\": \"CASSYEPGQVSHYSNQPQHF\", \"cdr3_old\": \"...      2  \n",
       "...                                                  ...    ...  \n",
       "61151  {\"cdr3\": \"CVVRELFSDGQKLLF\", \"cdr3_old\": \"CVVRE...      0  \n",
       "61152  {\"cdr3\": \"CARTDSWGKLQF\", \"cdr3_old\": \"CARTDSWG...      0  \n",
       "61153  {\"cdr3\": \"LRDNAGNMLTF\", \"cdr3_old\": \"LRDNAGNML...      0  \n",
       "61154  {\"cdr3\": \"CSLYNNNDMRF\", \"cdr3_old\": \"CSLYNNNDM...      0  \n",
       "61155  {\"cdr3\": \"CALSTDSWGKLQF\", \"cdr3_old\": \"CALSTDS...      0  \n",
       "\n",
       "[56664 rows x 17 columns]"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get paired chains dataset from vdjdb\n",
    "vdjdb_paired = pd.read_csv('../data/redownload/VDJdb_paired_2023_09_21.tsv', sep = '\\t')\n",
    "vdjdb_paired.query('Species==\"HomoSapiens\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434487b0-0b12-4a76-b7f4-4b17de5f4afd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "For the model: \n",
    "In DeepTCR, same model architecture (but not shared weight) for Alpha / Beta chains, concatenate the \"features\" (extracted from the Conv Layer)\n",
    "Each gene gets it's own OneHot -> Embedding \n",
    "--> Concatenate the TCR extracted features, VDJ embedding, --> flatten --> VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186d345-16c0-44f2-98f9-a036bee8500b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afdd44-d161-49af-9296-4c28bd189b01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## init tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "55dc5e1d-e050-4d3a-a751-0a7b2c296f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = merged_noswap\n",
    "vcol = 'TRBV_gene'\n",
    "v_map = {k:v for v,k in enumerate(sorted(df[vcol].unique()))}\n",
    "df['V_class'] = df[vcol].map(v_map).astype(int)\n",
    "vdjdb_filter['V_class'] = vdjdb_filter[vcol].map(v_map).fillna(-1).astype(int)\n",
    "x_vgene = F.one_hot(torch.from_numpy(df['V_class'].values), num_classes = 51).float()\n",
    "x_vgene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "01a49fce-4edb-44b4-8916-6021595cc6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['B3'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "22f16c46-2c1d-4816-b8d1-2ca1e1c56dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing import encode_batch\n",
    "\n",
    "x_cdr3b = encode_batch(df['B3'], max_len=23, encoding='BL50LO', pad_scale=-15).flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "e261b4d0-d315-4a38-82cd-e63c0b7b495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21217, 460])"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cdr3b.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c00b24-725a-4690-9eef-3c39f75c4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "3d68d429-2e07-430c-968b-87b4f1913688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21217, 511])"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x_cdr3b.flatten(start_dim=1), x_vgene], dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcc0f0-4d92-4c7e-bc1e-17493ddd2487",
   "metadata": {},
   "source": [
    "## dataset test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "f475d857-4d58-46d6-b222-ddfd0d82a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all:\n",
    "train_df = pd.read_csv('../data/filtered/230921_nettcr_immrepnegs_noswap.csv')\n",
    "test_df = pd.read_csv('../data/filtered/230921_vdjdb_mcpas_filtered_concat_cdr3b_vjgenes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "d126aa7d-ec6f-44d4-aa78-81190b542b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import CDR3BetaDataset\n",
    "train_dataset = CDR3BetaDataset(train_df, 23, 'BL50LO', None, 'B3', True, True, 'TRBV_gene', 'TRBJ_gene', 51, 13)\n",
    "test_dataset = CDR3BetaDataset(train_df, 23, 'BL50LO', None, 'B3', True, True, 'TRBV_gene', 'TRBJ_gene', 51, 13)\n",
    "\n",
    "idx = [124, 15 ,613 ,123 ,11 ,502 ,3469 ,1294]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "490900ec-1128-46b9-b6c6-6200362d331d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>DRGSQS</td>\n",
       "      <td>IYSNGD</td>\n",
       "      <td>AVTSFSDGQKLL</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARDGRNT</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV12-2*01</td>\n",
       "      <td>TRAJ16*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TSDPSYG</td>\n",
       "      <td>QGSYDQQN</td>\n",
       "      <td>AMRERQAGTALI</td>\n",
       "      <td>MGHRA</td>\n",
       "      <td>YSYEKL</td>\n",
       "      <td>ASSQDSGYEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV14/DV4*01</td>\n",
       "      <td>TRAJ15*01</td>\n",
       "      <td>TRBV4-1*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV4-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>DSAIYN</td>\n",
       "      <td>IQSSQRE</td>\n",
       "      <td>AALTTDSWGKLQ</td>\n",
       "      <td>SQVTM</td>\n",
       "      <td>ANQGSEA</td>\n",
       "      <td>SVRNTEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV21*01</td>\n",
       "      <td>TRAJ24*01</td>\n",
       "      <td>TRBV29-1*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>TTSDR</td>\n",
       "      <td>LLSNGAV</td>\n",
       "      <td>AVPYSGGGADGLT</td>\n",
       "      <td>SGDLS</td>\n",
       "      <td>YYNGEE</td>\n",
       "      <td>ASSVSISGQGRSGYT</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV39*01</td>\n",
       "      <td>TRAJ45*01</td>\n",
       "      <td>TRBV9*01</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DSSSTY</td>\n",
       "      <td>IFSNMDM</td>\n",
       "      <td>AEKSYNTDKLI</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SVGEGT</td>\n",
       "      <td>ASSSRTVYEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV5*01</td>\n",
       "      <td>TRAJ34*01</td>\n",
       "      <td>TRBV6-2*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ATGYPS</td>\n",
       "      <td>ATKADDK</td>\n",
       "      <td>ALRVDGNQFY</td>\n",
       "      <td>PRHDT</td>\n",
       "      <td>FYEKMQ</td>\n",
       "      <td>ASSLAFSTSGANVLT</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV9-2*01</td>\n",
       "      <td>TRAJ49*01</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ2-6*01</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV13</td>\n",
       "      <td>TRBJ2-6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>DSSSTY</td>\n",
       "      <td>IFSNMDM</td>\n",
       "      <td>AESPAGYALN</td>\n",
       "      <td>SGHTA</td>\n",
       "      <td>FQGNSA</td>\n",
       "      <td>ASSQGTHNEQF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV5*01</td>\n",
       "      <td>TRAJ41*01</td>\n",
       "      <td>TRBV7-2*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV7-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>SIFNT</td>\n",
       "      <td>LYKAGEL</td>\n",
       "      <td>AGPPNTGNQFY</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSIGGGTEAF</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV35*01</td>\n",
       "      <td>TRAJ49*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A1        A2             A3      B1       B2               B3  \\\n",
       "124    DRGSQS    IYSNGD   AVTSFSDGQKLL  DFQATT  SNEGSKA         SARDGRNT   \n",
       "15    TSDPSYG  QGSYDQQN   AMRERQAGTALI   MGHRA   YSYEKL      ASSQDSGYEQY   \n",
       "613    DSAIYN   IQSSQRE   AALTTDSWGKLQ   SQVTM  ANQGSEA         SVRNTEAF   \n",
       "123     TTSDR   LLSNGAV  AVPYSGGGADGLT   SGDLS   YYNGEE  ASSVSISGQGRSGYT   \n",
       "11     DSSSTY   IFSNMDM    AEKSYNTDKLI   MNHEY   SVGEGT      ASSSRTVYEQY   \n",
       "502    ATGYPS   ATKADDK     ALRVDGNQFY   PRHDT   FYEKMQ  ASSLAFSTSGANVLT   \n",
       "3469   DSSSTY   IFSNMDM     AESPAGYALN   SGHTA   FQGNSA      ASSQGTHNEQF   \n",
       "1294    SIFNT   LYKAGEL    AGPPNTGNQFY   LNHDA   SQIVND      ASSIGGGTEAF   \n",
       "\n",
       "      binder      peptide original_peptide           TRAV       TRAJ  \\\n",
       "124        0  immrep_negs      immrep_negs    TRAV12-2*01  TRAJ16*01   \n",
       "15         0  immrep_negs      immrep_negs  TRAV14/DV4*01  TRAJ15*01   \n",
       "613        0  immrep_negs      immrep_negs      TRAV21*01  TRAJ24*01   \n",
       "123        0  immrep_negs      immrep_negs      TRAV39*01  TRAJ45*01   \n",
       "11         0  immrep_negs      immrep_negs       TRAV5*01  TRAJ34*01   \n",
       "502        0  immrep_negs      immrep_negs     TRAV9-2*01  TRAJ49*01   \n",
       "3469       0  immrep_negs      immrep_negs       TRAV5*01  TRAJ41*01   \n",
       "1294       0  immrep_negs      immrep_negs      TRAV35*01  TRAJ49*01   \n",
       "\n",
       "             TRBV        TRBJ  partition  Unnamed: 0 allele  origin  \\\n",
       "124   TRBV20-1*01  TRBJ1-2*01          0         NaN    NaN  immrep   \n",
       "15     TRBV4-1*01  TRBJ2-7*01          3         NaN    NaN  immrep   \n",
       "613   TRBV29-1*01  TRBJ1-1*01          2         NaN    NaN  immrep   \n",
       "123      TRBV9*01  TRBJ1-2*01          4         NaN    NaN  immrep   \n",
       "11     TRBV6-2*01  TRBJ2-7*01          1         NaN    NaN  immrep   \n",
       "502     TRBV13*01  TRBJ2-6*01          2         NaN    NaN  immrep   \n",
       "3469   TRBV7-2*01  TRBJ2-1*01          0         NaN    NaN  immrep   \n",
       "1294    TRBV19*01  TRBJ1-1*01          3         NaN    NaN  immrep   \n",
       "\n",
       "      original_index TRBV_gene TRBJ_gene  len  \n",
       "124              NaN  TRBV20-1   TRBJ1-2    8  \n",
       "15               NaN   TRBV4-1   TRBJ2-7   11  \n",
       "613              NaN  TRBV29-1   TRBJ1-1    8  \n",
       "123              NaN     TRBV9   TRBJ1-2   15  \n",
       "11               NaN   TRBV6-2   TRBJ2-7   11  \n",
       "502              NaN    TRBV13   TRBJ2-6   15  \n",
       "3469             NaN   TRBV7-2   TRBJ2-1   11  \n",
       "1294             NaN    TRBV19   TRBJ1-1   11  "
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "7f4a7514-6215-4cc0-a4e0-12f3718af732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>CDR3</th>\n",
       "      <th>V</th>\n",
       "      <th>J</th>\n",
       "      <th>MHC A</th>\n",
       "      <th>MHC B</th>\n",
       "      <th>MHC class</th>\n",
       "      <th>Epitope</th>\n",
       "      <th>Epitope species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRA</td>\n",
       "      <td>CIVRAPGRADMRF</td>\n",
       "      <td>TRAV26-1*01</td>\n",
       "      <td>TRAJ43*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSYLPGQGDHYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSFEAGQGFFSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRA</td>\n",
       "      <td>CAVPSGAGSYQLTF</td>\n",
       "      <td>TRAV20*01</td>\n",
       "      <td>TRAJ28*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSFEPGQGFYSNQPQHF</td>\n",
       "      <td>TRBV13*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>HLA-B*08</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>FLKEKGGL</td>\n",
       "      <td>HIV-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80183</th>\n",
       "      <td>TRB</td>\n",
       "      <td>CASSSPTVPTSGGAVGEQFF</td>\n",
       "      <td>TRBV7-2*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80184</th>\n",
       "      <td>TRA</td>\n",
       "      <td>CAVRSWETSGSRLTF</td>\n",
       "      <td>TRAV41*01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80185</th>\n",
       "      <td>TRA</td>\n",
       "      <td>CALDQEEGQKLLF</td>\n",
       "      <td>TRAV6*01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80186</th>\n",
       "      <td>TRA</td>\n",
       "      <td>CARTDSWGKLQF</td>\n",
       "      <td>TRAV21*01</td>\n",
       "      <td>TRAJ24*01</td>\n",
       "      <td>HLA-B*07:02</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80187</th>\n",
       "      <td>TRA</td>\n",
       "      <td>LRDNAGNMLTF</td>\n",
       "      <td>TRAV12-3*01</td>\n",
       "      <td>TRAJ39*01</td>\n",
       "      <td>HLA-A*03:01</td>\n",
       "      <td>B2M</td>\n",
       "      <td>MHCI</td>\n",
       "      <td>KTFPPTEPK</td>\n",
       "      <td>SARS-CoV-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80188 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Gene                  CDR3            V           J        MHC A MHC B  \\\n",
       "0      TRA         CIVRAPGRADMRF  TRAV26-1*01   TRAJ43*01     HLA-B*08   B2M   \n",
       "1      TRB  CASSYLPGQGDHYSNQPQHF    TRBV13*01  TRBJ1-5*01     HLA-B*08   B2M   \n",
       "2      TRB   CASSFEAGQGFFSNQPQHF    TRBV13*01  TRBJ1-5*01     HLA-B*08   B2M   \n",
       "3      TRA        CAVPSGAGSYQLTF    TRAV20*01   TRAJ28*01     HLA-B*08   B2M   \n",
       "4      TRB   CASSFEPGQGFYSNQPQHF    TRBV13*01  TRBJ1-5*01     HLA-B*08   B2M   \n",
       "...    ...                   ...          ...         ...          ...   ...   \n",
       "80183  TRB  CASSSPTVPTSGGAVGEQFF   TRBV7-2*01  TRBJ2-7*01  HLA-B*07:02   B2M   \n",
       "80184  TRA       CAVRSWETSGSRLTF    TRAV41*01         NaN  HLA-B*07:02   B2M   \n",
       "80185  TRA         CALDQEEGQKLLF     TRAV6*01         NaN  HLA-B*07:02   B2M   \n",
       "80186  TRA          CARTDSWGKLQF    TRAV21*01   TRAJ24*01  HLA-B*07:02   B2M   \n",
       "80187  TRA           LRDNAGNMLTF  TRAV12-3*01   TRAJ39*01  HLA-A*03:01   B2M   \n",
       "\n",
       "      MHC class    Epitope Epitope species  \n",
       "0          MHCI   FLKEKGGL           HIV-1  \n",
       "1          MHCI   FLKEKGGL           HIV-1  \n",
       "2          MHCI   FLKEKGGL           HIV-1  \n",
       "3          MHCI   FLKEKGGL           HIV-1  \n",
       "4          MHCI   FLKEKGGL           HIV-1  \n",
       "...         ...        ...             ...  \n",
       "80183      MHCI  SPRWYFYYL      SARS-CoV-2  \n",
       "80184      MHCI  SPRWYFYYL      SARS-CoV-2  \n",
       "80185      MHCI  SPRWYFYYL      SARS-CoV-2  \n",
       "80186      MHCI  SPRWYFYYL      SARS-CoV-2  \n",
       "80187      MHCI  KTFPPTEPK      SARS-CoV-2  \n",
       "\n",
       "[80188 rows x 9 columns]"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdjdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "f56b89f0-e8fa-40ae-af71-8a82332e891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDR3a</th>\n",
       "      <th>CDR3b</th>\n",
       "      <th>peptide</th>\n",
       "      <th>MHC</th>\n",
       "      <th>Tissue</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>B3</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "      <th>MHC A</th>\n",
       "      <th>MHC B</th>\n",
       "      <th>MHC class</th>\n",
       "      <th>Epitope species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>CAASEGGFKTIF</td>\n",
       "      <td>CASSLGTGNNEQFF</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>HLA-B*8</td>\n",
       "      <td>CSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRAJ9</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>ASSLGTGNNEQF</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>CASPDAGGTSYGKLT</td>\n",
       "      <td>CASLAGQGYNEQF</td>\n",
       "      <td>SAYGEPRKL</td>\n",
       "      <td>HLA-Cw* 16:01</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>TRAV2</td>\n",
       "      <td>TRAJ5-1</td>\n",
       "      <td>TRBV4</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>ASLAGQGYNEQ</td>\n",
       "      <td>TRBV4</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>CAAPQAGTALIF</td>\n",
       "      <td>CASLGAQNNEQF</td>\n",
       "      <td>AARAVFLAL</td>\n",
       "      <td>HLA-Cw* 16:01</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>TRAV8-2</td>\n",
       "      <td>TRAJ15</td>\n",
       "      <td>TRBV12</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>ASLGAQNNEQ</td>\n",
       "      <td>TRBV12</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>CASIGSGGGADGLTF</td>\n",
       "      <td>CASRLWFWALEAF</td>\n",
       "      <td>SAYGEPRKL</td>\n",
       "      <td>HLA-Cw* 16:01</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>TRAV8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV6</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>ASRLWFWALEA</td>\n",
       "      <td>TRBV6</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>CTDVSTGGFKTIF</td>\n",
       "      <td>CASSYSTGDEQYF</td>\n",
       "      <td>AARAVFLAL</td>\n",
       "      <td>HLA-Cw* 16:01</td>\n",
       "      <td>PBMC</td>\n",
       "      <td>TRAV3</td>\n",
       "      <td>TRAJ9</td>\n",
       "      <td>TRBV6</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>ASSYSTGDEQY</td>\n",
       "      <td>TRBV6</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13092</th>\n",
       "      <td>CAVEDMNSGGYQKVTF</td>\n",
       "      <td>CASKRTATYEQYF</td>\n",
       "      <td>EPLPQGQLTAY</td>\n",
       "      <td>HLA-B*35:01</td>\n",
       "      <td>Stem cells</td>\n",
       "      <td>TRAV2-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV28</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>ASKRTATYEQY</td>\n",
       "      <td>TRBV28</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13095</th>\n",
       "      <td>CAILMDSNYQLIW</td>\n",
       "      <td>CASSEDGMNTEAFF</td>\n",
       "      <td>CLGGLLTMV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>Stem cells</td>\n",
       "      <td>TRA21-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV10-2*02</td>\n",
       "      <td>TRBJ21-1</td>\n",
       "      <td>ASSEDGMNTEAF</td>\n",
       "      <td>TRBV10-2</td>\n",
       "      <td>TRBJ21-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13096</th>\n",
       "      <td>CATEGDSGYSTLTF</td>\n",
       "      <td>CASSYQGGNYGYTF</td>\n",
       "      <td>FLYALALLL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>Stem cells</td>\n",
       "      <td>TRAV17-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV6-5*01</td>\n",
       "      <td>TRBJ17-1</td>\n",
       "      <td>ASSYQGGNYGYT</td>\n",
       "      <td>TRBV6-5</td>\n",
       "      <td>TRBJ17-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13097</th>\n",
       "      <td>CATVGNSGYSTLTF</td>\n",
       "      <td>CASSKQGGNIQYF</td>\n",
       "      <td>FLYALALLL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>Stem cells</td>\n",
       "      <td>TRAV17-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV6-5*01</td>\n",
       "      <td>TRBJ17-1</td>\n",
       "      <td>ASSKQGGNIQY</td>\n",
       "      <td>TRBV6-5</td>\n",
       "      <td>TRBJ17-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13098</th>\n",
       "      <td>CAYRSAFKLTF</td>\n",
       "      <td>CAWSVPLGRREKLFF</td>\n",
       "      <td>YVLDHLIVV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>Stem cells</td>\n",
       "      <td>TRAV38-2/DV8*01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV30*01</td>\n",
       "      <td>TRBJ38-2/DV8*01</td>\n",
       "      <td>AWSVPLGRREKLF</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>TRBJ38-2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  CDR3a            CDR3b      peptide            MHC  \\\n",
       "217        CAASEGGFKTIF   CASSLGTGNNEQFF     RAKFKQLL        HLA-B*8   \n",
       "4256    CASPDAGGTSYGKLT    CASLAGQGYNEQF    SAYGEPRKL  HLA-Cw* 16:01   \n",
       "4257       CAAPQAGTALIF     CASLGAQNNEQF    AARAVFLAL  HLA-Cw* 16:01   \n",
       "4258    CASIGSGGGADGLTF    CASRLWFWALEAF    SAYGEPRKL  HLA-Cw* 16:01   \n",
       "4259      CTDVSTGGFKTIF    CASSYSTGDEQYF    AARAVFLAL  HLA-Cw* 16:01   \n",
       "...                 ...              ...          ...            ...   \n",
       "13092  CAVEDMNSGGYQKVTF    CASKRTATYEQYF  EPLPQGQLTAY    HLA-B*35:01   \n",
       "13095     CAILMDSNYQLIW   CASSEDGMNTEAFF    CLGGLLTMV    HLA-A*02:01   \n",
       "13096    CATEGDSGYSTLTF   CASSYQGGNYGYTF    FLYALALLL    HLA-A*02:01   \n",
       "13097    CATVGNSGYSTLTF    CASSKQGGNIQYF    FLYALALLL    HLA-A*02:01   \n",
       "13098       CAYRSAFKLTF  CAWSVPLGRREKLFF    YVLDHLIVV    HLA-A*02:01   \n",
       "\n",
       "           Tissue             TRAV     TRAJ         TRBV             TRBJ  \\\n",
       "217           CSF              NaN    TRAJ9     TRBV11-2          TRBJ2-1   \n",
       "4256         PBMC            TRAV2  TRAJ5-1        TRBV4          TRBJ2-1   \n",
       "4257         PBMC          TRAV8-2   TRAJ15       TRBV12          TRBJ2-1   \n",
       "4258         PBMC            TRAV8      NaN        TRBV6          TRBJ1-1   \n",
       "4259         PBMC            TRAV3    TRAJ9        TRBV6          TRBJ2-7   \n",
       "...           ...              ...      ...          ...              ...   \n",
       "13092  Stem cells         TRAV2-01      NaN       TRBV28          TRBJ2-1   \n",
       "13095  Stem cells         TRA21-01      NaN  TRBV10-2*02         TRBJ21-1   \n",
       "13096  Stem cells        TRAV17-01      NaN   TRBV6-5*01         TRBJ17-1   \n",
       "13097  Stem cells        TRAV17-01      NaN   TRBV6-5*01         TRBJ17-1   \n",
       "13098  Stem cells  TRAV38-2/DV8*01      NaN    TRBV30*01  TRBJ38-2/DV8*01   \n",
       "\n",
       "                  B3 TRBV_gene TRBJ_gene MHC A MHC B MHC class Epitope species  \n",
       "217     ASSLGTGNNEQF  TRBV11-2   TRBJ2-1   NaN   NaN       NaN             NaN  \n",
       "4256     ASLAGQGYNEQ     TRBV4   TRBJ2-1   NaN   NaN       NaN             NaN  \n",
       "4257      ASLGAQNNEQ    TRBV12   TRBJ2-1   NaN   NaN       NaN             NaN  \n",
       "4258     ASRLWFWALEA     TRBV6   TRBJ1-1   NaN   NaN       NaN             NaN  \n",
       "4259     ASSYSTGDEQY     TRBV6   TRBJ2-7   NaN   NaN       NaN             NaN  \n",
       "...              ...       ...       ...   ...   ...       ...             ...  \n",
       "13092    ASKRTATYEQY    TRBV28   TRBJ2-1   NaN   NaN       NaN             NaN  \n",
       "13095   ASSEDGMNTEAF  TRBV10-2  TRBJ21-1   NaN   NaN       NaN             NaN  \n",
       "13096   ASSYQGGNYGYT   TRBV6-5  TRBJ17-1   NaN   NaN       NaN             NaN  \n",
       "13097    ASSKQGGNIQY   TRBV6-5  TRBJ17-1   NaN   NaN       NaN             NaN  \n",
       "13098  AWSVPLGRREKLF    TRBV30  TRBJ38-2   NaN   NaN       NaN             NaN  \n",
       "\n",
       "[440 rows x 16 columns]"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdjmcpas.dropna(subset=['CDR3a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "b9054350-3d5b-4d93-9174-7218e414dec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>binder</th>\n",
       "      <th>peptide</th>\n",
       "      <th>original_peptide</th>\n",
       "      <th>TRAV</th>\n",
       "      <th>TRAJ</th>\n",
       "      <th>TRBV</th>\n",
       "      <th>TRBJ</th>\n",
       "      <th>partition</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>allele</th>\n",
       "      <th>origin</th>\n",
       "      <th>original_index</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TSINN</td>\n",
       "      <td>IRSNERE</td>\n",
       "      <td>AMPNTGTASKLT</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SVGAGI</td>\n",
       "      <td>ASRYRQSGGNTIY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV17*01</td>\n",
       "      <td>TRAJ44*01</td>\n",
       "      <td>TRBV6-5*01</td>\n",
       "      <td>TRBJ1-3*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV6-5</td>\n",
       "      <td>TRBJ1-3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SIFNT</td>\n",
       "      <td>LYKAGEL</td>\n",
       "      <td>ATYYGQNFV</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSITGGNQPQH</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV35*01</td>\n",
       "      <td>TRAJ26*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NTAFDY</td>\n",
       "      <td>IRPDVSE</td>\n",
       "      <td>AASTSGGATNKLI</td>\n",
       "      <td>SGHRS</td>\n",
       "      <td>YFSETQ</td>\n",
       "      <td>ASSLEGQGIHEQY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV23/DV6*01</td>\n",
       "      <td>TRAJ32*01</td>\n",
       "      <td>TRBV5-1*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>NSMFDY</td>\n",
       "      <td>ISSIKDK</td>\n",
       "      <td>ADSKLV</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSITGGNQPQH</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV29/DV5*01</td>\n",
       "      <td>TRAJ47*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ1-5*01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>DSSSTY</td>\n",
       "      <td>IFSNMDM</td>\n",
       "      <td>AEDSQSRLM</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>SARVGVGNTIY</td>\n",
       "      <td>0</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>immrep_negs</td>\n",
       "      <td>TRAV5*01</td>\n",
       "      <td>TRAJ31*01</td>\n",
       "      <td>TRBV20-1*01</td>\n",
       "      <td>TRBJ1-3*01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>immrep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21210</th>\n",
       "      <td>NSAFQY</td>\n",
       "      <td>TYSSGN</td>\n",
       "      <td>AMISNTGGFKTI</td>\n",
       "      <td>SGHNT</td>\n",
       "      <td>YYREEE</td>\n",
       "      <td>ASSFSVGENEKLF</td>\n",
       "      <td>1</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "      <td>TRAV12-3*01</td>\n",
       "      <td>TRAJ9*01</td>\n",
       "      <td>TRBV5-4*01</td>\n",
       "      <td>TRBJ1-4*01</td>\n",
       "      <td>2</td>\n",
       "      <td>35811.0</td>\n",
       "      <td>HLA-B*08:01</td>\n",
       "      <td>10x</td>\n",
       "      <td>1667.0</td>\n",
       "      <td>TRBV5-4</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21212</th>\n",
       "      <td>TSENNYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>AFMLGAGGTSYGKLT</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SHIVND</td>\n",
       "      <td>ASSIGYYGYT</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV38-1*01</td>\n",
       "      <td>TRAJ52*01</td>\n",
       "      <td>TRBV19*02</td>\n",
       "      <td>TRBJ1-2*01</td>\n",
       "      <td>4</td>\n",
       "      <td>36260.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21213</th>\n",
       "      <td>DSASNY</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>AYGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSFRSSETQY</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV13-1*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-5*01</td>\n",
       "      <td>2</td>\n",
       "      <td>36817.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>10x</td>\n",
       "      <td>5082.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21215</th>\n",
       "      <td>SSVSVY</td>\n",
       "      <td>YLSGSTLV</td>\n",
       "      <td>AVGGDGGSQGNLI</td>\n",
       "      <td>LNHDA</td>\n",
       "      <td>SQIVND</td>\n",
       "      <td>ASSIRASGVEQF</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV8-6*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV19*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>2</td>\n",
       "      <td>37310.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21216</th>\n",
       "      <td>SIFNT</td>\n",
       "      <td>LYKAGEL</td>\n",
       "      <td>AGQLAGGSQGNLI</td>\n",
       "      <td>LGHNA</td>\n",
       "      <td>YSLEER</td>\n",
       "      <td>ASSQERGGKWAYEQY</td>\n",
       "      <td>1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "      <td>TRAV35*01</td>\n",
       "      <td>TRAJ42*01</td>\n",
       "      <td>TRBV4-3*01</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>1</td>\n",
       "      <td>37417.0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>VDJdb</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>TRBV4-3</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2328 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A1        A2               A3      B1       B2               B3  \\\n",
       "14       TSINN   IRSNERE     AMPNTGTASKLT   MNHEY   SVGAGI    ASRYRQSGGNTIY   \n",
       "58       SIFNT   LYKAGEL        ATYYGQNFV   LNHDA   SQIVND     ASSITGGNQPQH   \n",
       "106     NTAFDY   IRPDVSE    AASTSGGATNKLI   SGHRS   YFSETQ    ASSLEGQGIHEQY   \n",
       "151     NSMFDY   ISSIKDK           ADSKLV   LNHDA   SQIVND     ASSITGGNQPQH   \n",
       "180     DSSSTY   IFSNMDM        AEDSQSRLM  DFQATT  SNEGSKA      SARVGVGNTIY   \n",
       "...        ...       ...              ...     ...      ...              ...   \n",
       "21210   NSAFQY    TYSSGN     AMISNTGGFKTI   SGHNT   YYREEE    ASSFSVGENEKLF   \n",
       "21212  TSENNYY  QEAYKQQN  AFMLGAGGTSYGKLT   LNHDA   SHIVND       ASSIGYYGYT   \n",
       "21213   DSASNY   IRSNVGE       AYGGSQGNLI   LNHDA   SQIVND      ASSFRSSETQY   \n",
       "21215   SSVSVY  YLSGSTLV    AVGGDGGSQGNLI   LNHDA   SQIVND     ASSIRASGVEQF   \n",
       "21216    SIFNT   LYKAGEL    AGQLAGGSQGNLI   LGHNA   YSLEER  ASSQERGGKWAYEQY   \n",
       "\n",
       "       binder      peptide original_peptide           TRAV       TRAJ  \\\n",
       "14          0  immrep_negs      immrep_negs      TRAV17*01  TRAJ44*01   \n",
       "58          0  immrep_negs      immrep_negs      TRAV35*01  TRAJ26*01   \n",
       "106         0  immrep_negs      immrep_negs  TRAV23/DV6*01  TRAJ32*01   \n",
       "151         0  immrep_negs      immrep_negs  TRAV29/DV5*01  TRAJ47*01   \n",
       "180         0  immrep_negs      immrep_negs       TRAV5*01  TRAJ31*01   \n",
       "...       ...          ...              ...            ...        ...   \n",
       "21210       1     RAKFKQLL         RAKFKQLL    TRAV12-3*01   TRAJ9*01   \n",
       "21212       1    GILGFVFTL        GILGFVFTL    TRAV38-1*01  TRAJ52*01   \n",
       "21213       1    GILGFVFTL        GILGFVFTL    TRAV13-1*01  TRAJ42*01   \n",
       "21215       1    GILGFVFTL        GILGFVFTL     TRAV8-6*01  TRAJ42*01   \n",
       "21216       1    GILGFVFTL        GILGFVFTL      TRAV35*01  TRAJ42*01   \n",
       "\n",
       "              TRBV        TRBJ  partition  Unnamed: 0       allele  origin  \\\n",
       "14      TRBV6-5*01  TRBJ1-3*01          1         NaN          NaN  immrep   \n",
       "58       TRBV19*01  TRBJ1-5*01          0         NaN          NaN  immrep   \n",
       "106     TRBV5-1*01  TRBJ2-7*01          1         NaN          NaN  immrep   \n",
       "151      TRBV19*01  TRBJ1-5*01          0         NaN          NaN  immrep   \n",
       "180    TRBV20-1*01  TRBJ1-3*01          0         NaN          NaN  immrep   \n",
       "...            ...         ...        ...         ...          ...     ...   \n",
       "21210   TRBV5-4*01  TRBJ1-4*01          2     35811.0  HLA-B*08:01     10x   \n",
       "21212    TRBV19*02  TRBJ1-2*01          4     36260.0  HLA-A*02:01   VDJdb   \n",
       "21213    TRBV19*01  TRBJ2-5*01          2     36817.0  HLA-A*02:01     10x   \n",
       "21215    TRBV19*01  TRBJ2-1*01          2     37310.0  HLA-A*02:01   VDJdb   \n",
       "21216   TRBV4-3*01  TRBJ2-7*01          1     37417.0  HLA-A*02:01   VDJdb   \n",
       "\n",
       "       original_index TRBV_gene TRBJ_gene  len  \n",
       "14                NaN   TRBV6-5   TRBJ1-3   13  \n",
       "58                NaN    TRBV19   TRBJ1-5   12  \n",
       "106               NaN   TRBV5-1   TRBJ2-7   13  \n",
       "151               NaN    TRBV19   TRBJ1-5   12  \n",
       "180               NaN  TRBV20-1   TRBJ1-3   11  \n",
       "...               ...       ...       ...  ...  \n",
       "21210          1667.0   TRBV5-4   TRBJ1-4   13  \n",
       "21212          2580.0    TRBV19   TRBJ1-2   10  \n",
       "21213          5082.0    TRBV19   TRBJ2-5   11  \n",
       "21215          4078.0    TRBV19   TRBJ2-1   12  \n",
       "21216          5236.0   TRBV4-3   TRBJ2-7   15  \n",
       "\n",
       "[2328 rows x 21 columns]"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.duplicated(['B3', 'TRBV_gene', 'TRBJ_gene'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "0692167a-9bd5-4b4c-ac93-9e53b83ce3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B3</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "      <th>peptide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>ATSDTQY</td>\n",
       "      <td>TRBV7-9</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>TPVTEAF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>ASSWY</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>RDNEQF</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>APKELF</td>\n",
       "      <td>TRBV5-5</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6881</th>\n",
       "      <td>SVDGF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7261</th>\n",
       "      <td>SASPHWL</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9514</th>\n",
       "      <td>AAGQPQH</td>\n",
       "      <td>TRBV6-6</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11520</th>\n",
       "      <td>ASAGH</td>\n",
       "      <td>TRBV2</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12679</th>\n",
       "      <td>ARINEQF</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13069</th>\n",
       "      <td>ARINEQF</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13095</th>\n",
       "      <td>ARINEQF</td>\n",
       "      <td>TRBV6-2</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>AVSGYT</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>immrep_negs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15641</th>\n",
       "      <td>ASSPTDN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15919</th>\n",
       "      <td>ASSPTDN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16551</th>\n",
       "      <td>SVEDL</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16895</th>\n",
       "      <td>SAHNEQF</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>AVFDRKSDAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17430</th>\n",
       "      <td>ANNCGF</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>TRBJ1-4</td>\n",
       "      <td>LLWNGPMAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17543</th>\n",
       "      <td>SASEGYT</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>RLPGVLPRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18503</th>\n",
       "      <td>ASSARD</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18517</th>\n",
       "      <td>ASSPTGF</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>AVFDRKSDAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125</th>\n",
       "      <td>SFGSYGY</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19622</th>\n",
       "      <td>SDGQAF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19810</th>\n",
       "      <td>TSIAPI</td>\n",
       "      <td>TRBV24-1</td>\n",
       "      <td>TRBJ1-6</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            B3 TRBV_gene TRBJ_gene      peptide\n",
       "1348   ATSDTQY   TRBV7-9   TRBJ2-3  immrep_negs\n",
       "2180   TPVTEAF  TRBV29-1   TRBJ1-1  immrep_negs\n",
       "2422     ASSWY    TRBV27   TRBJ2-3  immrep_negs\n",
       "4240    RDNEQF  TRBV20-1   TRBJ2-1  immrep_negs\n",
       "6586    APKELF   TRBV5-5   TRBJ2-2  immrep_negs\n",
       "6881     SVDGF  TRBV29-1   TRBJ2-1  immrep_negs\n",
       "7261   SASPHWL  TRBV20-1   TRBJ2-1  immrep_negs\n",
       "9514   AAGQPQH   TRBV6-6   TRBJ1-5  immrep_negs\n",
       "11520    ASAGH     TRBV2   TRBJ2-7  immrep_negs\n",
       "12679  ARINEQF   TRBV6-2   TRBJ2-1  immrep_negs\n",
       "13069  ARINEQF   TRBV6-2   TRBJ2-1  immrep_negs\n",
       "13095  ARINEQF   TRBV6-2   TRBJ2-1  immrep_negs\n",
       "13306   AVSGYT    TRBV30   TRBJ1-2  immrep_negs\n",
       "15641  ASSPTDN     TRBV9   TRBJ2-3    SPRWYFYYL\n",
       "15919  ASSPTDN     TRBV9   TRBJ2-3    SPRWYFYYL\n",
       "16551    SVEDL  TRBV29-1   TRBJ2-3    KLGGALQAK\n",
       "16895  SAHNEQF  TRBV20-1   TRBJ2-1   AVFDRKSDAK\n",
       "17430   ANNCGF   TRBV5-1   TRBJ1-4    LLWNGPMAV\n",
       "17543  SASEGYT  TRBV20-1   TRBJ1-2    RLPGVLPRA\n",
       "18503   ASSARD     TRBV9   TRBJ2-1    SPRWYFYYL\n",
       "18517  ASSPTGF  TRBV12-3   TRBJ2-1   AVFDRKSDAK\n",
       "19125  SFGSYGY    TRBV19   TRBJ1-2    GILGFVFTL\n",
       "19622   SDGQAF  TRBV29-1   TRBJ1-1    NLVPMVATV\n",
       "19810   TSIAPI  TRBV24-1   TRBJ1-6    GILGFVFTL"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.B3.apply(len)<8][['B3', 'TRBV_gene', 'TRBJ_gene', 'peptide']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "f0ffe52a-39df-4564-a5ad-f55279aec5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B3</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "      <th>peptide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SADAF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>GLCTLVAML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>SVLRY</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>SASGYT</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>SAERL</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>ASMTG</td>\n",
       "      <td>TRBV7-3</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>ALDGYT</td>\n",
       "      <td>TRBV30</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>LPRRSGAAGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>ASTPPG</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>KRWIIMGLNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>ASSV</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>YSEHPTFTSQY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4805</th>\n",
       "      <td>NPTA</td>\n",
       "      <td>TRBV19</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>TPRVTGGGAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5339</th>\n",
       "      <td>SGGTGEL</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-2</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7136</th>\n",
       "      <td>ASGTEAF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ5-1</td>\n",
       "      <td>HPVGEADYFEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7817</th>\n",
       "      <td>ASGTEAF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>ASRNEQY</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>ASGREQY</td>\n",
       "      <td>TRBV11-2</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8152</th>\n",
       "      <td>ASAPPQH</td>\n",
       "      <td>TRBV5-1</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8195</th>\n",
       "      <td>SVPGGYT</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>ASGSEAF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>ASTDTQY</td>\n",
       "      <td>TRBV2</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>TPQDLNTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10213</th>\n",
       "      <td>SVGNEQF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11314</th>\n",
       "      <td>SADREAF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11523</th>\n",
       "      <td>ASSQTQY</td>\n",
       "      <td>TRBV7-2</td>\n",
       "      <td>TRBJ2-5</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13079</th>\n",
       "      <td>AMGGEQF</td>\n",
       "      <td>TRBV7-9</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13945</th>\n",
       "      <td>ASSYRF</td>\n",
       "      <td>TRBV7-9</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16275</th>\n",
       "      <td>SNQPQH</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>NLVPMVATV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18249</th>\n",
       "      <td>ASGEAF</td>\n",
       "      <td>TRBV6-3</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18828</th>\n",
       "      <td>ATMVTY</td>\n",
       "      <td>TRBV24-1</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>RAKFKQLL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19015</th>\n",
       "      <td>SATYEQY</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19556</th>\n",
       "      <td>ATMVTY</td>\n",
       "      <td>TRBV24-1</td>\n",
       "      <td>TRBJ2-3</td>\n",
       "      <td>AVFDRKSDAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>SAEAEAF</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22569</th>\n",
       "      <td>SATDRF</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27785</th>\n",
       "      <td>ASSPQAF</td>\n",
       "      <td>TRBV12-3</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28065</th>\n",
       "      <td>AINNEQF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28134</th>\n",
       "      <td>ASSFRAF</td>\n",
       "      <td>TRBV27</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28363</th>\n",
       "      <td>SPDGYT</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-2</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29853</th>\n",
       "      <td>ASGGEAF</td>\n",
       "      <td>TRBV5-5</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156</th>\n",
       "      <td>ASGTEAF</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>HPVGEADYFEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35239</th>\n",
       "      <td>SNQPQH</td>\n",
       "      <td>TRBV20-1</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>ELAGIGILTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37766</th>\n",
       "      <td>SGWQPQH</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-5</td>\n",
       "      <td>YLQPRTFLL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37993</th>\n",
       "      <td>SVPEQF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>KRWIILGLNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38886</th>\n",
       "      <td>SVGNEQF</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39072</th>\n",
       "      <td>SVRPPGI</td>\n",
       "      <td>TRBV29-1</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39078</th>\n",
       "      <td>WGNTEAF</td>\n",
       "      <td>TRBV12-4</td>\n",
       "      <td>TRBJ1-1</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39092</th>\n",
       "      <td>ASSYLLW</td>\n",
       "      <td>TRBV6-1</td>\n",
       "      <td>TRBJ2-6</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39116</th>\n",
       "      <td>ASSVEQN</td>\n",
       "      <td>TRBV9</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>SPRWYFYYL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            B3 TRBV_gene TRBJ_gene      peptide\n",
       "15       SADAF  TRBV29-1   TRBJ1-1    GLCTLVAML\n",
       "73       SVLRY  TRBV29-1   TRBJ2-3    GILGFVFTL\n",
       "403     SASGYT  TRBV20-1   TRBJ1-2   LPRRSGAAGA\n",
       "471      SAERL  TRBV20-1   TRBJ1-1   LPRRSGAAGA\n",
       "3157     ASMTG   TRBV7-3   TRBJ2-5   LPRRSGAAGA\n",
       "3497    ALDGYT    TRBV30   TRBJ1-2   LPRRSGAAGA\n",
       "4431    ASTPPG    TRBV19   TRBJ1-2   KRWIIMGLNK\n",
       "4761      ASSV    TRBV27   TRBJ2-7  YSEHPTFTSQY\n",
       "4805      NPTA    TRBV19   TRBJ1-1   TPRVTGGGAM\n",
       "5339   SGGTGEL  TRBV29-1   TRBJ2-2    NLVPMVATV\n",
       "7136   ASGTEAF   TRBV6-1   TRBJ5-1  HPVGEADYFEY\n",
       "7817   ASGTEAF   TRBV6-1   TRBJ1-1   ELAGIGILTV\n",
       "7976   ASRNEQY    TRBV27   TRBJ2-7   ELAGIGILTV\n",
       "7977   ASGREQY  TRBV11-2   TRBJ2-7   ELAGIGILTV\n",
       "8152   ASAPPQH   TRBV5-1   TRBJ1-5   ELAGIGILTV\n",
       "8195   SVPGGYT  TRBV20-1   TRBJ1-2   ELAGIGILTV\n",
       "8369   ASGSEAF   TRBV6-1   TRBJ1-1   ELAGIGILTV\n",
       "9090   ASTDTQY     TRBV2   TRBJ2-3    TPQDLNTML\n",
       "10213  SVGNEQF  TRBV29-1   TRBJ2-1    NLVPMVATV\n",
       "11314  SADREAF  TRBV29-1   TRBJ1-1    NLVPMVATV\n",
       "11523  ASSQTQY   TRBV7-2   TRBJ2-5    NLVPMVATV\n",
       "13079  AMGGEQF   TRBV7-9   TRBJ2-1    NLVPMVATV\n",
       "13945   ASSYRF   TRBV7-9   TRBJ2-1    GILGFVFTL\n",
       "16275   SNQPQH  TRBV20-1   TRBJ1-5    NLVPMVATV\n",
       "18249   ASGEAF   TRBV6-3   TRBJ1-1   ELAGIGILTV\n",
       "18828   ATMVTY  TRBV24-1   TRBJ2-3     RAKFKQLL\n",
       "19015  SATYEQY  TRBV20-1   TRBJ2-7    KLGGALQAK\n",
       "19556   ATMVTY  TRBV24-1   TRBJ2-3   AVFDRKSDAK\n",
       "21415  SAEAEAF  TRBV20-1   TRBJ1-1    KLGGALQAK\n",
       "22569   SATDRF  TRBV20-1   TRBJ2-1    KLGGALQAK\n",
       "27785  ASSPQAF  TRBV12-3   TRBJ1-1    KLGGALQAK\n",
       "28065  AINNEQF   TRBV6-1   TRBJ2-1    KLGGALQAK\n",
       "28134  ASSFRAF    TRBV27   TRBJ1-1    KLGGALQAK\n",
       "28363   SPDGYT  TRBV29-1   TRBJ1-2    KLGGALQAK\n",
       "29853  ASGGEAF   TRBV5-5   TRBJ1-1    KLGGALQAK\n",
       "32156  ASGTEAF   TRBV6-1   TRBJ1-1  HPVGEADYFEY\n",
       "35239   SNQPQH  TRBV20-1   TRBJ1-5   ELAGIGILTV\n",
       "37766  SGWQPQH  TRBV29-1   TRBJ1-5    YLQPRTFLL\n",
       "37993   SVPEQF  TRBV29-1   TRBJ2-1   KRWIILGLNK\n",
       "38886  SVGNEQF  TRBV29-1   TRBJ2-1    SPRWYFYYL\n",
       "39072  SVRPPGI  TRBV29-1   TRBJ1-1    KLGGALQAK\n",
       "39078  WGNTEAF  TRBV12-4   TRBJ1-1    KLGGALQAK\n",
       "39092  ASSYLLW   TRBV6-1   TRBJ2-6    KLGGALQAK\n",
       "39116  ASSVEQN     TRBV9   TRBJ2-1    SPRWYFYYL"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df.B3.apply(len)<8][['B3', 'TRBV_gene', 'TRBJ_gene', 'peptide']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "518e2cd0-f4ac-4022-8bd5-eab4f52d0d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B3</th>\n",
       "      <th>TRBV_gene</th>\n",
       "      <th>TRBJ_gene</th>\n",
       "      <th>peptide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15108</th>\n",
       "      <td>ATILYEILLGKATLYAVLVSALEQY</td>\n",
       "      <td>TRBV15</td>\n",
       "      <td>TRBJ2-7</td>\n",
       "      <td>GILGFVFTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19047</th>\n",
       "      <td>ASSPPRVYSNGAGLAGVGWRNEQF</td>\n",
       "      <td>TRBV5-4</td>\n",
       "      <td>TRBJ2-1</td>\n",
       "      <td>AVFDRKSDAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28125</th>\n",
       "      <td>ASSWTWDAATLWGQGALGGANVLT</td>\n",
       "      <td>TRBV5-5</td>\n",
       "      <td>TRBJ2-6</td>\n",
       "      <td>KLGGALQAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              B3 TRBV_gene TRBJ_gene     peptide\n",
       "15108  ATILYEILLGKATLYAVLVSALEQY    TRBV15   TRBJ2-7   GILGFVFTL\n",
       "19047   ASSPPRVYSNGAGLAGVGWRNEQF   TRBV5-4   TRBJ2-1  AVFDRKSDAK\n",
       "28125   ASSWTWDAATLWGQGALGGANVLT   TRBV5-5   TRBJ2-6   KLGGALQAK"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[test_df.B3.apply(len)>23][['B3', 'TRBV_gene', 'TRBJ_gene', 'peptide']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "f6aa0823-621d-45e7-af1b-098245380772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 23, 4, 25)"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.B3.apply(len).min(), train_df.B3.apply(len).max(), test_df.B3.apply(len).min(), test_df.B3.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "f099cca8-0fc3-4e8f-ba1d-719cc9d8f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "06f51898-95cd-470d-b2e4-e8243413266e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 524])"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "ecfb8d2b-d6d3-487a-a8d5-e3f75bd11e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 460])"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :23*20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "45938077-b93b-40c3-b512-1392e810ea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 13])"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 524-13:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "b8cb6eac-9dae-437e-8dd2-dfd51c1a2c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 51])"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 23*20:(23*20)+51].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "bc00d973-fe6c-4783-8963-c81355d471bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(23*20)+51+13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886479bc-bb5d-41ae-9911-0710d33cdfee",
   "metadata": {},
   "source": [
    "# Full tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610e851-ae8b-4765-94f9-39ef6432b69a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## model dims tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "d4cf8693-457b-4dbe-8f72-fa6622e649b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import CDR3BetaDataset\n",
    "from torch.utils.data import DataLoader, BatchSampler, RandomSampler, SequentialSampler\n",
    "from src.models import FullFVAE\n",
    "model = FullFVAE(max_len=23, aa_dim= 20)\n",
    "train_dataset = CDR3BetaDataset(train_df, use_v=True, use_j=True)\n",
    "train_loader = train_dataset.get_dataloader(batch_size=256, sampler=SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "0c0dac62-0e47-4d39-92d4-0d88517d5c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset[:100]\n",
    "x_hat, mu, logvar = model(x)\n",
    "seq, v, j = model.slice_xhat(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "id": "99c039d3-13c7-47cc-a884-8588b79f65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq, x_v, x_j = model.slice_xhat(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "8830aeab-a0a5-4816-ae14-505b69181c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(106.2137, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss()(seq, x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "219e9fc7-be04-478a-8f29-e6074ed660ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 23, 20])"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "1de5c89c-9e3a-449d-a869-6c0bbad17b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 51])"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "fbe1a1f5-41e6-49db-a5da-c0f364340530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 23, 20])"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "e35ed202-9a4f-490f-a656-3878b7b3d78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.,  -2.,  -1.,  -2.,  -1.,  -1.,  -1.,   0.,  -2.,  -1.,  -2.,  -1.,\n",
       "          -1.,  -3.,  -1.,   1.,   0.,  -3.,  -2.,   0.],\n",
       "        [ -2.,   7.,  -1.,  -2.,  -4.,   1.,   0.,  -3.,   0.,  -4.,  -3.,   3.,\n",
       "          -2.,  -3.,  -3.,  -1.,  -1.,  -3.,  -1.,  -3.],\n",
       "        [ -2.,  -3.,  -4.,  -4.,  -2.,  -2.,  -3.,  -4.,  -3.,   2.,   5.,  -3.,\n",
       "           3.,   1.,  -4.,  -3.,  -1.,  -2.,  -1.,   1.],\n",
       "        [  0.,  -3.,  -3.,  -4.,  -1.,  -3.,  -3.,  -4.,  -4.,   4.,   1.,  -3.,\n",
       "           1.,  -1.,  -3.,  -2.,   0.,  -3.,  -1.,   5.],\n",
       "        [  5.,  -2.,  -1.,  -2.,  -1.,  -1.,  -1.,   0.,  -2.,  -1.,  -2.,  -1.,\n",
       "          -1.,  -3.,  -1.,   1.,   0.,  -3.,  -2.,   0.],\n",
       "        [ -1.,   0.,   0.,   2.,  -3.,   2.,   6.,  -3.,   0.,  -4.,  -3.,   1.,\n",
       "          -2.,  -3.,  -1.,  -1.,  -1.,  -3.,  -2.,  -3.],\n",
       "        [ -1.,  -1.,   7.,   2.,  -2.,   0.,   0.,   0.,   1.,  -3.,  -4.,   0.,\n",
       "          -2.,  -4.,  -2.,   1.,   0.,  -4.,  -2.,  -3.],\n",
       "        [ -3.,  -3.,  -4.,  -5.,  -2.,  -4.,  -3.,  -4.,  -1.,   0.,   1.,  -4.,\n",
       "           0.,   8.,  -4.,  -3.,  -2.,   1.,   4.,  -1.],\n",
       "        [  5.,  -2.,  -1.,  -2.,  -1.,  -1.,  -1.,   0.,  -2.,  -1.,  -2.,  -1.,\n",
       "          -1.,  -3.,  -1.,   1.,   0.,  -3.,  -2.,   0.],\n",
       "        [ -2.,  -3.,  -4.,  -4.,  -2.,  -2.,  -3.,  -4.,  -3.,   2.,   5.,  -3.,\n",
       "           3.,   1.,  -4.,  -3.,  -1.,  -2.,  -1.,   1.],\n",
       "        [-15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15.,\n",
       "         -15., -15., -15., -15., -15., -15., -15., -15.],\n",
       "        [-15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15.,\n",
       "         -15., -15., -15., -15., -15., -15., -15., -15.],\n",
       "        [-15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15.,\n",
       "         -15., -15., -15., -15., -15., -15., -15., -15.],\n",
       "        [-15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15.,\n",
       "         -15., -15., -15., -15., -15., -15., -15., -15.],\n",
       "        [-15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15., -15.,\n",
       "         -15., -15., -15., -15., -15., -15., -15., -15.]])"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_processing import encode\n",
    "seq = 'ARLVAENFAL'\n",
    "encode(seq, max_len=15, encoding='BL50LO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643da85-082a-4721-96b2-f43d1fc88d82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loss tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "6a657f03-8edb-44d2-a394-73a474d414ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_col = 'TRBV_gene'\n",
    "v_dim=51\n",
    "v_map = {k: v for v, k in enumerate(sorted(train_df[v_col].unique()))}\n",
    "train_df['v_class'] = train_df[v_col].map(v_map).astype(int)\n",
    "x_v = F.one_hot(torch.from_numpy(train_df['v_class'].values), num_classes=v_dim).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "0cccb164-5b78-438e-bc02-a95b062b9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "id": "e0c790bf-fee0-45d4-be17-e0257ee420ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import VAELoss\n",
    "from src.datasets import CDR3BetaDataset\n",
    "from src.models import FullFVAE\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(0)\n",
    "use_v = True\n",
    "use_j = True\n",
    "v_dim = 51\n",
    "j_dim = 13\n",
    "max_len = 23\n",
    "\n",
    "# doing a single fold for test reasons, will do normal kcv when I have stuff figured out.\n",
    "df = pd.read_csv('../data/filtered/230921_nettcr_immrepnegs_noswap.csv')\n",
    "train_df = df.query('partition!=0')\n",
    "valid_df = df.query('partition==0')\n",
    "\n",
    "train_dataset = CDR3BetaDataset(train_df, encoding='BL50LO', use_v=use_v, use_j=use_j, v_dim=v_dim, j_dim=j_dim)\n",
    "train_loader = train_dataset.get_dataloader(batch_size=512, sampler=RandomSampler)\n",
    "\n",
    "valid_dataset = CDR3BetaDataset(valid_df, encoding='BL50LO', use_v=use_v, use_j=use_j, v_dim=v_dim, j_dim=j_dim)\n",
    "valid_loader = valid_dataset.get_dataloader(batch_size=1024, sampler=SequentialSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb897d-abd4-4a8d-819d-40642bf89e37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "id": "2b08fec2-8d9b-4917-845b-0baa6ddc4ac7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5660377358490566 0.09433962264150944 0.05660377358490566 0.2830188679245283\n",
      "\n",
      "0 epochs, Train Rec:\t2.405e+01, KLD:\t5.649e-02\n",
      "seq_loss tensor(27.3967)\n",
      "v_loss tensor(0.0954)\n",
      "j_loss tensor(0.3308)\n",
      "kld_weight 0.2731132075471698\n",
      "kld_loss tensor(0.6517) \n",
      "\n",
      "seq_loss tensor(27.4751)\n",
      "v_loss tensor(0.0830)\n",
      "j_loss tensor(0.3318)\n",
      "kld_weight 0.2728301886792453\n",
      "kld_loss tensor(0.6711) \n",
      "\n",
      "seq_loss tensor(27.3524)\n",
      "v_loss tensor(0.0780)\n",
      "j_loss tensor(0.3187)\n",
      "kld_weight 0.27254716981132077\n",
      "kld_loss tensor(0.6872) \n",
      "\n",
      "seq_loss tensor(27.2461)\n",
      "v_loss tensor(0.0928)\n",
      "j_loss tensor(0.3011)\n",
      "kld_weight 0.27226415094339623\n",
      "kld_loss tensor(0.7039) \n",
      "\n",
      "seq_loss tensor(27.9227)\n",
      "v_loss tensor(0.0578)\n",
      "j_loss tensor(0.2926)\n",
      "kld_weight 0.2719811320754717\n",
      "kld_loss tensor(0.7565) \n",
      "\n",
      "\n",
      "0 epochs, Valid Rec:\t8.401e+00, KLD:\t2.092e-01\n",
      "seq_loss tensor(27.3816, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0857, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2619, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27169811320754716\n",
      "kld_loss tensor(0.7476, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(25.8211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1137, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27141509433962263\n",
      "kld_loss tensor(0.8544, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(24.2909, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2711320754716981\n",
      "kld_loss tensor(0.9627, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(23.0340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0820, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2662, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27084905660377356\n",
      "kld_loss tensor(1.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(21.8655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2666, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27056603773584903\n",
      "kld_loss tensor(1.2079, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(20.9491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1111, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2558, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2702830188679245\n",
      "kld_loss tensor(1.3282, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(20.1168, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0854, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2664, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27\n",
      "kld_loss tensor(1.4435, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(19.1754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0852, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2586, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2697169811320755\n",
      "kld_loss tensor(1.5071, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(18.1541, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0899, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2642, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26943396226415095\n",
      "kld_loss tensor(1.5841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(17.3724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0835, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2472, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2691509433962264\n",
      "kld_loss tensor(1.5854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(16.2028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0817, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2513, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2688679245283019\n",
      "kld_loss tensor(1.6280, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(15.2198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0807, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2512, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26858490566037735\n",
      "kld_loss tensor(1.6312, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(14.3628, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0805, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2339, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2683018867924528\n",
      "kld_loss tensor(1.6224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(13.5287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0976, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2368, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2680188679245283\n",
      "kld_loss tensor(1.5827, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(12.8735, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0901, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2350, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26773584905660375\n",
      "kld_loss tensor(1.5564, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(12.3251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0809, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2287, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2674528301886792\n",
      "kld_loss tensor(1.5660, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(11.8585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2266, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26716981132075474\n",
      "kld_loss tensor(1.5284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(11.1971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0856, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2201, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2668867924528302\n",
      "kld_loss tensor(1.5205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(11.0573, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0753, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2198, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26660377358490567\n",
      "kld_loss tensor(1.4752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(10.3456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0814, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2178, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26632075471698113\n",
      "kld_loss tensor(1.5184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(10.0243, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0721, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2660377358490566\n",
      "kld_loss tensor(1.5146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(9.6185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26575471698113207\n",
      "kld_loss tensor(1.5574, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(9.1204, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26547169811320753\n",
      "kld_loss tensor(1.6047, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(8.8061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.265188679245283\n",
      "kld_loss tensor(1.6407, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(8.6798, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0751, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26490566037735847\n",
      "kld_loss tensor(1.6284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(8.1923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0816, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26462264150943393\n",
      "kld_loss tensor(1.6923, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(8.0516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0754, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26433962264150945\n",
      "kld_loss tensor(1.6918, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(7.6406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0725, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2640566037735849\n",
      "kld_loss tensor(1.7485, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(7.5606, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2637735849056604\n",
      "kld_loss tensor(1.7728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(7.4037, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0863, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26349056603773585\n",
      "kld_loss tensor(1.7766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(7.3697, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0708, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2632075471698113\n",
      "kld_loss tensor(1.7409, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.9601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2629245283018868\n",
      "kld_loss tensor(1.7411, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.8283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0795, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26264150943396225\n",
      "kld_loss tensor(1.7330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(7.3480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2623584905660377\n",
      "kld_loss tensor(1.7198, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "10 epochs, Train Rec:\t1.588e+00, KLD:\t2.334e-01\n",
      "seq_loss tensor(2.7070)\n",
      "v_loss tensor(0.0675)\n",
      "j_loss tensor(0.2258)\n",
      "kld_weight 0.16273584905660377\n",
      "kld_loss tensor(0.4237) \n",
      "\n",
      "seq_loss tensor(2.7984)\n",
      "v_loss tensor(0.0607)\n",
      "j_loss tensor(0.2307)\n",
      "kld_weight 0.16245283018867923\n",
      "kld_loss tensor(0.4353) \n",
      "\n",
      "seq_loss tensor(2.7927)\n",
      "v_loss tensor(0.0584)\n",
      "j_loss tensor(0.2268)\n",
      "kld_weight 0.1621698113207547\n",
      "kld_loss tensor(0.4244) \n",
      "\n",
      "seq_loss tensor(2.8630)\n",
      "v_loss tensor(0.0693)\n",
      "j_loss tensor(0.2230)\n",
      "kld_weight 0.1618867924528302\n",
      "kld_loss tensor(0.4315) \n",
      "\n",
      "seq_loss tensor(2.6928)\n",
      "v_loss tensor(0.0467)\n",
      "j_loss tensor(0.2225)\n",
      "kld_weight 0.16160377358490566\n",
      "kld_loss tensor(0.4601) \n",
      "\n",
      "\n",
      "10 epochs, Valid Rec:\t9.214e-01, KLD:\t1.311e-01\n",
      "seq_loss tensor(2.7414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16132075471698112\n",
      "kld_loss tensor(0.4319, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.9406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2009, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16103773584905662\n",
      "kld_loss tensor(0.4652, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7630, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16075471698113208\n",
      "kld_loss tensor(0.4426, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16047169811320755\n",
      "kld_loss tensor(0.4239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0738, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2005, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16018867924528302\n",
      "kld_loss tensor(0.4524, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1983, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15990566037735848\n",
      "kld_loss tensor(0.4213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0713, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15962264150943395\n",
      "kld_loss tensor(0.4258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15933962264150942\n",
      "kld_loss tensor(0.4445, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1996, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15905660377358488\n",
      "kld_loss tensor(0.4258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7442, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15877358490566038\n",
      "kld_loss tensor(0.4194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15849056603773584\n",
      "kld_loss tensor(0.4237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.8007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15820754716981134\n",
      "kld_loss tensor(0.4080, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2003, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15792452830188677\n",
      "kld_loss tensor(0.4218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7106, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2014, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15764150943396227\n",
      "kld_loss tensor(0.4243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.5869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15735849056603773\n",
      "kld_loss tensor(0.4153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7054, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1570754716981132\n",
      "kld_loss tensor(0.4236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1965, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15679245283018867\n",
      "kld_loss tensor(0.4189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15650943396226413\n",
      "kld_loss tensor(0.4234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7661, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0702, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1977, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15622641509433963\n",
      "kld_loss tensor(0.4326, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1559433962264151\n",
      "kld_loss tensor(0.4231, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1962, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15566037735849056\n",
      "kld_loss tensor(0.4293, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.8300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0768, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2009, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15537735849056603\n",
      "kld_loss tensor(0.4143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0736, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1550943396226415\n",
      "kld_loss tensor(0.4278, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0736, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2011, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15481132075471699\n",
      "kld_loss tensor(0.4073, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0738, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15452830188679245\n",
      "kld_loss tensor(0.4127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15424528301886792\n",
      "kld_loss tensor(0.4097, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.7028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15396226415094338\n",
      "kld_loss tensor(0.4292, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15367924528301885\n",
      "kld_loss tensor(0.4100, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6608, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2005, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15339622641509432\n",
      "kld_loss tensor(0.4221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2027, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1531132075471698\n",
      "kld_loss tensor(0.4123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15283018867924528\n",
      "kld_loss tensor(0.4215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.6234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15254716981132074\n",
      "kld_loss tensor(0.4147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.5547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1999, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1522641509433962\n",
      "kld_loss tensor(0.4029, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.2982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1970, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15198113207547168\n",
      "kld_loss tensor(0.4118, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "20 epochs, Train Rec:\t1.052e+00, KLD:\t7.808e-02\n",
      "seq_loss tensor(1.7279)\n",
      "v_loss tensor(0.0677)\n",
      "j_loss tensor(0.2261)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1476) \n",
      "\n",
      "seq_loss tensor(1.7481)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2295)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1524) \n",
      "\n",
      "seq_loss tensor(1.7847)\n",
      "v_loss tensor(0.0570)\n",
      "j_loss tensor(0.2267)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1481) \n",
      "\n",
      "seq_loss tensor(1.7880)\n",
      "v_loss tensor(0.0678)\n",
      "j_loss tensor(0.2206)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1525) \n",
      "\n",
      "seq_loss tensor(1.6488)\n",
      "v_loss tensor(0.0464)\n",
      "j_loss tensor(0.2171)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1577) \n",
      "\n",
      "\n",
      "20 epochs, Valid Rec:\t6.098e-01, KLD:\t4.571e-02\n",
      "seq_loss tensor(1.7954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1960, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1542, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7742, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0702, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1534, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7771, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1554, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2002, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1485, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7064, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1507, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1505, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7751, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1508, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7649, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1518, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7517, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0727, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2010, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1481, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6689, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1498, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6782, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1518, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6822, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1958, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1477, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.8116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1944, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1517, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7852, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0715, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1498, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.8087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1512, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6749, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1957, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1514, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1511, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0736, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1507, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7709, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1962, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1537, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1477, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1545, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1503, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1498, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7102, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0712, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2003, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1508, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1523, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7207, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1507, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6708, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1525, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1482, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7399, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0736, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1987, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1513, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0693, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1965, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1510, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1531, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0735, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1997, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1551, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.7634, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0702, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1983, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1532, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0461, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1954, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1434, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "30 epochs, Train Rec:\t8.987e-01, KLD:\t7.601e-02\n",
      "seq_loss tensor(1.4550)\n",
      "v_loss tensor(0.0673)\n",
      "j_loss tensor(0.2266)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1454) \n",
      "\n",
      "seq_loss tensor(1.4598)\n",
      "v_loss tensor(0.0608)\n",
      "j_loss tensor(0.2311)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1502) \n",
      "\n",
      "seq_loss tensor(1.4983)\n",
      "v_loss tensor(0.0571)\n",
      "j_loss tensor(0.2273)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1463) \n",
      "\n",
      "seq_loss tensor(1.4915)\n",
      "v_loss tensor(0.0675)\n",
      "j_loss tensor(0.2208)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1508) \n",
      "\n",
      "seq_loss tensor(1.3853)\n",
      "v_loss tensor(0.0464)\n",
      "j_loss tensor(0.2183)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1529) \n",
      "\n",
      "\n",
      "30 epochs, Valid Rec:\t5.252e-01, KLD:\t4.494e-02\n",
      "seq_loss tensor(1.4634, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1952, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1463, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0755, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1498, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1485, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4792, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1934, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1438, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1958, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1505, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4311, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1983, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1485, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4683, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1941, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1465, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4373, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1986, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1480, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1966, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1506, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4690, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0733, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1459, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5292, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0725, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1502, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4412, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1977, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1452, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4792, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0709, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1480, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1967, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1492, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1474, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1464, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1480, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4657, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1996, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1461, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4569, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1953, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1474, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0790, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1496, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1464, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1493, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1934, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1509, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4511, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1986, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1458, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5472, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1530, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4557, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1490, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4730, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1965, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1474, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4898, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1981, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1505, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1460, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0703, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1438, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4698, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1960, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1476, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1978, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1489, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1493, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0364, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1929, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1400, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "40 epochs, Train Rec:\t8.739e-01, KLD:\t7.183e-02\n",
      "seq_loss tensor(1.4256)\n",
      "v_loss tensor(0.0675)\n",
      "j_loss tensor(0.2259)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367) \n",
      "\n",
      "seq_loss tensor(1.4309)\n",
      "v_loss tensor(0.0607)\n",
      "j_loss tensor(0.2302)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1408) \n",
      "\n",
      "seq_loss tensor(1.4628)\n",
      "v_loss tensor(0.0570)\n",
      "j_loss tensor(0.2269)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378) \n",
      "\n",
      "seq_loss tensor(1.4589)\n",
      "v_loss tensor(0.0668)\n",
      "j_loss tensor(0.2205)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1416) \n",
      "\n",
      "seq_loss tensor(1.3605)\n",
      "v_loss tensor(0.0464)\n",
      "j_loss tensor(0.2175)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1425) \n",
      "\n",
      "\n",
      "40 epochs, Valid Rec:\t5.159e-01, KLD:\t4.216e-02\n",
      "seq_loss tensor(1.4900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1952, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1382, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1942, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1434, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1399, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4557, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1919, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0756, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1991, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1436, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4346, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1963, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1404, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5054, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1402, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4478, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0723, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1439, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1933, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1411, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0728, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1371, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2038, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1435, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3983, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1408, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4545, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1386, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4794, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1398, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4050, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1399, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3767, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1986, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1938, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1418, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1987, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1424, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1952, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1399, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1415, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1978, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1407, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4521, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1407, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1978, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1406, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1404, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4526, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2009, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1415, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4572, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1959, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1418, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3884, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0787, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1386, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3851, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1945, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4791, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1395, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1950, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1262, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "50 epochs, Train Rec:\t8.659e-01, KLD:\t6.978e-02\n",
      "seq_loss tensor(1.4016)\n",
      "v_loss tensor(0.0672)\n",
      "j_loss tensor(0.2255)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337) \n",
      "\n",
      "seq_loss tensor(1.4055)\n",
      "v_loss tensor(0.0608)\n",
      "j_loss tensor(0.2298)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373) \n",
      "\n",
      "seq_loss tensor(1.4359)\n",
      "v_loss tensor(0.0570)\n",
      "j_loss tensor(0.2264)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348) \n",
      "\n",
      "seq_loss tensor(1.4281)\n",
      "v_loss tensor(0.0669)\n",
      "j_loss tensor(0.2188)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1382) \n",
      "\n",
      "seq_loss tensor(1.3434)\n",
      "v_loss tensor(0.0462)\n",
      "j_loss tensor(0.2158)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388) \n",
      "\n",
      "\n",
      "50 epochs, Valid Rec:\t5.081e-01, KLD:\t4.116e-02\n",
      "seq_loss tensor(1.4612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0734, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1965, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1393, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1968, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1320, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0806, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4127, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4816, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1948, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0683, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1376, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4709, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1977, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4413, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1968, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1381, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1372, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1981, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3760, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1950, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1937, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1963, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1991, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1968, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3953, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2004, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4030, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3918, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0713, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1967, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0733, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1949, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4515, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0714, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1999, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1387, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1372, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3717, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1321, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3684, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1977, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0720, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1374, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1966, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0784, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4266, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "60 epochs, Train Rec:\t8.527e-01, KLD:\t6.903e-02\n",
      "seq_loss tensor(1.3801)\n",
      "v_loss tensor(0.0670)\n",
      "j_loss tensor(0.2265)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329) \n",
      "\n",
      "seq_loss tensor(1.3791)\n",
      "v_loss tensor(0.0608)\n",
      "j_loss tensor(0.2302)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366) \n",
      "\n",
      "seq_loss tensor(1.4069)\n",
      "v_loss tensor(0.0570)\n",
      "j_loss tensor(0.2267)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344) \n",
      "\n",
      "seq_loss tensor(1.3937)\n",
      "v_loss tensor(0.0670)\n",
      "j_loss tensor(0.2207)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377) \n",
      "\n",
      "seq_loss tensor(1.3257)\n",
      "v_loss tensor(0.0465)\n",
      "j_loss tensor(0.2181)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375) \n",
      "\n",
      "\n",
      "60 epochs, Valid Rec:\t5.007e-01, KLD:\t4.094e-02\n",
      "seq_loss tensor(1.3991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1970, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1327, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3907, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1960, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2023, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3782, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1966, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0729, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1958, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1962, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3953, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2031, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3636, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1995, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1950, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0673, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3828, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1958, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3535, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0708, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3890, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1954, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2003, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1376, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3849, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0811, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1977, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1341, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0735, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4401, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1967, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1368, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1967, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1363, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1959, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1937, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1390, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1368, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0696, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1991, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1946, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1983, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "70 epochs, Train Rec:\t8.445e-01, KLD:\t6.851e-02\n",
      "seq_loss tensor(1.3742)\n",
      "v_loss tensor(0.0673)\n",
      "j_loss tensor(0.2265)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1308) \n",
      "\n",
      "seq_loss tensor(1.3724)\n",
      "v_loss tensor(0.0607)\n",
      "j_loss tensor(0.2305)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345) \n",
      "\n",
      "seq_loss tensor(1.4007)\n",
      "v_loss tensor(0.0569)\n",
      "j_loss tensor(0.2267)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326) \n",
      "\n",
      "seq_loss tensor(1.3838)\n",
      "v_loss tensor(0.0670)\n",
      "j_loss tensor(0.2202)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359) \n",
      "\n",
      "seq_loss tensor(1.3196)\n",
      "v_loss tensor(0.0457)\n",
      "j_loss tensor(0.2166)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349) \n",
      "\n",
      "\n",
      "70 epochs, Valid Rec:\t4.984e-01, KLD:\t4.030e-02\n",
      "seq_loss tensor(1.3656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1943, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3782, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0766, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1335, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3646, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2004, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3569, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1946, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2014, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1312, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3507, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4245, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1953, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3684, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3691, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1953, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3627, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1931, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1292, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3830, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4412, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1939, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1965, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1357, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3822, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1945, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1940, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3732, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3748, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0710, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3720, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1298, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0694, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0734, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0693, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1983, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3784, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1955, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1341, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3443, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0701, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1319, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0761, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "80 epochs, Train Rec:\t8.233e-01, KLD:\t6.935e-02\n",
      "seq_loss tensor(1.3287)\n",
      "v_loss tensor(0.0664)\n",
      "j_loss tensor(0.2229)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334) \n",
      "\n",
      "seq_loss tensor(1.3243)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2276)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370) \n",
      "\n",
      "seq_loss tensor(1.3569)\n",
      "v_loss tensor(0.0567)\n",
      "j_loss tensor(0.2234)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350) \n",
      "\n",
      "seq_loss tensor(1.3344)\n",
      "v_loss tensor(0.0659)\n",
      "j_loss tensor(0.2163)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1386) \n",
      "\n",
      "seq_loss tensor(1.2727)\n",
      "v_loss tensor(0.0459)\n",
      "j_loss tensor(0.2129)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1376) \n",
      "\n",
      "\n",
      "80 epochs, Valid Rec:\t4.831e-01, KLD:\t4.109e-02\n",
      "seq_loss tensor(1.3487, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1931, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1354, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1963, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3626, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1954, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1938, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1938, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1389, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1962, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1361, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3378, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1893, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1399, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3904, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1952, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1389, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1941, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1930, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1374, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1930, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1372, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3311, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3196, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1360, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3349, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0700, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1949, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1361, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3433, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1909, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1954, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1927, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0714, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1941, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1364, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1919, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1936, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1914, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1921, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1392, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3136, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1952, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1360, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1892, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1957, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1911, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1937, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3750, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1919, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3457, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1910, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1360, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3678, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1913, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1935, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1957, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1317, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "90 epochs, Train Rec:\t7.983e-01, KLD:\t6.966e-02\n",
      "seq_loss tensor(1.2869)\n",
      "v_loss tensor(0.0649)\n",
      "j_loss tensor(0.2186)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340) \n",
      "\n",
      "seq_loss tensor(1.2843)\n",
      "v_loss tensor(0.0591)\n",
      "j_loss tensor(0.2232)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1374) \n",
      "\n",
      "seq_loss tensor(1.3155)\n",
      "v_loss tensor(0.0558)\n",
      "j_loss tensor(0.2191)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356) \n",
      "\n",
      "seq_loss tensor(1.2921)\n",
      "v_loss tensor(0.0643)\n",
      "j_loss tensor(0.2117)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388) \n",
      "\n",
      "seq_loss tensor(1.2317)\n",
      "v_loss tensor(0.0459)\n",
      "j_loss tensor(0.2074)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379) \n",
      "\n",
      "\n",
      "90 epochs, Valid Rec:\t4.690e-01, KLD:\t4.121e-02\n",
      "seq_loss tensor(1.3418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1893, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1374, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0713, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1912, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1372, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1837, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1357, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1923, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1881, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2804, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1882, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1394, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1893, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0708, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1891, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1905, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0709, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1954, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1886, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2813, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1919, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0751, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1923, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3193, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1919, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1893, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2667, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1885, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1882, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1887, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2780, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3268, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1876, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1913, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2717, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1872, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1368, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1896, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2651, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1878, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1371, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1867, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1381, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1887, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3163, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1929, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1902, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1921, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0709, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1882, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1395, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "100 epochs, Train Rec:\t7.899e-01, KLD:\t6.934e-02\n",
      "seq_loss tensor(1.2742)\n",
      "v_loss tensor(0.0648)\n",
      "j_loss tensor(0.2179)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333) \n",
      "\n",
      "seq_loss tensor(1.2711)\n",
      "v_loss tensor(0.0588)\n",
      "j_loss tensor(0.2218)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1365) \n",
      "\n",
      "seq_loss tensor(1.2985)\n",
      "v_loss tensor(0.0551)\n",
      "j_loss tensor(0.2182)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350) \n",
      "\n",
      "seq_loss tensor(1.2779)\n",
      "v_loss tensor(0.0639)\n",
      "j_loss tensor(0.2122)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1383) \n",
      "\n",
      "seq_loss tensor(1.2229)\n",
      "v_loss tensor(0.0443)\n",
      "j_loss tensor(0.2093)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1369) \n",
      "\n",
      "\n",
      "100 epochs, Valid Rec:\t4.648e-01, KLD:\t4.099e-02\n",
      "seq_loss tensor(1.2514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0721, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1875, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3306, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1369, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1889, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2755, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1837, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2846, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1888, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1325, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2696, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1897, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1369, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1920, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1327, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1365, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1874, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1357, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1892, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2548, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1914, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3619, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1880, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1887, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1887, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1905, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1857, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2738, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1845, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2763, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1853, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1363, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1870, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1854, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2828, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1847, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2653, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0741, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1899, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1885, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3465, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1361, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1870, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1916, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2474, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1325, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "110 epochs, Train Rec:\t7.806e-01, KLD:\t6.923e-02\n",
      "seq_loss tensor(1.2589)\n",
      "v_loss tensor(0.0639)\n",
      "j_loss tensor(0.2161)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330) \n",
      "\n",
      "seq_loss tensor(1.2558)\n",
      "v_loss tensor(0.0584)\n",
      "j_loss tensor(0.2205)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1363) \n",
      "\n",
      "seq_loss tensor(1.2815)\n",
      "v_loss tensor(0.0550)\n",
      "j_loss tensor(0.2171)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349) \n",
      "\n",
      "seq_loss tensor(1.2617)\n",
      "v_loss tensor(0.0628)\n",
      "j_loss tensor(0.2105)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1384) \n",
      "\n",
      "seq_loss tensor(1.2075)\n",
      "v_loss tensor(0.0457)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366) \n",
      "\n",
      "\n",
      "110 epochs, Valid Rec:\t4.595e-01, KLD:\t4.094e-02\n",
      "seq_loss tensor(1.2631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1872, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2831, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2468, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0709, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1874, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2846, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1850, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1365, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1323, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2837, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1862, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2373, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1354, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1317, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2671, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1867, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1889, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2712, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2606, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1846, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1839, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1341, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1853, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2586, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1855, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1860, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1386, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2704, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0711, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1899, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2805, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2645, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1883, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2732, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1864, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1360, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1857, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2506, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0739, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1943, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2693, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1884, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0714, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1887, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1380, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2885, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1891, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1867, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1390, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1896, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "120 epochs, Train Rec:\t7.714e-01, KLD:\t6.951e-02\n",
      "seq_loss tensor(1.2423)\n",
      "v_loss tensor(0.0639)\n",
      "j_loss tensor(0.2143)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342) \n",
      "\n",
      "seq_loss tensor(1.2352)\n",
      "v_loss tensor(0.0581)\n",
      "j_loss tensor(0.2193)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375) \n",
      "\n",
      "seq_loss tensor(1.2626)\n",
      "v_loss tensor(0.0544)\n",
      "j_loss tensor(0.2154)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362) \n",
      "\n",
      "seq_loss tensor(1.2416)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2087)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1398) \n",
      "\n",
      "seq_loss tensor(1.1873)\n",
      "v_loss tensor(0.0439)\n",
      "j_loss tensor(0.2032)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378) \n",
      "\n",
      "\n",
      "120 epochs, Valid Rec:\t4.529e-01, KLD:\t4.132e-02\n",
      "seq_loss tensor(1.2537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1865, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2544, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1848, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1369, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1831, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1868, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2698, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0703, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1397, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1855, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0481, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1394, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2894, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1860, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1878, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1369, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2699, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1831, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1325, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1892, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2465, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1892, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1379, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1862, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2760, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1883, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2330, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0424, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0731, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2781, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1385, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1899, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2543, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1308, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0762, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1903, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0491, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1874, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1371, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2457, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2727, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0736, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1848, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1396, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1381, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1881, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1357, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "130 epochs, Train Rec:\t7.618e-01, KLD:\t6.911e-02\n",
      "seq_loss tensor(1.2337)\n",
      "v_loss tensor(0.0637)\n",
      "j_loss tensor(0.2150)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340) \n",
      "\n",
      "seq_loss tensor(1.2250)\n",
      "v_loss tensor(0.0577)\n",
      "j_loss tensor(0.2191)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373) \n",
      "\n",
      "seq_loss tensor(1.2526)\n",
      "v_loss tensor(0.0544)\n",
      "j_loss tensor(0.2153)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1359) \n",
      "\n",
      "seq_loss tensor(1.2332)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2080)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1395) \n",
      "\n",
      "seq_loss tensor(1.1759)\n",
      "v_loss tensor(0.0449)\n",
      "j_loss tensor(0.2026)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375) \n",
      "\n",
      "\n",
      "130 epochs, Valid Rec:\t4.499e-01, KLD:\t4.124e-02\n",
      "seq_loss tensor(1.2842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2106, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1829, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1857, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0524, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1373, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1363, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2638, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1858, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1302, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2349, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1319, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1384, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1371, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1393, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1392, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2678, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1826, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0697, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1837, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1364, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1385, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2582, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1851, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1357, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1402, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1400, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1911, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "140 epochs, Train Rec:\t7.574e-01, KLD:\t6.901e-02\n",
      "seq_loss tensor(1.2219)\n",
      "v_loss tensor(0.0637)\n",
      "j_loss tensor(0.2136)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332) \n",
      "\n",
      "seq_loss tensor(1.2140)\n",
      "v_loss tensor(0.0574)\n",
      "j_loss tensor(0.2188)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366) \n",
      "\n",
      "seq_loss tensor(1.2409)\n",
      "v_loss tensor(0.0542)\n",
      "j_loss tensor(0.2140)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352) \n",
      "\n",
      "seq_loss tensor(1.2203)\n",
      "v_loss tensor(0.0622)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388) \n",
      "\n",
      "seq_loss tensor(1.1662)\n",
      "v_loss tensor(0.0444)\n",
      "j_loss tensor(0.2014)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1368) \n",
      "\n",
      "\n",
      "140 epochs, Valid Rec:\t4.460e-01, KLD:\t4.103e-02\n",
      "seq_loss tensor(1.2451, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2375, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1380, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1372, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1839, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1325, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2884, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1884, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1365, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2523, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0726, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1403, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1385, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1831, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2248, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0713, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1845, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1387, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0734, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1729, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2413, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1850, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0524, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2386, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2206, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1882, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1868, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1934, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "150 epochs, Train Rec:\t7.559e-01, KLD:\t6.843e-02\n",
      "seq_loss tensor(1.2173)\n",
      "v_loss tensor(0.0634)\n",
      "j_loss tensor(0.2136)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1319) \n",
      "\n",
      "seq_loss tensor(1.2087)\n",
      "v_loss tensor(0.0573)\n",
      "j_loss tensor(0.2193)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352) \n",
      "\n",
      "seq_loss tensor(1.2342)\n",
      "v_loss tensor(0.0540)\n",
      "j_loss tensor(0.2145)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339) \n",
      "\n",
      "seq_loss tensor(1.2141)\n",
      "v_loss tensor(0.0616)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1375) \n",
      "\n",
      "seq_loss tensor(1.1585)\n",
      "v_loss tensor(0.0443)\n",
      "j_loss tensor(0.2014)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355) \n",
      "\n",
      "\n",
      "150 epochs, Valid Rec:\t4.442e-01, KLD:\t4.063e-02\n",
      "seq_loss tensor(1.1992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1998, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1360, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1370, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1319, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2136, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1298, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1301, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1374, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1356, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1875, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2685, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1341, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2315, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1389, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1989, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1302, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1837, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1327, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0809, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "160 epochs, Train Rec:\t7.491e-01, KLD:\t6.807e-02\n",
      "seq_loss tensor(1.2101)\n",
      "v_loss tensor(0.0634)\n",
      "j_loss tensor(0.2120)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1309) \n",
      "\n",
      "seq_loss tensor(1.2017)\n",
      "v_loss tensor(0.0568)\n",
      "j_loss tensor(0.2175)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343) \n",
      "\n",
      "seq_loss tensor(1.2279)\n",
      "v_loss tensor(0.0537)\n",
      "j_loss tensor(0.2130)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330) \n",
      "\n",
      "seq_loss tensor(1.2074)\n",
      "v_loss tensor(0.0617)\n",
      "j_loss tensor(0.2059)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366) \n",
      "\n",
      "seq_loss tensor(1.1499)\n",
      "v_loss tensor(0.0442)\n",
      "j_loss tensor(0.2004)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346) \n",
      "\n",
      "\n",
      "160 epochs, Valid Rec:\t4.416e-01, KLD:\t4.035e-02\n",
      "seq_loss tensor(1.2216, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0490, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1963, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2102, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0720, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1853, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1352, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0717, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2319, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1298, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2310, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0694, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1849, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0743, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1350, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2408, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1335, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0505, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1320, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2831, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1888, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1761, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1305, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2700, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0510, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1389, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2086, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2124, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1293, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1317, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2330, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2292, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1886, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2448, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1861, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1321, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1366, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2539, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1526, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1855, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1368, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "170 epochs, Train Rec:\t7.482e-01, KLD:\t6.779e-02\n",
      "seq_loss tensor(1.2085)\n",
      "v_loss tensor(0.0632)\n",
      "j_loss tensor(0.2109)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311) \n",
      "\n",
      "seq_loss tensor(1.2000)\n",
      "v_loss tensor(0.0571)\n",
      "j_loss tensor(0.2161)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343) \n",
      "\n",
      "seq_loss tensor(1.2255)\n",
      "v_loss tensor(0.0537)\n",
      "j_loss tensor(0.2117)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329) \n",
      "\n",
      "seq_loss tensor(1.2056)\n",
      "v_loss tensor(0.0612)\n",
      "j_loss tensor(0.2043)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1367) \n",
      "\n",
      "seq_loss tensor(1.1512)\n",
      "v_loss tensor(0.0436)\n",
      "j_loss tensor(0.1993)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1351) \n",
      "\n",
      "\n",
      "170 epochs, Valid Rec:\t4.407e-01, KLD:\t4.039e-02\n",
      "seq_loss tensor(1.2728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1281, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2592, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2624, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1739, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1312, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2523, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0467, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2103, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2082, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1340, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2446, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1650, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1314, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2303, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1341, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1337, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2037, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1263, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1368, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1342, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1650, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1835, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1335, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1347, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1349, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1328, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1355, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1960, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1280, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0419, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1698, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1360, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "180 epochs, Train Rec:\t7.438e-01, KLD:\t6.734e-02\n",
      "seq_loss tensor(1.1994)\n",
      "v_loss tensor(0.0632)\n",
      "j_loss tensor(0.2110)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1296) \n",
      "\n",
      "seq_loss tensor(1.1907)\n",
      "v_loss tensor(0.0566)\n",
      "j_loss tensor(0.2161)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329) \n",
      "\n",
      "seq_loss tensor(1.2186)\n",
      "v_loss tensor(0.0532)\n",
      "j_loss tensor(0.2115)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1316) \n",
      "\n",
      "seq_loss tensor(1.1973)\n",
      "v_loss tensor(0.0612)\n",
      "j_loss tensor(0.2043)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353) \n",
      "\n",
      "seq_loss tensor(1.1405)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1996)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1334) \n",
      "\n",
      "\n",
      "180 epochs, Valid Rec:\t4.380e-01, KLD:\t3.995e-02\n",
      "seq_loss tensor(1.2176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1946, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1755, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1323, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1390, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1534, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1681, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1845, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1335, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1278, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1804, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2427, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1850, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1749, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1336, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1305, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2112, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1344, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1801, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1314, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1858, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1305, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1807, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1762, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0699, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1298, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1317, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1855, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2692, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "190 epochs, Train Rec:\t7.410e-01, KLD:\t6.718e-02\n",
      "seq_loss tensor(1.1946)\n",
      "v_loss tensor(0.0630)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291) \n",
      "\n",
      "seq_loss tensor(1.1843)\n",
      "v_loss tensor(0.0569)\n",
      "j_loss tensor(0.2156)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1325) \n",
      "\n",
      "seq_loss tensor(1.2108)\n",
      "v_loss tensor(0.0533)\n",
      "j_loss tensor(0.2108)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310) \n",
      "\n",
      "seq_loss tensor(1.1906)\n",
      "v_loss tensor(0.0610)\n",
      "j_loss tensor(0.2036)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346) \n",
      "\n",
      "seq_loss tensor(1.1344)\n",
      "v_loss tensor(0.0439)\n",
      "j_loss tensor(0.1982)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326) \n",
      "\n",
      "\n",
      "190 epochs, Valid Rec:\t4.359e-01, KLD:\t3.977e-02\n",
      "seq_loss tensor(1.1821, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0508, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1660, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1302, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1314, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2489, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1286, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2392, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1694, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1302, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1829, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1831, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1314, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1783, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1853, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1278, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1882, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1319, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1316, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1301, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1802, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1353, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1851, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0732, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1839, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1282, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0510, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1320, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1785, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1790, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1309, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1686, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1854, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1281, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0840, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "200 epochs, Train Rec:\t7.370e-01, KLD:\t6.681e-02\n",
      "seq_loss tensor(1.1886)\n",
      "v_loss tensor(0.0630)\n",
      "j_loss tensor(0.2113)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288) \n",
      "\n",
      "seq_loss tensor(1.1809)\n",
      "v_loss tensor(0.0565)\n",
      "j_loss tensor(0.2173)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1322) \n",
      "\n",
      "seq_loss tensor(1.2049)\n",
      "v_loss tensor(0.0532)\n",
      "j_loss tensor(0.2122)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307) \n",
      "\n",
      "seq_loss tensor(1.1854)\n",
      "v_loss tensor(0.0610)\n",
      "j_loss tensor(0.2049)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1343) \n",
      "\n",
      "seq_loss tensor(1.1300)\n",
      "v_loss tensor(0.0438)\n",
      "j_loss tensor(0.1989)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324) \n",
      "\n",
      "\n",
      "200 epochs, Valid Rec:\t4.347e-01, KLD:\t3.969e-02\n",
      "seq_loss tensor(1.2032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1296, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1283, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1289, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1296, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2140, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2299, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1321, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1324, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1326, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2089, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1303, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1277, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1333, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1702, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1323, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2347, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1279, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1689, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1303, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1551, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2299, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1898, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1275, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1854, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1287, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1838, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1287, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0715, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1876, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1285, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1676, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1267, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1919, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1296, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1348, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1321, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1608, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1292, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1279, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2646, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "210 epochs, Train Rec:\t7.357e-01, KLD:\t6.642e-02\n",
      "seq_loss tensor(1.1858)\n",
      "v_loss tensor(0.0633)\n",
      "j_loss tensor(0.2094)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284) \n",
      "\n",
      "seq_loss tensor(1.1762)\n",
      "v_loss tensor(0.0565)\n",
      "j_loss tensor(0.2142)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318) \n",
      "\n",
      "seq_loss tensor(1.2022)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304) \n",
      "\n",
      "seq_loss tensor(1.1832)\n",
      "v_loss tensor(0.0613)\n",
      "j_loss tensor(0.2037)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1339) \n",
      "\n",
      "seq_loss tensor(1.1257)\n",
      "v_loss tensor(0.0430)\n",
      "j_loss tensor(0.1985)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1318) \n",
      "\n",
      "\n",
      "210 epochs, Valid Rec:\t4.332e-01, KLD:\t3.955e-02\n",
      "seq_loss tensor(1.1878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1734, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1851, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1280, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1944, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1345, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1695, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1378, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1726, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1252, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2206, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1303, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1314, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1822, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1316, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1974, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0465, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1650, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1823, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1875, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1327, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1289, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1919, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1330, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1793, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1839, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1309, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2681, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0778, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1362, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "220 epochs, Train Rec:\t7.336e-01, KLD:\t6.604e-02\n",
      "seq_loss tensor(1.1819)\n",
      "v_loss tensor(0.0629)\n",
      "j_loss tensor(0.2088)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1275) \n",
      "\n",
      "seq_loss tensor(1.1730)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2139)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1309) \n",
      "\n",
      "seq_loss tensor(1.1980)\n",
      "v_loss tensor(0.0532)\n",
      "j_loss tensor(0.2095)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1295) \n",
      "\n",
      "seq_loss tensor(1.1788)\n",
      "v_loss tensor(0.0611)\n",
      "j_loss tensor(0.2024)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329) \n",
      "\n",
      "seq_loss tensor(1.1245)\n",
      "v_loss tensor(0.0430)\n",
      "j_loss tensor(0.1974)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1308) \n",
      "\n",
      "\n",
      "220 epochs, Valid Rec:\t4.319e-01, KLD:\t3.927e-02\n",
      "seq_loss tensor(1.1632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1297, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1274, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1312, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1668, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1267, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2103, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1710, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1305, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1865, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1760, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1272, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1286, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1658, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1286, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1785, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1715, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1474, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1259, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2369, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1287, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1698, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0754, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1238, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1825, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1317, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1332, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1905, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1301, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1285, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1313, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1303, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1689, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1331, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1265, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0361, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "230 epochs, Train Rec:\t7.316e-01, KLD:\t6.576e-02\n",
      "seq_loss tensor(1.1800)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2095)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1259) \n",
      "\n",
      "seq_loss tensor(1.1709)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2143)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294) \n",
      "\n",
      "seq_loss tensor(1.1957)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1279) \n",
      "\n",
      "seq_loss tensor(1.1773)\n",
      "v_loss tensor(0.0608)\n",
      "j_loss tensor(0.2029)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315) \n",
      "\n",
      "seq_loss tensor(1.1187)\n",
      "v_loss tensor(0.0443)\n",
      "j_loss tensor(0.1966)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294) \n",
      "\n",
      "\n",
      "230 epochs, Valid Rec:\t4.312e-01, KLD:\t3.883e-02\n",
      "seq_loss tensor(1.2126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1299, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1789, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1277, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1850, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1277, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1303, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1325, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1779, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1259, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1726, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1354, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1298, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1893, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0497, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1273, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2002, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0448, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1624, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1327, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1622, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1437, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1693, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1281, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1837, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1269, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2124, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1281, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1249, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1279, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1287, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0423, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1666, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "240 epochs, Train Rec:\t7.297e-01, KLD:\t6.522e-02\n",
      "seq_loss tensor(1.1774)\n",
      "v_loss tensor(0.0629)\n",
      "j_loss tensor(0.2096)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1263) \n",
      "\n",
      "seq_loss tensor(1.1677)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2152)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1297) \n",
      "\n",
      "seq_loss tensor(1.1933)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1282) \n",
      "\n",
      "seq_loss tensor(1.1749)\n",
      "v_loss tensor(0.0605)\n",
      "j_loss tensor(0.2029)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1316) \n",
      "\n",
      "seq_loss tensor(1.1191)\n",
      "v_loss tensor(0.0438)\n",
      "j_loss tensor(0.1961)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1296) \n",
      "\n",
      "\n",
      "240 epochs, Valid Rec:\t4.305e-01, KLD:\t3.890e-02\n",
      "seq_loss tensor(1.1901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1269, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1747, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1275, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1768, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1287, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2248, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1289, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1651, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1685, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1306, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1283, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1361, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1279, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1671, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1251, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1272, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1839, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1289, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1626, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1974, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1297, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1265, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1886, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1265, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1801, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1289, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1642, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1285, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1758, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1995, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1316, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1567, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1316, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1675, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1338, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "250 epochs, Train Rec:\t7.277e-01, KLD:\t6.498e-02\n",
      "seq_loss tensor(1.1738)\n",
      "v_loss tensor(0.0630)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253) \n",
      "\n",
      "seq_loss tensor(1.1650)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2125)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288) \n",
      "\n",
      "seq_loss tensor(1.1901)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2084)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1274) \n",
      "\n",
      "seq_loss tensor(1.1717)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2019)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1308) \n",
      "\n",
      "seq_loss tensor(1.1174)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1968)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284) \n",
      "\n",
      "\n",
      "250 epochs, Valid Rec:\t4.292e-01, KLD:\t3.862e-02\n",
      "seq_loss tensor(1.1988, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1638, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1702, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1890, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1278, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1798, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1636, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1292, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1259, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2055, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1280, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1575, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1813, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1310, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1282, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1865, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1277, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1802, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1280, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1717, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1860, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1267, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1822, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1267, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0414, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "260 epochs, Train Rec:\t7.281e-01, KLD:\t6.474e-02\n",
      "seq_loss tensor(1.1715)\n",
      "v_loss tensor(0.0629)\n",
      "j_loss tensor(0.2088)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245) \n",
      "\n",
      "seq_loss tensor(1.1624)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2143)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1279) \n",
      "\n",
      "seq_loss tensor(1.1887)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2093)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1265) \n",
      "\n",
      "seq_loss tensor(1.1696)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2021)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1300) \n",
      "\n",
      "seq_loss tensor(1.1159)\n",
      "v_loss tensor(0.0432)\n",
      "j_loss tensor(0.1969)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1277) \n",
      "\n",
      "\n",
      "260 epochs, Valid Rec:\t4.289e-01, KLD:\t3.837e-02\n",
      "seq_loss tensor(1.1708, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1998, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1262, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1283, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1857, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1263, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1794, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1285, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1302, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1683, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1252, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1265, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1661, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1746, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1464, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1726, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1263, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1949, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1550, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1293, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1515, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2084, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1780, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1278, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1586, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1745, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1690, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1549, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2183, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1946, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0772, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1281, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "270 epochs, Train Rec:\t7.254e-01, KLD:\t6.435e-02\n",
      "seq_loss tensor(1.1697)\n",
      "v_loss tensor(0.0630)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233) \n",
      "\n",
      "seq_loss tensor(1.1605)\n",
      "v_loss tensor(0.0568)\n",
      "j_loss tensor(0.2130)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268) \n",
      "\n",
      "seq_loss tensor(1.1873)\n",
      "v_loss tensor(0.0531)\n",
      "j_loss tensor(0.2083)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253) \n",
      "\n",
      "seq_loss tensor(1.1670)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2017)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1289) \n",
      "\n",
      "seq_loss tensor(1.1133)\n",
      "v_loss tensor(0.0429)\n",
      "j_loss tensor(0.1955)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266) \n",
      "\n",
      "\n",
      "270 epochs, Valid Rec:\t4.280e-01, KLD:\t3.802e-02\n",
      "seq_loss tensor(1.1446, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2186, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1314, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1894, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1609, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1280, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1275, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1413, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1273, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1627, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1308, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1396, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1259, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1664, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1759, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1272, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1290, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1287, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1615, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1773, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1861, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1308, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1772, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1836, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1262, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1726, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1700, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1802, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0683, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2343, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0719, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0728, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "280 epochs, Train Rec:\t7.234e-01, KLD:\t6.393e-02\n",
      "seq_loss tensor(1.1670)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2102)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239) \n",
      "\n",
      "seq_loss tensor(1.1585)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2145)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1273) \n",
      "\n",
      "seq_loss tensor(1.1825)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1259) \n",
      "\n",
      "seq_loss tensor(1.1645)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2032)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1294) \n",
      "\n",
      "seq_loss tensor(1.1079)\n",
      "v_loss tensor(0.0435)\n",
      "j_loss tensor(0.1987)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270) \n",
      "\n",
      "\n",
      "280 epochs, Valid Rec:\t4.275e-01, KLD:\t3.818e-02\n",
      "seq_loss tensor(1.1737, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1249, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1548, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1142, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1935, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1772, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1950, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0418, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1282, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1564, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1272, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1597, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1909, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1675, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1284, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1794, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1802, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1278, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1674, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2030, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1282, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1790, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1686, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1282, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1643, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2385, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1238, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1851, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1742, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0499, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1229, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0457, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "290 epochs, Train Rec:\t7.238e-01, KLD:\t6.371e-02\n",
      "seq_loss tensor(1.1678)\n",
      "v_loss tensor(0.0628)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232) \n",
      "\n",
      "seq_loss tensor(1.1588)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2132)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266) \n",
      "\n",
      "seq_loss tensor(1.1841)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2091)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1252) \n",
      "\n",
      "seq_loss tensor(1.1651)\n",
      "v_loss tensor(0.0603)\n",
      "j_loss tensor(0.2021)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1286) \n",
      "\n",
      "seq_loss tensor(1.1105)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1973)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1263) \n",
      "\n",
      "\n",
      "290 epochs, Valid Rec:\t4.275e-01, KLD:\t3.797e-02\n",
      "seq_loss tensor(1.1721, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1704, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1831, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1251, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1697, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1307, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1712, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0696, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1275, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0737, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1794, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2084, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1378, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1690, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1291, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1660, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1791, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1674, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0359, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1311, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "300 epochs, Train Rec:\t7.236e-01, KLD:\t6.352e-02\n",
      "seq_loss tensor(1.1655)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2088)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218) \n",
      "\n",
      "seq_loss tensor(1.1574)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2145)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1252) \n",
      "\n",
      "seq_loss tensor(1.1814)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2100)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1238) \n",
      "\n",
      "seq_loss tensor(1.1631)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2024)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271) \n",
      "\n",
      "seq_loss tensor(1.1094)\n",
      "v_loss tensor(0.0441)\n",
      "j_loss tensor(0.1975)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248) \n",
      "\n",
      "\n",
      "300 epochs, Valid Rec:\t4.271e-01, KLD:\t3.753e-02\n",
      "seq_loss tensor(1.1792, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1670, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1525, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1831, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1738, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1917, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1635, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1692, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1811, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1654, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1288, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1895, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1807, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1762, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1504, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1277, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1275, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1844, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0493, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1750, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "310 epochs, Train Rec:\t7.222e-01, KLD:\t6.316e-02\n",
      "seq_loss tensor(1.1645)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2096)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221) \n",
      "\n",
      "seq_loss tensor(1.1550)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2154)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256) \n",
      "\n",
      "seq_loss tensor(1.1794)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2103)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242) \n",
      "\n",
      "seq_loss tensor(1.1624)\n",
      "v_loss tensor(0.0607)\n",
      "j_loss tensor(0.2038)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1276) \n",
      "\n",
      "seq_loss tensor(1.1079)\n",
      "v_loss tensor(0.0433)\n",
      "j_loss tensor(0.1986)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250) \n",
      "\n",
      "\n",
      "310 epochs, Valid Rec:\t4.269e-01, KLD:\t3.765e-02\n",
      "seq_loss tensor(1.1401, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1693, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2058, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1767, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0705, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1882, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1517, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1802, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1262, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1347, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1681, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1593, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1580, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1742, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1468, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1588, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0700, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1643, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1681, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1851, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1256, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1755, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1763, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1847, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "320 epochs, Train Rec:\t7.213e-01, KLD:\t6.287e-02\n",
      "seq_loss tensor(1.1623)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2084)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206) \n",
      "\n",
      "seq_loss tensor(1.1532)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2136)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241) \n",
      "\n",
      "seq_loss tensor(1.1777)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2090)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227) \n",
      "\n",
      "seq_loss tensor(1.1598)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2013)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260) \n",
      "\n",
      "seq_loss tensor(1.1093)\n",
      "v_loss tensor(0.0435)\n",
      "j_loss tensor(0.1958)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235) \n",
      "\n",
      "\n",
      "320 epochs, Valid Rec:\t4.259e-01, KLD:\t3.719e-02\n",
      "seq_loss tensor(1.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1268, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1662, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0693, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1792, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1670, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1588, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1849, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1261, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1720, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1740, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1797, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0497, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1715, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1682, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1251, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1252, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1686, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0441, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1684, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "330 epochs, Train Rec:\t7.209e-01, KLD:\t6.267e-02\n",
      "seq_loss tensor(1.1583)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2086)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "seq_loss tensor(1.1487)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2149)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248) \n",
      "\n",
      "seq_loss tensor(1.1728)\n",
      "v_loss tensor(0.0524)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234) \n",
      "\n",
      "seq_loss tensor(1.1558)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2024)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1267) \n",
      "\n",
      "seq_loss tensor(1.1017)\n",
      "v_loss tensor(0.0428)\n",
      "j_loss tensor(0.1975)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242) \n",
      "\n",
      "\n",
      "330 epochs, Valid Rec:\t4.247e-01, KLD:\t3.739e-02\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2027, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1662, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1640, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1471, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1759, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1570, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1562, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1730, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1721, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1654, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1747, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1266, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2046, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1466, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1503, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1770, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1613, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1262, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0429, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "340 epochs, Train Rec:\t7.197e-01, KLD:\t6.240e-02\n",
      "seq_loss tensor(1.1575)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(1.1497)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2127)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237) \n",
      "\n",
      "seq_loss tensor(1.1729)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2085)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223) \n",
      "\n",
      "seq_loss tensor(1.1540)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2009)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1258) \n",
      "\n",
      "seq_loss tensor(1.1010)\n",
      "v_loss tensor(0.0432)\n",
      "j_loss tensor(0.1954)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231) \n",
      "\n",
      "\n",
      "340 epochs, Valid Rec:\t4.241e-01, KLD:\t3.708e-02\n",
      "seq_loss tensor(1.1615, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1799, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1756, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1586, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0483, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1415, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1243, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1799, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1486, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1840, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "350 epochs, Train Rec:\t7.194e-01, KLD:\t6.220e-02\n",
      "seq_loss tensor(1.1568)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2080)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200) \n",
      "\n",
      "seq_loss tensor(1.1485)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2133)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234) \n",
      "\n",
      "seq_loss tensor(1.1727)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2089)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221) \n",
      "\n",
      "seq_loss tensor(1.1544)\n",
      "v_loss tensor(0.0605)\n",
      "j_loss tensor(0.2016)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1253) \n",
      "\n",
      "seq_loss tensor(1.1012)\n",
      "v_loss tensor(0.0439)\n",
      "j_loss tensor(0.1970)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227) \n",
      "\n",
      "\n",
      "350 epochs, Valid Rec:\t4.243e-01, KLD:\t3.698e-02\n",
      "seq_loss tensor(1.1584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1622, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1703, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1785, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1695, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2324, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2063, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0718, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1763, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1195, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1490, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0494, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1828, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1691, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1252, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1588, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3595, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0798, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1864, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "360 epochs, Train Rec:\t7.186e-01, KLD:\t6.193e-02\n",
      "seq_loss tensor(1.1547)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199) \n",
      "\n",
      "seq_loss tensor(1.1460)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2130)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233) \n",
      "\n",
      "seq_loss tensor(1.1698)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220) \n",
      "\n",
      "seq_loss tensor(1.1513)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2011)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254) \n",
      "\n",
      "seq_loss tensor(1.0997)\n",
      "v_loss tensor(0.0432)\n",
      "j_loss tensor(0.1949)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228) \n",
      "\n",
      "\n",
      "360 epochs, Valid Rec:\t4.232e-01, KLD:\t3.697e-02\n",
      "seq_loss tensor(1.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1696, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1355, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1840, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1500, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1715, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1593, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2259, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1696, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1826, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1497, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1243, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1853, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1706, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1700, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1789, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0451, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1605, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "370 epochs, Train Rec:\t7.164e-01, KLD:\t6.204e-02\n",
      "seq_loss tensor(1.1543)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196) \n",
      "\n",
      "seq_loss tensor(1.1446)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2129)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231) \n",
      "\n",
      "seq_loss tensor(1.1681)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2089)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217) \n",
      "\n",
      "seq_loss tensor(1.1495)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2025)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1251) \n",
      "\n",
      "seq_loss tensor(1.0994)\n",
      "v_loss tensor(0.0428)\n",
      "j_loss tensor(0.1968)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226) \n",
      "\n",
      "\n",
      "370 epochs, Valid Rec:\t4.231e-01, KLD:\t3.689e-02\n",
      "seq_loss tensor(1.1465, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1455, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0482, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1790, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1904, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1248, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1938, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1721, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1777, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1825, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0683, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1829, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1635, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0701, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1676, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1775, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1251, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1595, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1392, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "380 epochs, Train Rec:\t7.184e-01, KLD:\t6.182e-02\n",
      "seq_loss tensor(1.1563)\n",
      "v_loss tensor(0.0629)\n",
      "j_loss tensor(0.2075)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185) \n",
      "\n",
      "seq_loss tensor(1.1477)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2129)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219) \n",
      "\n",
      "seq_loss tensor(1.1713)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2081)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "seq_loss tensor(1.1526)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2009)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240) \n",
      "\n",
      "seq_loss tensor(1.1013)\n",
      "v_loss tensor(0.0418)\n",
      "j_loss tensor(0.1947)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216) \n",
      "\n",
      "\n",
      "380 epochs, Valid Rec:\t4.236e-01, KLD:\t3.657e-02\n",
      "seq_loss tensor(1.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0760, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1705, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1561, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0744, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0508, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1581, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1575, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1609, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1396, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1544, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1018, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1735, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1709, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0462, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0398, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1320, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "390 epochs, Train Rec:\t7.146e-01, KLD:\t6.163e-02\n",
      "seq_loss tensor(1.1549)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183) \n",
      "\n",
      "seq_loss tensor(1.1463)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2124)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218) \n",
      "\n",
      "seq_loss tensor(1.1696)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204) \n",
      "\n",
      "seq_loss tensor(1.1505)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.2009)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1238) \n",
      "\n",
      "seq_loss tensor(1.1012)\n",
      "v_loss tensor(0.0435)\n",
      "j_loss tensor(0.1952)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212) \n",
      "\n",
      "\n",
      "390 epochs, Valid Rec:\t4.233e-01, KLD:\t3.650e-02\n",
      "seq_loss tensor(1.1677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1768, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1249, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1793, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1772, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1398, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1543, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1670, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1295, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1424, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0733, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1662, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1614, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1528, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1950, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1592, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0756, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "400 epochs, Train Rec:\t7.130e-01, KLD:\t6.157e-02\n",
      "seq_loss tensor(1.1530)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182) \n",
      "\n",
      "seq_loss tensor(1.1429)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2129)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217) \n",
      "\n",
      "seq_loss tensor(1.1664)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2080)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203) \n",
      "\n",
      "seq_loss tensor(1.1476)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2007)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236) \n",
      "\n",
      "seq_loss tensor(1.0990)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1956)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211) \n",
      "\n",
      "\n",
      "400 epochs, Valid Rec:\t4.225e-01, KLD:\t3.646e-02\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1720, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1581, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1777, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1653, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1424, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1229, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1782, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1657, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0489, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0719, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1507, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1839, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1254, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1362, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1219, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "410 epochs, Train Rec:\t7.174e-01, KLD:\t6.167e-02\n",
      "seq_loss tensor(1.1496)\n",
      "v_loss tensor(0.0629)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185) \n",
      "\n",
      "seq_loss tensor(1.1403)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2128)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219) \n",
      "\n",
      "seq_loss tensor(1.1618)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206) \n",
      "\n",
      "seq_loss tensor(1.1449)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2004)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239) \n",
      "\n",
      "seq_loss tensor(1.0950)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1956)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "\n",
      "410 epochs, Valid Rec:\t4.213e-01, KLD:\t3.654e-02\n",
      "seq_loss tensor(1.2071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1607, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0742, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1586, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1627, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1342, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1650, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1563, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1267, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1719, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1457, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1689, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1336, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1638, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0441, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1847, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "420 epochs, Train Rec:\t7.136e-01, KLD:\t6.141e-02\n",
      "seq_loss tensor(1.1468)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "seq_loss tensor(1.1399)\n",
      "v_loss tensor(0.0565)\n",
      "j_loss tensor(0.2129)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220) \n",
      "\n",
      "seq_loss tensor(1.1621)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2081)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "seq_loss tensor(1.1444)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2011)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239) \n",
      "\n",
      "seq_loss tensor(1.0935)\n",
      "v_loss tensor(0.0428)\n",
      "j_loss tensor(0.1966)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212) \n",
      "\n",
      "\n",
      "420 epochs, Valid Rec:\t4.212e-01, KLD:\t3.655e-02\n",
      "seq_loss tensor(1.2061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0479, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1933, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1581, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1668, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0505, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1366, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1683, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1517, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1268, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1399, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1260, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1551, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1741, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0734, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1597, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0494, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1412, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1497, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0754, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1229, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1219, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0494, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1695, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "430 epochs, Train Rec:\t7.132e-01, KLD:\t6.159e-02\n",
      "seq_loss tensor(1.1471)\n",
      "v_loss tensor(0.0628)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "seq_loss tensor(1.1382)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2128)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214) \n",
      "\n",
      "seq_loss tensor(1.1601)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2079)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(1.1423)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.2004)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233) \n",
      "\n",
      "seq_loss tensor(1.0926)\n",
      "v_loss tensor(0.0417)\n",
      "j_loss tensor(0.1958)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "\n",
      "430 epochs, Valid Rec:\t4.206e-01, KLD:\t3.637e-02\n",
      "seq_loss tensor(1.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0673, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1486, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1465, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1238, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1238, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1760, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1521, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1450, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1732, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1299, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1708, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1487, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1781, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1241, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1764, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1619, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1448, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0448, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "440 epochs, Train Rec:\t7.106e-01, KLD:\t6.130e-02\n",
      "seq_loss tensor(1.1458)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "seq_loss tensor(1.1365)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2132)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211) \n",
      "\n",
      "seq_loss tensor(1.1587)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2079)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200) \n",
      "\n",
      "seq_loss tensor(1.1416)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2012)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1229) \n",
      "\n",
      "seq_loss tensor(1.0917)\n",
      "v_loss tensor(0.0427)\n",
      "j_loss tensor(0.1951)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "\n",
      "440 epochs, Valid Rec:\t4.203e-01, KLD:\t3.628e-02\n",
      "seq_loss tensor(1.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0510, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2030, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0752, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1536, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1528, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1394, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1804, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1525, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0524, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1567, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1738, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1264, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1650, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0789, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1566, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1236, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0477, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1862, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1354, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "450 epochs, Train Rec:\t7.100e-01, KLD:\t6.121e-02\n",
      "seq_loss tensor(1.1428)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.1348)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2127)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210) \n",
      "\n",
      "seq_loss tensor(1.1573)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2083)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198) \n",
      "\n",
      "seq_loss tensor(1.1393)\n",
      "v_loss tensor(0.0595)\n",
      "j_loss tensor(0.2013)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228) \n",
      "\n",
      "seq_loss tensor(1.0917)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1964)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201) \n",
      "\n",
      "\n",
      "450 epochs, Valid Rec:\t4.199e-01, KLD:\t3.624e-02\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0518, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1792, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1249, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1703, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1246, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1448, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0718, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1474, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1764, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1411, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0727, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1734, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1264, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1564, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1868, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1794, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1398, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1336, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1549, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "460 epochs, Train Rec:\t7.084e-01, KLD:\t6.122e-02\n",
      "seq_loss tensor(1.1417)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178) \n",
      "\n",
      "seq_loss tensor(1.1325)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2140)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "seq_loss tensor(1.1555)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2091)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201) \n",
      "\n",
      "seq_loss tensor(1.1374)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.2010)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231) \n",
      "\n",
      "seq_loss tensor(1.0887)\n",
      "v_loss tensor(0.0428)\n",
      "j_loss tensor(0.1951)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204) \n",
      "\n",
      "\n",
      "460 epochs, Valid Rec:\t4.194e-01, KLD:\t3.633e-02\n",
      "seq_loss tensor(1.1609, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1304, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0475, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0699, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0741, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1539, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0494, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1705, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0508, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1670, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1392, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1098, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1437, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1570, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1825, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1797, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1667, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0360, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1690, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "470 epochs, Train Rec:\t7.093e-01, KLD:\t6.108e-02\n",
      "seq_loss tensor(1.1412)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "seq_loss tensor(1.1325)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2118)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "seq_loss tensor(1.1540)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2072)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201) \n",
      "\n",
      "seq_loss tensor(1.1358)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.1997)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231) \n",
      "\n",
      "seq_loss tensor(1.0875)\n",
      "v_loss tensor(0.0432)\n",
      "j_loss tensor(0.1949)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204) \n",
      "\n",
      "\n",
      "470 epochs, Valid Rec:\t4.187e-01, KLD:\t3.633e-02\n",
      "seq_loss tensor(1.1551, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1709, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1990, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1128, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1830, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1541, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0459, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1650, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1649, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1251, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1527, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1683, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0717, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1868, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0492, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1606, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1852, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0371, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "480 epochs, Train Rec:\t7.086e-01, KLD:\t6.121e-02\n",
      "seq_loss tensor(1.1404)\n",
      "v_loss tensor(0.0628)\n",
      "j_loss tensor(0.2075)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "seq_loss tensor(1.1332)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2129)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "seq_loss tensor(1.1548)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2079)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(1.1360)\n",
      "v_loss tensor(0.0595)\n",
      "j_loss tensor(0.2001)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1231) \n",
      "\n",
      "seq_loss tensor(1.0897)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1944)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205) \n",
      "\n",
      "\n",
      "480 epochs, Valid Rec:\t4.190e-01, KLD:\t3.634e-02\n",
      "seq_loss tensor(1.1407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1592, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1653, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1466, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1472, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1455, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0745, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1531, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1303, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0499, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1249, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1242, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1684, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1619, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1712, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1692, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1064, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "490 epochs, Train Rec:\t7.074e-01, KLD:\t6.113e-02\n",
      "seq_loss tensor(1.1372)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173) \n",
      "\n",
      "seq_loss tensor(1.1296)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2127)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "seq_loss tensor(1.1512)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196) \n",
      "\n",
      "seq_loss tensor(1.1328)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.2008)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225) \n",
      "\n",
      "seq_loss tensor(1.0852)\n",
      "v_loss tensor(0.0433)\n",
      "j_loss tensor(0.1947)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198) \n",
      "\n",
      "\n",
      "490 epochs, Valid Rec:\t4.180e-01, KLD:\t3.616e-02\n",
      "seq_loss tensor(1.1391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0696, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1608, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1682, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1515, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1642, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1303, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1472, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1694, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0474, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1841, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1689, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1229, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1324, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1736, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1257, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1254, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0434, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1254, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1899, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1607, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1519, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1416, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1280, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "500 epochs, Train Rec:\t7.066e-01, KLD:\t6.097e-02\n",
      "seq_loss tensor(1.1389)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2072)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182) \n",
      "\n",
      "seq_loss tensor(1.1305)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2123)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216) \n",
      "\n",
      "seq_loss tensor(1.1518)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206) \n",
      "\n",
      "seq_loss tensor(1.1332)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.2001)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234) \n",
      "\n",
      "seq_loss tensor(1.0868)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1945)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205) \n",
      "\n",
      "\n",
      "500 epochs, Valid Rec:\t4.182e-01, KLD:\t3.643e-02\n",
      "seq_loss tensor(1.1568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1654, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1686, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1659, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1978, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1503, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1515, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0905, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0371, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "510 epochs, Train Rec:\t7.063e-01, KLD:\t6.119e-02\n",
      "seq_loss tensor(1.1375)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172) \n",
      "\n",
      "seq_loss tensor(1.1280)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2125)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "seq_loss tensor(1.1498)\n",
      "v_loss tensor(0.0524)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197) \n",
      "\n",
      "seq_loss tensor(1.1316)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2001)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225) \n",
      "\n",
      "seq_loss tensor(1.0853)\n",
      "v_loss tensor(0.0424)\n",
      "j_loss tensor(0.1963)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196) \n",
      "\n",
      "\n",
      "510 epochs, Valid Rec:\t4.177e-01, KLD:\t3.615e-02\n",
      "seq_loss tensor(1.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1814, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1581, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1711, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0505, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1487, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1535, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1708, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1588, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1847, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1306, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1451, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1608, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1076, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1673, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1854, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "520 epochs, Train Rec:\t7.046e-01, KLD:\t6.090e-02\n",
      "seq_loss tensor(1.1342)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172) \n",
      "\n",
      "seq_loss tensor(1.1250)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2116)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "seq_loss tensor(1.1469)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2072)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197) \n",
      "\n",
      "seq_loss tensor(1.1293)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2002)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226) \n",
      "\n",
      "seq_loss tensor(1.0858)\n",
      "v_loss tensor(0.0429)\n",
      "j_loss tensor(0.1947)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197) \n",
      "\n",
      "\n",
      "520 epochs, Valid Rec:\t4.169e-01, KLD:\t3.616e-02\n",
      "seq_loss tensor(1.1490, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1580, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1806, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1658, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0436, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1453, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0505, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1396, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1693, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1193, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0706, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1617, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1080, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "530 epochs, Train Rec:\t7.055e-01, KLD:\t6.082e-02\n",
      "seq_loss tensor(1.1337)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169) \n",
      "\n",
      "seq_loss tensor(1.1253)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2112)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203) \n",
      "\n",
      "seq_loss tensor(1.1463)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193) \n",
      "\n",
      "seq_loss tensor(1.1297)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.1998)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221) \n",
      "\n",
      "seq_loss tensor(1.0834)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1948)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "\n",
      "530 epochs, Valid Rec:\t4.166e-01, KLD:\t3.602e-02\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1782, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1693, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0479, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1793, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1392, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1082, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1793, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1455, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1590, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0699, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1694, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1149, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1150, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0703, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1527, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1394, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0762, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0725, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1637, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "540 epochs, Train Rec:\t7.049e-01, KLD:\t6.093e-02\n",
      "seq_loss tensor(1.1327)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2080)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172) \n",
      "\n",
      "seq_loss tensor(1.1234)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2133)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206) \n",
      "\n",
      "seq_loss tensor(1.1451)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2084)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196) \n",
      "\n",
      "seq_loss tensor(1.1274)\n",
      "v_loss tensor(0.0604)\n",
      "j_loss tensor(0.2012)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223) \n",
      "\n",
      "seq_loss tensor(1.0847)\n",
      "v_loss tensor(0.0429)\n",
      "j_loss tensor(0.1966)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "\n",
      "540 epochs, Valid Rec:\t4.168e-01, KLD:\t3.611e-02\n",
      "seq_loss tensor(1.1037, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1767, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1466, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0764, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1557, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1462, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1342, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1066, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1557, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1091, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1773, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2207, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1647, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "550 epochs, Train Rec:\t7.038e-01, KLD:\t6.088e-02\n",
      "seq_loss tensor(1.1304)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169) \n",
      "\n",
      "seq_loss tensor(1.1219)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2112)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204) \n",
      "\n",
      "seq_loss tensor(1.1431)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2068)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "seq_loss tensor(1.1250)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.1991)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222) \n",
      "\n",
      "seq_loss tensor(1.0800)\n",
      "v_loss tensor(0.0424)\n",
      "j_loss tensor(0.1936)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193) \n",
      "\n",
      "\n",
      "550 epochs, Valid Rec:\t4.154e-01, KLD:\t3.606e-02\n",
      "seq_loss tensor(1.1313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1026, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0466, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1464, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1259, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1411, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1538, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1286, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1571, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0707, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1702, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "560 epochs, Train Rec:\t7.037e-01, KLD:\t6.082e-02\n",
      "seq_loss tensor(1.1319)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2066)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169) \n",
      "\n",
      "seq_loss tensor(1.1240)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2119)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203) \n",
      "\n",
      "seq_loss tensor(1.1436)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2071)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "seq_loss tensor(1.1262)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.2003)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221) \n",
      "\n",
      "seq_loss tensor(1.0808)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1946)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190) \n",
      "\n",
      "\n",
      "560 epochs, Valid Rec:\t4.159e-01, KLD:\t3.603e-02\n",
      "seq_loss tensor(1.1536, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0673, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0763, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1385, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1781, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1416, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1304, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0488, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1725, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1151, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1696, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "570 epochs, Train Rec:\t7.020e-01, KLD:\t6.059e-02\n",
      "seq_loss tensor(1.1302)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2080)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167) \n",
      "\n",
      "seq_loss tensor(1.1216)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2133)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201) \n",
      "\n",
      "seq_loss tensor(1.1431)\n",
      "v_loss tensor(0.0523)\n",
      "j_loss tensor(0.2085)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192) \n",
      "\n",
      "seq_loss tensor(1.1251)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2014)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219) \n",
      "\n",
      "seq_loss tensor(1.0814)\n",
      "v_loss tensor(0.0421)\n",
      "j_loss tensor(0.1956)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188) \n",
      "\n",
      "\n",
      "570 epochs, Valid Rec:\t4.160e-01, KLD:\t3.596e-02\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1665, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1245, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1106, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1424, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1606, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1457, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0946, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0727, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1224, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1703, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1229, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1464, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0747, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "580 epochs, Train Rec:\t7.022e-01, KLD:\t6.060e-02\n",
      "seq_loss tensor(1.1298)\n",
      "v_loss tensor(0.0622)\n",
      "j_loss tensor(0.2068)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163) \n",
      "\n",
      "seq_loss tensor(1.1205)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2122)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197) \n",
      "\n",
      "seq_loss tensor(1.1420)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2071)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188) \n",
      "\n",
      "seq_loss tensor(1.1242)\n",
      "v_loss tensor(0.0603)\n",
      "j_loss tensor(0.1997)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215) \n",
      "\n",
      "seq_loss tensor(1.0822)\n",
      "v_loss tensor(0.0427)\n",
      "j_loss tensor(0.1953)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "\n",
      "580 epochs, Valid Rec:\t4.155e-01, KLD:\t3.585e-02\n",
      "seq_loss tensor(1.1884, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1614, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0482, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1295, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1399, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1249, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1508, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1543, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1303, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0751, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0823, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "590 epochs, Train Rec:\t7.015e-01, KLD:\t6.071e-02\n",
      "seq_loss tensor(1.1282)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2073)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170) \n",
      "\n",
      "seq_loss tensor(1.1188)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2125)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204) \n",
      "\n",
      "seq_loss tensor(1.1408)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2075)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195) \n",
      "\n",
      "seq_loss tensor(1.1240)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.2006)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220) \n",
      "\n",
      "seq_loss tensor(1.0811)\n",
      "v_loss tensor(0.0419)\n",
      "j_loss tensor(0.1954)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "\n",
      "590 epochs, Valid Rec:\t4.153e-01, KLD:\t3.604e-02\n",
      "seq_loss tensor(1.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0703, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1832, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1235, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1243, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1220, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1392, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1450, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1237, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0720, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1254, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1727, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1244, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1607, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0510, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1706, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1245, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1433, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1622, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "600 epochs, Train Rec:\t7.027e-01, KLD:\t6.065e-02\n",
      "seq_loss tensor(1.1294)\n",
      "v_loss tensor(0.0628)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160) \n",
      "\n",
      "seq_loss tensor(1.1198)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2118)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "seq_loss tensor(1.1415)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185) \n",
      "\n",
      "seq_loss tensor(1.1233)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.1998)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210) \n",
      "\n",
      "seq_loss tensor(1.0808)\n",
      "v_loss tensor(0.0430)\n",
      "j_loss tensor(0.1950)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181) \n",
      "\n",
      "\n",
      "600 epochs, Valid Rec:\t4.152e-01, KLD:\t3.575e-02\n",
      "seq_loss tensor(1.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1838, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1401, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1810, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1659, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1327, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1852, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0995, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0953, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1330, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1361, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1294, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1696, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1122, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1350, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0754, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1870, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1110, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "610 epochs, Train Rec:\t7.004e-01, KLD:\t6.062e-02\n",
      "seq_loss tensor(1.1268)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167) \n",
      "\n",
      "seq_loss tensor(1.1186)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2120)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201) \n",
      "\n",
      "seq_loss tensor(1.1392)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "seq_loss tensor(1.1216)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.2005)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218) \n",
      "\n",
      "seq_loss tensor(1.0799)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1956)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189) \n",
      "\n",
      "\n",
      "610 epochs, Valid Rec:\t4.149e-01, KLD:\t3.597e-02\n",
      "seq_loss tensor(1.1028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1347, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0710, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1845, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1064, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1163, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1708, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1163, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1312, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1163, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1545, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1206, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1285, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0671, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0822, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1848, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "620 epochs, Train Rec:\t7.023e-01, KLD:\t6.037e-02\n",
      "seq_loss tensor(1.1262)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2066)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165) \n",
      "\n",
      "seq_loss tensor(1.1169)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2115)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199) \n",
      "\n",
      "seq_loss tensor(1.1378)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2067)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190) \n",
      "\n",
      "seq_loss tensor(1.1216)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.1998)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215) \n",
      "\n",
      "seq_loss tensor(1.0796)\n",
      "v_loss tensor(0.0438)\n",
      "j_loss tensor(0.1946)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "\n",
      "620 epochs, Valid Rec:\t4.145e-01, KLD:\t3.589e-02\n",
      "seq_loss tensor(1.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1398, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1567, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1725, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0495, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1272, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0493, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1137, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1698, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1533, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1346, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1478, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0748, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1046, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0418, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "630 epochs, Train Rec:\t7.017e-01, KLD:\t6.064e-02\n",
      "seq_loss tensor(1.1255)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2072)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165) \n",
      "\n",
      "seq_loss tensor(1.1171)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2120)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200) \n",
      "\n",
      "seq_loss tensor(1.1376)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "seq_loss tensor(1.1198)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.2006)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217) \n",
      "\n",
      "seq_loss tensor(1.0770)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1954)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "\n",
      "630 epochs, Valid Rec:\t4.143e-01, KLD:\t3.591e-02\n",
      "seq_loss tensor(1.0993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1712, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1259, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1776, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1052, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1549, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1453, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1327, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0989, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0736, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1239, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1462, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1489, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1205, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0832, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1118, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "640 epochs, Train Rec:\t6.999e-01, KLD:\t6.032e-02\n",
      "seq_loss tensor(1.1255)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162) \n",
      "\n",
      "seq_loss tensor(1.1172)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2116)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197) \n",
      "\n",
      "seq_loss tensor(1.1381)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187) \n",
      "\n",
      "seq_loss tensor(1.1209)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.1998)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "seq_loss tensor(1.0800)\n",
      "v_loss tensor(0.0429)\n",
      "j_loss tensor(0.1941)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183) \n",
      "\n",
      "\n",
      "640 epochs, Valid Rec:\t4.144e-01, KLD:\t3.582e-02\n",
      "seq_loss tensor(1.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1769, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0953, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0955, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1591, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1326, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1701, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1707, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0514, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0809, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1563, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "650 epochs, Train Rec:\t7.007e-01, KLD:\t6.045e-02\n",
      "seq_loss tensor(1.1280)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166) \n",
      "\n",
      "seq_loss tensor(1.1198)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2130)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201) \n",
      "\n",
      "seq_loss tensor(1.1398)\n",
      "v_loss tensor(0.0530)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192) \n",
      "\n",
      "seq_loss tensor(1.1225)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2011)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216) \n",
      "\n",
      "seq_loss tensor(1.0827)\n",
      "v_loss tensor(0.0428)\n",
      "j_loss tensor(0.1960)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "\n",
      "650 epochs, Valid Rec:\t4.155e-01, KLD:\t3.593e-02\n",
      "seq_loss tensor(1.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1315, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1441, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1098, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0480, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1707, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0495, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1268, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0718, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1598, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1240, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1690, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0765, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "660 epochs, Train Rec:\t6.993e-01, KLD:\t6.033e-02\n",
      "seq_loss tensor(1.1248)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2074)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162) \n",
      "\n",
      "seq_loss tensor(1.1165)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2124)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196) \n",
      "\n",
      "seq_loss tensor(1.1351)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2078)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187) \n",
      "\n",
      "seq_loss tensor(1.1196)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.2007)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213) \n",
      "\n",
      "seq_loss tensor(1.0778)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1961)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182) \n",
      "\n",
      "\n",
      "660 epochs, Valid Rec:\t4.142e-01, KLD:\t3.581e-02\n",
      "seq_loss tensor(1.1251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0719, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0713, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0747, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0478, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1054, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1136, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1692, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0482, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1693, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1226, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1306, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0701, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1224, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1416, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1538, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0789, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "670 epochs, Train Rec:\t6.992e-01, KLD:\t6.038e-02\n",
      "seq_loss tensor(1.1261)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2064)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161) \n",
      "\n",
      "seq_loss tensor(1.1176)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2119)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "seq_loss tensor(1.1371)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2067)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "seq_loss tensor(1.1204)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.1990)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212) \n",
      "\n",
      "seq_loss tensor(1.0813)\n",
      "v_loss tensor(0.0428)\n",
      "j_loss tensor(0.1939)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182) \n",
      "\n",
      "\n",
      "670 epochs, Valid Rec:\t4.144e-01, KLD:\t3.577e-02\n",
      "seq_loss tensor(1.1056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0898, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1245, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1450, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1216, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1136, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1517, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1573, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0696, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1597, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0422, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1672, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "680 epochs, Train Rec:\t6.984e-01, KLD:\t6.021e-02\n",
      "seq_loss tensor(1.1262)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152) \n",
      "\n",
      "seq_loss tensor(1.1169)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2116)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187) \n",
      "\n",
      "seq_loss tensor(1.1380)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178) \n",
      "\n",
      "seq_loss tensor(1.1212)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.2004)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(1.0795)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1965)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172) \n",
      "\n",
      "\n",
      "680 epochs, Valid Rec:\t4.145e-01, KLD:\t3.551e-02\n",
      "seq_loss tensor(1.1691, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1122, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1196, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0998, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1312, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0485, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1708, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1129, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1254, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0893, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1466, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0884, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1060, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "690 epochs, Train Rec:\t7.013e-01, KLD:\t6.023e-02\n",
      "seq_loss tensor(1.1253)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2067)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157) \n",
      "\n",
      "seq_loss tensor(1.1167)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2114)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192) \n",
      "\n",
      "seq_loss tensor(1.1357)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183) \n",
      "\n",
      "seq_loss tensor(1.1209)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1998)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208) \n",
      "\n",
      "seq_loss tensor(1.0801)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1961)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "\n",
      "690 epochs, Valid Rec:\t4.143e-01, KLD:\t3.567e-02\n",
      "seq_loss tensor(1.1011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0831, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1350, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1409, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1386, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1442, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0499, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0478, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1324, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1507, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1595, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0719, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0762, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1636, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1084, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1539, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0804, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1207, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1355, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1719, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0420, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1657, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "700 epochs, Train Rec:\t6.997e-01, KLD:\t6.036e-02\n",
      "seq_loss tensor(1.1246)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2072)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159) \n",
      "\n",
      "seq_loss tensor(1.1161)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2120)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "seq_loss tensor(1.1366)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2074)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185) \n",
      "\n",
      "seq_loss tensor(1.1199)\n",
      "v_loss tensor(0.0596)\n",
      "j_loss tensor(0.2006)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211) \n",
      "\n",
      "seq_loss tensor(1.0790)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1953)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183) \n",
      "\n",
      "\n",
      "700 epochs, Valid Rec:\t4.143e-01, KLD:\t3.576e-02\n",
      "seq_loss tensor(1.0863, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1673, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1630, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0950, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1285, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0716, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1519, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1752, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1294, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1102, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1695, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "710 epochs, Train Rec:\t7.004e-01, KLD:\t6.027e-02\n",
      "seq_loss tensor(1.1227)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156) \n",
      "\n",
      "seq_loss tensor(1.1153)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2119)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190) \n",
      "\n",
      "seq_loss tensor(1.1346)\n",
      "v_loss tensor(0.0532)\n",
      "j_loss tensor(0.2074)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181) \n",
      "\n",
      "seq_loss tensor(1.1193)\n",
      "v_loss tensor(0.0596)\n",
      "j_loss tensor(0.2001)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206) \n",
      "\n",
      "seq_loss tensor(1.0760)\n",
      "v_loss tensor(0.0433)\n",
      "j_loss tensor(0.1952)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "\n",
      "710 epochs, Valid Rec:\t4.138e-01, KLD:\t3.563e-02\n",
      "seq_loss tensor(1.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1799, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1213, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1707, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1496, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0495, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1361, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1657, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0696, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1106, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1503, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1058, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1635, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1219, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0809, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "720 epochs, Train Rec:\t6.992e-01, KLD:\t6.013e-02\n",
      "seq_loss tensor(1.1220)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159) \n",
      "\n",
      "seq_loss tensor(1.1134)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2111)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194) \n",
      "\n",
      "seq_loss tensor(1.1338)\n",
      "v_loss tensor(0.0530)\n",
      "j_loss tensor(0.2068)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184) \n",
      "\n",
      "seq_loss tensor(1.1173)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.1996)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210) \n",
      "\n",
      "seq_loss tensor(1.0762)\n",
      "v_loss tensor(0.0431)\n",
      "j_loss tensor(0.1939)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "\n",
      "720 epochs, Valid Rec:\t4.132e-01, KLD:\t3.573e-02\n",
      "seq_loss tensor(1.1626, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1223, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1215, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0488, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1542, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1052, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1315, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0949, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0952, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1394, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1137, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1124, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1740, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1656, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "730 epochs, Train Rec:\t6.990e-01, KLD:\t6.018e-02\n",
      "seq_loss tensor(1.1213)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2076)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160) \n",
      "\n",
      "seq_loss tensor(1.1128)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2132)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195) \n",
      "\n",
      "seq_loss tensor(1.1338)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2079)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "seq_loss tensor(1.1172)\n",
      "v_loss tensor(0.0596)\n",
      "j_loss tensor(0.2001)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211) \n",
      "\n",
      "seq_loss tensor(1.0758)\n",
      "v_loss tensor(0.0423)\n",
      "j_loss tensor(0.1940)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180) \n",
      "\n",
      "\n",
      "730 epochs, Valid Rec:\t4.133e-01, KLD:\t3.576e-02\n",
      "seq_loss tensor(1.1480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0882, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1793, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1234, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1026, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0885, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1233, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1643, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0845, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1373, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0746, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1506, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1106, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "740 epochs, Train Rec:\t6.982e-01, KLD:\t6.003e-02\n",
      "seq_loss tensor(1.1251)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2056)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149) \n",
      "\n",
      "seq_loss tensor(1.1157)\n",
      "v_loss tensor(0.0565)\n",
      "j_loss tensor(0.2109)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184) \n",
      "\n",
      "seq_loss tensor(1.1365)\n",
      "v_loss tensor(0.0532)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174) \n",
      "\n",
      "seq_loss tensor(1.1207)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1990)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199) \n",
      "\n",
      "seq_loss tensor(1.0776)\n",
      "v_loss tensor(0.0435)\n",
      "j_loss tensor(0.1942)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169) \n",
      "\n",
      "\n",
      "740 epochs, Valid Rec:\t4.139e-01, KLD:\t3.542e-02\n",
      "seq_loss tensor(1.0792, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1117, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1122, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1658, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1675, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1196, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1089, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1203, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1060, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1285, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1695, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0939, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1204, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1571, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "750 epochs, Train Rec:\t7.000e-01, KLD:\t6.001e-02\n",
      "seq_loss tensor(1.1220)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2064)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156) \n",
      "\n",
      "seq_loss tensor(1.1122)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2114)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "seq_loss tensor(1.1332)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2073)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182) \n",
      "\n",
      "seq_loss tensor(1.1168)\n",
      "v_loss tensor(0.0597)\n",
      "j_loss tensor(0.1992)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205) \n",
      "\n",
      "seq_loss tensor(1.0767)\n",
      "v_loss tensor(0.0427)\n",
      "j_loss tensor(0.1938)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175) \n",
      "\n",
      "\n",
      "750 epochs, Valid Rec:\t4.131e-01, KLD:\t3.562e-02\n",
      "seq_loss tensor(1.1442, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1630, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1128, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1270, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1230, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1412, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0853, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1550, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1550, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0773, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1026, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1854, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0756, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0712, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "760 epochs, Train Rec:\t6.987e-01, KLD:\t5.979e-02\n",
      "seq_loss tensor(1.1222)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2061)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152) \n",
      "\n",
      "seq_loss tensor(1.1121)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2113)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "seq_loss tensor(1.1331)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2064)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.1177)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1992)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(1.0781)\n",
      "v_loss tensor(0.0430)\n",
      "j_loss tensor(0.1944)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172) \n",
      "\n",
      "\n",
      "760 epochs, Valid Rec:\t4.132e-01, KLD:\t3.550e-02\n",
      "seq_loss tensor(1.0873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1149, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1117, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1052, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1665, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1218, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0811, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1510, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1651, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1536, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1561, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0766, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "770 epochs, Train Rec:\t6.979e-01, KLD:\t5.997e-02\n",
      "seq_loss tensor(1.1224)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2068)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154) \n",
      "\n",
      "seq_loss tensor(1.1134)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2115)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188) \n",
      "\n",
      "seq_loss tensor(1.1332)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "seq_loss tensor(1.1180)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.1999)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1205) \n",
      "\n",
      "seq_loss tensor(1.0753)\n",
      "v_loss tensor(0.0427)\n",
      "j_loss tensor(0.1939)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175) \n",
      "\n",
      "\n",
      "770 epochs, Valid Rec:\t4.132e-01, KLD:\t3.557e-02\n",
      "seq_loss tensor(1.1548, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1112, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0524, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1127, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1098, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1709, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1060, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1847, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0707, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1214, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1362, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1638, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1254, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1845, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1424, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1175, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0706, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "780 epochs, Train Rec:\t6.980e-01, KLD:\t5.963e-02\n",
      "seq_loss tensor(1.1210)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2073)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151) \n",
      "\n",
      "seq_loss tensor(1.1118)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2123)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187) \n",
      "\n",
      "seq_loss tensor(1.1332)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.1173)\n",
      "v_loss tensor(0.0604)\n",
      "j_loss tensor(0.1997)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(1.0774)\n",
      "v_loss tensor(0.0423)\n",
      "j_loss tensor(0.1931)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173) \n",
      "\n",
      "\n",
      "780 epochs, Valid Rec:\t4.132e-01, KLD:\t3.550e-02\n",
      "seq_loss tensor(1.0678, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1294, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0481, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0762, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0909, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1691, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1446, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1831, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1236, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1465, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0676, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0939, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1299, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1699, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0364, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1677, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1264, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "790 epochs, Train Rec:\t6.958e-01, KLD:\t5.971e-02\n",
      "seq_loss tensor(1.1204)\n",
      "v_loss tensor(0.0622)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149) \n",
      "\n",
      "seq_loss tensor(1.1117)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2109)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184) \n",
      "\n",
      "seq_loss tensor(1.1326)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175) \n",
      "\n",
      "seq_loss tensor(1.1165)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.1991)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199) \n",
      "\n",
      "seq_loss tensor(1.0769)\n",
      "v_loss tensor(0.0432)\n",
      "j_loss tensor(0.1944)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170) \n",
      "\n",
      "\n",
      "790 epochs, Valid Rec:\t4.129e-01, KLD:\t3.543e-02\n",
      "seq_loss tensor(1.1421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1475, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1763, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0479, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1277, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0776, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0692, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1268, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0725, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1517, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1804, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0510, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1117, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0727, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0800, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1107, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "800 epochs, Train Rec:\t6.977e-01, KLD:\t5.955e-02\n",
      "seq_loss tensor(1.1198)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2061)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143) \n",
      "\n",
      "seq_loss tensor(1.1112)\n",
      "v_loss tensor(0.0558)\n",
      "j_loss tensor(0.2110)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178) \n",
      "\n",
      "seq_loss tensor(1.1321)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2064)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169) \n",
      "\n",
      "seq_loss tensor(1.1184)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1988)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193) \n",
      "\n",
      "seq_loss tensor(1.0774)\n",
      "v_loss tensor(0.0432)\n",
      "j_loss tensor(0.1936)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163) \n",
      "\n",
      "\n",
      "800 epochs, Valid Rec:\t4.128e-01, KLD:\t3.523e-02\n",
      "seq_loss tensor(1.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0483, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1142, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0490, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1204, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1033, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1249, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1208, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1566, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1791, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1668, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1837, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0524, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1539, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1212, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0719, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1272, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0674, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0461, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1675, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "810 epochs, Train Rec:\t6.972e-01, KLD:\t5.950e-02\n",
      "seq_loss tensor(1.1206)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142) \n",
      "\n",
      "seq_loss tensor(1.1116)\n",
      "v_loss tensor(0.0562)\n",
      "j_loss tensor(0.2113)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "seq_loss tensor(1.1319)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2069)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167) \n",
      "\n",
      "seq_loss tensor(1.1157)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1995)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192) \n",
      "\n",
      "seq_loss tensor(1.0757)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1953)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159) \n",
      "\n",
      "\n",
      "810 epochs, Valid Rec:\t4.128e-01, KLD:\t3.518e-02\n",
      "seq_loss tensor(1.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1346, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1366, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1225, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1132, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0884, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0904, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0478, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1249, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1116, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1430, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1511, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1398, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1245, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0491, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "820 epochs, Train Rec:\t6.967e-01, KLD:\t5.930e-02\n",
      "seq_loss tensor(1.1199)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2071)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142) \n",
      "\n",
      "seq_loss tensor(1.1108)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2122)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175) \n",
      "\n",
      "seq_loss tensor(1.1305)\n",
      "v_loss tensor(0.0525)\n",
      "j_loss tensor(0.2071)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166) \n",
      "\n",
      "seq_loss tensor(1.1166)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.2001)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190) \n",
      "\n",
      "seq_loss tensor(1.0770)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1962)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159) \n",
      "\n",
      "\n",
      "820 epochs, Valid Rec:\t4.129e-01, KLD:\t3.515e-02\n",
      "seq_loss tensor(1.1178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1086, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0708, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0934, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1098, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1336, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1439, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1124, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1211, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1132, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1528, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1124, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1195, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "830 epochs, Train Rec:\t6.985e-01, KLD:\t5.942e-02\n",
      "seq_loss tensor(1.1204)\n",
      "v_loss tensor(0.0623)\n",
      "j_loss tensor(0.2059)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138) \n",
      "\n",
      "seq_loss tensor(1.1114)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2107)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173) \n",
      "\n",
      "seq_loss tensor(1.1312)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164) \n",
      "\n",
      "seq_loss tensor(1.1164)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1980)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188) \n",
      "\n",
      "seq_loss tensor(1.0753)\n",
      "v_loss tensor(0.0435)\n",
      "j_loss tensor(0.1923)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158) \n",
      "\n",
      "\n",
      "830 epochs, Valid Rec:\t4.125e-01, KLD:\t3.509e-02\n",
      "seq_loss tensor(1.0973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1375, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1102, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1752, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1378, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1573, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0594, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1661, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "840 epochs, Train Rec:\t6.987e-01, KLD:\t5.935e-02\n",
      "seq_loss tensor(1.1179)\n",
      "v_loss tensor(0.0623)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141) \n",
      "\n",
      "seq_loss tensor(1.1091)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2126)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.1292)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2077)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167) \n",
      "\n",
      "seq_loss tensor(1.1150)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2000)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "seq_loss tensor(1.0747)\n",
      "v_loss tensor(0.0422)\n",
      "j_loss tensor(0.1943)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160) \n",
      "\n",
      "\n",
      "840 epochs, Valid Rec:\t4.123e-01, KLD:\t3.517e-02\n",
      "seq_loss tensor(1.0924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1362, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1541, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0524, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0468, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1385, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1076, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1310, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1309, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1668, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1236, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1086, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1596, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0822, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1872, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "850 epochs, Train Rec:\t6.970e-01, KLD:\t5.931e-02\n",
      "seq_loss tensor(1.1176)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2073)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141) \n",
      "\n",
      "seq_loss tensor(1.1085)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2125)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.1289)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2075)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167) \n",
      "\n",
      "seq_loss tensor(1.1134)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.2006)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191) \n",
      "\n",
      "seq_loss tensor(1.0732)\n",
      "v_loss tensor(0.0433)\n",
      "j_loss tensor(0.1970)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160) \n",
      "\n",
      "\n",
      "850 epochs, Valid Rec:\t4.124e-01, KLD:\t3.517e-02\n",
      "seq_loss tensor(1.1275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1107, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1116, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0715, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1788, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1084, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0530, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1188, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1748, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1102, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0857, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0692, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0822, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0662, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0510, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1038, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "860 epochs, Train Rec:\t6.983e-01, KLD:\t5.912e-02\n",
      "seq_loss tensor(1.1176)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2060)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138) \n",
      "\n",
      "seq_loss tensor(1.1090)\n",
      "v_loss tensor(0.0558)\n",
      "j_loss tensor(0.2110)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173) \n",
      "\n",
      "seq_loss tensor(1.1287)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164) \n",
      "\n",
      "seq_loss tensor(1.1144)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1995)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189) \n",
      "\n",
      "seq_loss tensor(1.0760)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1940)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159) \n",
      "\n",
      "\n",
      "860 epochs, Valid Rec:\t4.121e-01, KLD:\t3.510e-02\n",
      "seq_loss tensor(1.1225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1709, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1398, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1115, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1149, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0491, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1190, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0767, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1538, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1227, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1549, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0738, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1243, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1366, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1311, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1884, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1201, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "870 epochs, Train Rec:\t6.955e-01, KLD:\t5.905e-02\n",
      "seq_loss tensor(1.1186)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2061)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137) \n",
      "\n",
      "seq_loss tensor(1.1103)\n",
      "v_loss tensor(0.0563)\n",
      "j_loss tensor(0.2108)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172) \n",
      "\n",
      "seq_loss tensor(1.1301)\n",
      "v_loss tensor(0.0531)\n",
      "j_loss tensor(0.2064)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163) \n",
      "\n",
      "seq_loss tensor(1.1165)\n",
      "v_loss tensor(0.0596)\n",
      "j_loss tensor(0.1993)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1186) \n",
      "\n",
      "seq_loss tensor(1.0774)\n",
      "v_loss tensor(0.0436)\n",
      "j_loss tensor(0.1936)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156) \n",
      "\n",
      "\n",
      "870 epochs, Valid Rec:\t4.126e-01, KLD:\t3.505e-02\n",
      "seq_loss tensor(1.1374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0728, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0554, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0783, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1103, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1413, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1295, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0717, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1735, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1523, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1132, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1409, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1700, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1286, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0440, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "880 epochs, Train Rec:\t6.966e-01, KLD:\t5.902e-02\n",
      "seq_loss tensor(1.1163)\n",
      "v_loss tensor(0.0627)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131) \n",
      "\n",
      "seq_loss tensor(1.1074)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2112)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166) \n",
      "\n",
      "seq_loss tensor(1.1280)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157) \n",
      "\n",
      "seq_loss tensor(1.1138)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.1989)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181) \n",
      "\n",
      "seq_loss tensor(1.0760)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1930)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150) \n",
      "\n",
      "\n",
      "880 epochs, Valid Rec:\t4.118e-01, KLD:\t3.487e-02\n",
      "seq_loss tensor(1.1033, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1027, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1113, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1378, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1117, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0703, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1140, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0723, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0480, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1110, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1310, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1117, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1205, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1209, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1136, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0693, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1216, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1114, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0834, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1023, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "890 epochs, Train Rec:\t6.957e-01, KLD:\t5.899e-02\n",
      "seq_loss tensor(1.1163)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2057)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130) \n",
      "\n",
      "seq_loss tensor(1.1081)\n",
      "v_loss tensor(0.0558)\n",
      "j_loss tensor(0.2102)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165) \n",
      "\n",
      "seq_loss tensor(1.1279)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2059)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156) \n",
      "\n",
      "seq_loss tensor(1.1144)\n",
      "v_loss tensor(0.0600)\n",
      "j_loss tensor(0.1988)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "seq_loss tensor(1.0752)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1932)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148) \n",
      "\n",
      "\n",
      "890 epochs, Valid Rec:\t4.117e-01, KLD:\t3.483e-02\n",
      "seq_loss tensor(1.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1545, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0898, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1513, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0467, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1185, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0983, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1595, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1140, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0694, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1101, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1112, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1684, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1124, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1566, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0699, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0939, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1102, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0716, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "900 epochs, Train Rec:\t6.949e-01, KLD:\t5.890e-02\n",
      "seq_loss tensor(1.1160)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133) \n",
      "\n",
      "seq_loss tensor(1.1072)\n",
      "v_loss tensor(0.0557)\n",
      "j_loss tensor(0.2116)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168) \n",
      "\n",
      "seq_loss tensor(1.1276)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2072)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159) \n",
      "\n",
      "seq_loss tensor(1.1130)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.1992)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181) \n",
      "\n",
      "seq_loss tensor(1.0732)\n",
      "v_loss tensor(0.0427)\n",
      "j_loss tensor(0.1939)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152) \n",
      "\n",
      "\n",
      "900 epochs, Valid Rec:\t4.116e-01, KLD:\t3.492e-02\n",
      "seq_loss tensor(1.1464, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1326, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0826, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1098, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1206, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1095, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1696, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1596, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1361, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1741, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0838, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1075, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "910 epochs, Train Rec:\t6.952e-01, KLD:\t5.891e-02\n",
      "seq_loss tensor(1.1166)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129) \n",
      "\n",
      "seq_loss tensor(1.1066)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2116)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163) \n",
      "\n",
      "seq_loss tensor(1.1279)\n",
      "v_loss tensor(0.0529)\n",
      "j_loss tensor(0.2071)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154) \n",
      "\n",
      "seq_loss tensor(1.1132)\n",
      "v_loss tensor(0.0596)\n",
      "j_loss tensor(0.1994)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "seq_loss tensor(1.0743)\n",
      "v_loss tensor(0.0433)\n",
      "j_loss tensor(0.1941)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146) \n",
      "\n",
      "\n",
      "910 epochs, Valid Rec:\t4.118e-01, KLD:\t3.478e-02\n",
      "seq_loss tensor(1.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0722, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1134, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0491, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1690, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0806, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1112, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1523, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1708, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1264, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1697, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1254, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1181, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1054, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0482, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1183, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0935, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0621, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1312, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1114, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1112, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1437, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0591, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1113, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1110, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "920 epochs, Train Rec:\t6.940e-01, KLD:\t5.869e-02\n",
      "seq_loss tensor(1.1153)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2061)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129) \n",
      "\n",
      "seq_loss tensor(1.1058)\n",
      "v_loss tensor(0.0558)\n",
      "j_loss tensor(0.2110)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164) \n",
      "\n",
      "seq_loss tensor(1.1270)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155) \n",
      "\n",
      "seq_loss tensor(1.1147)\n",
      "v_loss tensor(0.0602)\n",
      "j_loss tensor(0.1988)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178) \n",
      "\n",
      "seq_loss tensor(1.0743)\n",
      "v_loss tensor(0.0425)\n",
      "j_loss tensor(0.1938)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148) \n",
      "\n",
      "\n",
      "920 epochs, Valid Rec:\t4.115e-01, KLD:\t3.481e-02\n",
      "seq_loss tensor(1.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1222, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0633, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1628, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1448, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1259, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0987, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0487, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1679, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1361, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1807, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1088, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1519, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0989, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1411, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0691, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0505, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1088, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0859, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1081, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1189, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "930 epochs, Train Rec:\t6.958e-01, KLD:\t5.884e-02\n",
      "seq_loss tensor(1.1146)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2066)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127) \n",
      "\n",
      "seq_loss tensor(1.1053)\n",
      "v_loss tensor(0.0561)\n",
      "j_loss tensor(0.2115)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162) \n",
      "\n",
      "seq_loss tensor(1.1266)\n",
      "v_loss tensor(0.0530)\n",
      "j_loss tensor(0.2066)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152) \n",
      "\n",
      "seq_loss tensor(1.1123)\n",
      "v_loss tensor(0.0601)\n",
      "j_loss tensor(0.1995)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.0739)\n",
      "v_loss tensor(0.0433)\n",
      "j_loss tensor(0.1942)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146) \n",
      "\n",
      "\n",
      "930 epochs, Valid Rec:\t4.115e-01, KLD:\t3.474e-02\n",
      "seq_loss tensor(1.1625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0689, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1427, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0499, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1366, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1719, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1336, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1110, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1224, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1700, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1129, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1102, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0701, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1107, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0711, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1825, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0499, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0851, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1312, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1193, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1155, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0771, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1872, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "940 epochs, Train Rec:\t6.943e-01, KLD:\t5.870e-02\n",
      "seq_loss tensor(1.1143)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2075)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126) \n",
      "\n",
      "seq_loss tensor(1.1065)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2124)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161) \n",
      "\n",
      "seq_loss tensor(1.1257)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2082)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152) \n",
      "\n",
      "seq_loss tensor(1.1116)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.2010)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175) \n",
      "\n",
      "seq_loss tensor(1.0736)\n",
      "v_loss tensor(0.0424)\n",
      "j_loss tensor(0.1966)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145) \n",
      "\n",
      "\n",
      "940 epochs, Valid Rec:\t4.117e-01, KLD:\t3.471e-02\n",
      "seq_loss tensor(1.0858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1314, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0810, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1100, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1018, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0745, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1113, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1116, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1669, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1303, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1166, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0480, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1115, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1103, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1132, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1198, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1569, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1545, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1111, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1707, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1168, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0433, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1065, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "950 epochs, Train Rec:\t6.946e-01, KLD:\t5.888e-02\n",
      "seq_loss tensor(1.1149)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2061)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127) \n",
      "\n",
      "seq_loss tensor(1.1056)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2107)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1162) \n",
      "\n",
      "seq_loss tensor(1.1257)\n",
      "v_loss tensor(0.0526)\n",
      "j_loss tensor(0.2064)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153) \n",
      "\n",
      "seq_loss tensor(1.1130)\n",
      "v_loss tensor(0.0599)\n",
      "j_loss tensor(0.1994)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176) \n",
      "\n",
      "seq_loss tensor(1.0752)\n",
      "v_loss tensor(0.0426)\n",
      "j_loss tensor(0.1947)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145) \n",
      "\n",
      "\n",
      "950 epochs, Valid Rec:\t4.114e-01, KLD:\t3.473e-02\n",
      "seq_loss tensor(1.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1150, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1763, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1174, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1087, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1785, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0642, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1110, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0541, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0775, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0733, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1527, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0632, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1701, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1107, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1064, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0602, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1566, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1730, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1362, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0609, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0852, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1801, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1255, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "960 epochs, Train Rec:\t6.942e-01, KLD:\t5.859e-02\n",
      "seq_loss tensor(1.1139)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2063)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130) \n",
      "\n",
      "seq_loss tensor(1.1044)\n",
      "v_loss tensor(0.0560)\n",
      "j_loss tensor(0.2115)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165) \n",
      "\n",
      "seq_loss tensor(1.1249)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156) \n",
      "\n",
      "seq_loss tensor(1.1113)\n",
      "v_loss tensor(0.0595)\n",
      "j_loss tensor(0.1994)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "seq_loss tensor(1.0738)\n",
      "v_loss tensor(0.0424)\n",
      "j_loss tensor(0.1939)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148) \n",
      "\n",
      "\n",
      "960 epochs, Valid Rec:\t4.111e-01, KLD:\t3.483e-02\n",
      "seq_loss tensor(1.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1145, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1765, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1180, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1112, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1087, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0963, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1102, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0707, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0455, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1683, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1689, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1199, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0702, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1191, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1129, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1450, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1082, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1135, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0783, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1173, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "970 epochs, Train Rec:\t6.945e-01, KLD:\t5.876e-02\n",
      "seq_loss tensor(1.1126)\n",
      "v_loss tensor(0.0625)\n",
      "j_loss tensor(0.2061)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130) \n",
      "\n",
      "seq_loss tensor(1.1041)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2108)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165) \n",
      "\n",
      "seq_loss tensor(1.1239)\n",
      "v_loss tensor(0.0528)\n",
      "j_loss tensor(0.2059)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156) \n",
      "\n",
      "seq_loss tensor(1.1113)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.1988)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1179) \n",
      "\n",
      "seq_loss tensor(1.0725)\n",
      "v_loss tensor(0.0420)\n",
      "j_loss tensor(0.1937)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148) \n",
      "\n",
      "\n",
      "970 epochs, Valid Rec:\t4.107e-01, KLD:\t3.483e-02\n",
      "seq_loss tensor(1.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1394, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1187, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1267, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1118, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1183, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1147, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1707, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1111, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1165, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1122, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1294, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1140, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1127, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1127, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1118, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1105, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1130, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1531, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1131, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1682, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1137, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0348, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "980 epochs, Train Rec:\t6.951e-01, KLD:\t5.888e-02\n",
      "seq_loss tensor(1.1155)\n",
      "v_loss tensor(0.0633)\n",
      "j_loss tensor(0.2056)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1134) \n",
      "\n",
      "seq_loss tensor(1.1062)\n",
      "v_loss tensor(0.0564)\n",
      "j_loss tensor(0.2107)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169) \n",
      "\n",
      "seq_loss tensor(1.1261)\n",
      "v_loss tensor(0.0535)\n",
      "j_loss tensor(0.2056)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160) \n",
      "\n",
      "seq_loss tensor(1.1120)\n",
      "v_loss tensor(0.0596)\n",
      "j_loss tensor(0.1987)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1182) \n",
      "\n",
      "seq_loss tensor(1.0755)\n",
      "v_loss tensor(0.0435)\n",
      "j_loss tensor(0.1929)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1152) \n",
      "\n",
      "\n",
      "980 epochs, Valid Rec:\t4.114e-01, KLD:\t3.495e-02\n",
      "seq_loss tensor(1.1213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1732, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1791, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1159, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1091, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1160, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1794, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0467, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1164, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1144, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0770, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1168, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0549, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1175, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1285, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1136, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0847, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0693, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1116, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1351, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1044, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1695, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1124, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0678, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1104, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1044, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0601, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1156, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0882, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1687, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1123, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1787, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1167, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0917, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1881, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "990 epochs, Train Rec:\t6.932e-01, KLD:\t5.864e-02\n",
      "seq_loss tensor(1.1127)\n",
      "v_loss tensor(0.0624)\n",
      "j_loss tensor(0.2060)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128) \n",
      "\n",
      "seq_loss tensor(1.1036)\n",
      "v_loss tensor(0.0559)\n",
      "j_loss tensor(0.2111)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163) \n",
      "\n",
      "seq_loss tensor(1.1241)\n",
      "v_loss tensor(0.0527)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1153) \n",
      "\n",
      "seq_loss tensor(1.1107)\n",
      "v_loss tensor(0.0598)\n",
      "j_loss tensor(0.1988)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1177) \n",
      "\n",
      "seq_loss tensor(1.0720)\n",
      "v_loss tensor(0.0422)\n",
      "j_loss tensor(0.1945)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1146) \n",
      "\n",
      "\n",
      "990 epochs, Valid Rec:\t4.107e-01, KLD:\t3.476e-02\n",
      "seq_loss tensor(1.0954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1125, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1141, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1586, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1184, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1107, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1129, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0508, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1124, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0495, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1727, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1126, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1163, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0987, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0523, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1761, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1151, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0555, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1161, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0702, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1133, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1063, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1171, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1058, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1138, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1172, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1148, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0543, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1110, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1175, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1149, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1525, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1158, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1428, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1132, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1169, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0766, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0697, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1121, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1330, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1124, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1139, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1157, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1142, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0788, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1120, grad_fn=<MulBackward0>) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b2f5fdd0>"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACIQAAAOMCAYAAAA2PPoSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAABuvAAAbrwFeGpEcAAEAAElEQVR4nOzdeXhU9b3H8c+ZmUwy2UMICRAIq2DFDbBuKCi27ljcUVu1Uq9o63LdN6oVUWy1V8DiRhWtoqLFgoqoKIoIFJHVhSWQhJCFBLJNltnvH5FJJglbMpmZJO/X8/S58zvnd37nO4dM5j5PPn5/hs/n8wkAAAAAAAAAAAAAAACdhincBQAAAAAAAAAAAAAAACC4CIQAAAAAAAAAAAAAAAB0MgRCAAAAAAAAAAAAAAAAOhkCIQAAAAAAAAAAAAAAAJ0MgRAAAAAAAAAAAAAAAIBOhkAIAAAAAAAAAAAAAABAJ0MgBAAAAAAAAAAAAAAAoJMhEAIAAAAAAAAAAAAAANDJEAgBAAAAAAAAAAAAAADoZAiEAAAAAAAAAAAAAAAAdDIEQgAAAAAAAAAAAAAAADoZAiEAAAAAAAAAAAAAAACdDIEQAAAAAAAAAAAAAACAToZACAAAAAAAAAAAAAAAQCdDIAQAAAAAAAAAAAAAAKCTIRACAAAAAAAAAAAAAADQyRAIAQAAAAAAAAAAAAAA6GQIhAAAAAAAAAAAAAAAAHQylnAXEMlKSqrCXUKXk5aWoJoy6eFuDccGne3WyW8t0ofJ4/zHRlTfrxE194ehQgBAe0lLS5DE9y8AdEV8BwBA18TvfwDouvgOAICui++AQ7PvObUVHUIQcWKSJBk+/7iuQjL7bAFz3EZdiKsCAAAAAAAAAAAAAKDjIBCCiGMySdGNAk+OCkMWxQTM8ag2xFUBAAAAAAAAAAAAANBxEAhBRIpJbtwhxJCFDiEAAAAAAAAAAAAAABwyAiGISNGJjQIh5YbMvsAOIW6DDiEAAAAAAAAAAAAAAOwPgRBEpMYdQlw1hgxnYIcQDx1CAAAAAAAAAAAAAADYLwIhiEgxjTqESJK7IjZwLDqEAAAAAAAAAAAAAACwPwRCEJEadwiRJHcFHUIAAAAAAAAAAAAAADhUBEIQkWKSAsfOcotMvij/2G3QIQQAAAAAAAAAAAAAgP0hEIKIFJ0U2CGkrtyQ2dfQJYRACAAAAAAAAAAAAAAA+2cJdwFAS2KaBEIclYaifLFyqVKS5FZNOMoCAAAAAAAAAAAAOiW326W6uhq5XA75fD75fAe/BjhcNTV7JUm1ta4wV9J+DEMyDENRUdGKiYmVxRJ18IvaCYEQRKSmgZC6ckMWX6x/7DYIhAAAAAAAAAAAAABt5fP5VFm5V7W19nCXgi7A43FKkrxeb5graX8OR63s9nLZbPFKTOwmwzBCXgOBEESkmOQmgZBKQ1GK849dBEIAAAAAAAAAAACANmsaBjGbLTKZzJJC/8drdH4mU/3/7dx5EJ+8Xo88Hrck+T9fSUmpIa+EQAgiUnRi0w4hksVn84/dRrV88sngiwgAAAAAAAAAAABoFbfb5f9jtdkcpeTk7rJYosLSyQBdg8VSnwhxuzt1IkQ+n09ut0vl5aXyeOo/Z3FxiSHfPsYU0rsBhygmOXDsqDBk8TV0CPEZHnnlDG1RAAAAAAAAAAAAQCdSV9fQlT85ubuioqyEQYAgMAxDUVFWJSd39x9r/HkLFQIhiEgxSU06hFQYsvhiA4652TYGAAAAAAAAAAAAaDWXyyGpfpuYUHcuALoCiyVKZnP9xi0uV+gbHhAIQURqKRAS1SQQ4iIQAgAAAAAAAAAAALSaz1f/NzmTyUxnEKAdGIYhk8ksSfL5Qr9NDoEQRCRLjGSJaQiF1DXZMkaiQwgAAAAAAAAAAADQFj7/n+MIgwDtp/7z5fMdZFo7IBCCiBWd2CgQUt7SljHVoS4JAAAAAAAAAAAAAIAOgUAIIlZMckMgxFGpZoEQtowBAAAAAAAAAAAAAKBlBEIQsaITGl47KlvoECI6hAAAAAAAAAAAAAAA0BICIYhY0QkNHUK8bkOmuoSA83QIAQAAAAAAAAAAAACgZQRCELGs8b6AsVGVHDB2EwgBAAAAAAAAAAAAECHq6upUWFjQ7vcpLCzQqFEjNWrUSOXn72z3+x3M7t3Fqq62B3XNnJwd8vl8B594CC699EKNGjVSCxe+H5T1OhICIYhY1vjAsa8iKWBMIAQAAAAAAAAAAABAJPjkk481YcLF+vbb/4a7lJBxuVx6+eXnNWHCxSorKwvKmtXVdj3zzDRde+2V8ng8QVmzK7OEuwBgf5p2CPFVJQaMCYQAAAAAAAAAAAAAiAQvvvicSkp2h+ReaWk99MYb70qSMjJ6huSeLSktLdGrr74c1DU3b/5J//73vKCu2ZURCEHEah4IiQsYu4zqUJYDAAAAAAAAAAAAAGFnsViUldUv3GWgA2DLGESsplvGeO2BgRA6hAAAAAAAAAAAAAAA0DI6hCBiWeMCO4R46RACAAAAAAAAAAAAIILMnv2CXnnlJf942rQpmjZtiq6//g/q2bOXpk59VGPH/kqXXHKFnnnmKeXm7lBiYpKuuuq3uuKKqyVJpaWleu+9t/Xtt6uUn5+vmppqxcbGKSurn0aPPlMXX3ypoqNj/PcoLCzQZZeNkyS99dZ8ZWb2Cajl6quv1YQJv9Wrr76s5cu/UmlpieLjEzR8+Aj97nc3aODAQW1+33/8441at+47//jKK8dLkqZPf17Dh4/0H//ppx80b95crV37nfbu3aPY2FgNHDhYZ599ns499wKZzWb/3EsvvVBFRYX+8ZgxJ0mS5s1boJ49e0mSHI46ffDBAi1btlTZ2dtUVVUpqzVa6enp+uUvT9aECdeoe/e0Nr+/zoJACCJW0y1jvFW2gLFbtaEsBwAAAAAAAAAAAAACpKdn6Oijj9XmzT/K6XQqM7OPUlK6KT09wz8nNzdXd955q8xmk/r3H6CcnBz17z9QkrRp00bdddetsturZLVGq3fv3rJYMlRYWKBNmzZo06YN+vrrLzV9+vMB4YkDKSoq1PXXX6XS0hJlZPRUVlZ/bd++TUuWfKrly5fpuede1pAhQ9v0vgcOHKS6ujr99NMPkqQhQ46U1WpVfHzDNhBvvDFHL7zwnLxer+Li4jRo0BGqqCjX2rVrtHbtGn388Yd68sln/NcMHfoLxcbGavv2bEnS0UcfK0myWq2SpLKyMt1++yRlZ2+TYRjq3TtT6ekZKikp1o4d27Vjx3Z98skizZ79unr0SG/T++ssCIQgYkUnBI499piAMVvGAAAAAAAAAAAAAO2r3LxVK+LvVZl5S7hLCYoUzxE62T5NyZ7BQVnvggsu0gUXXOTvbnH11dfqwgt/I0n66KOFkqRt27boqKOO1tNPz1B8fLwqKsqVmJgkj8ejxx57WHZ7lU47bYzuv3+yEhMTJUlut1tvvfUvPf/8TK1fv1arVq3QKaeMOqSaliz5RH37Zumll+Zo6NBfSJLy8nJ0++23aPfuYr366kt64omn2/S+77jjnoBOJY8+OtXfqUSSli5dolmzZkiSrrtuon73u9/LarXKYjFpzZrVevjhB7Ru3XeaMmWynnzyGUnSlCnT9N133+rWW2+SJM2Y8YIsloZIwz/+8ayys7cpM7OP/vrXZ9WnT1//uVWrVujBB+9WWdlezZv3lm655bY2vb/OgkAIIlbTDiHuqujAMVvGAAAAAAAAAAAAAO3qm/i7lW/9PNxlBI3dnCdf/N06r+L9kN73xhtv9nfCSEpKliRt27ZVFRUVslqtuu++h/xhEEmyWCy65prrtGDBfBUU7FJ29rZDDoRI0p///HhAF5C+ffvp8ssnaObM/9PGjeuD86YO4MUX/yFJGjduvCZOvCng3IgRJ2jq1L/qppt+r6+//krr16/Tscced8D13G631q9fK8Mw9Kc//W9AGESSTjzxZI0d+2t9+OECZWdvC+p76cgIhCBiNQ+EWAPGLjqEAAAAAAAAAAAAAIhwJpNJw4Yd3ez4kCFD9fHHX8jhqFN0dEyz806nU4mJSSoo2CWHo+6Q79e9e1qLW8JkZfWXJFVVVR1G9Ydv58485eXlSpIuv/yqFucMG3aMjj76GG3cuEHLli09aCDEYrHonXf+I4fD4d9CpjGfzyebLVaSDutZdXYEQhCxrPGBY5fdJLMvWh7DIYkOIQAAAAAAAAAAAEB7O8X+V62Iu09lls3hLiUoUtxDdHL1kyG9Z3x8QouBj32io2O0c2eefvrpB+3ala+Cgl3asWO7srO3yems/9uo1+s95PulpaXt5z71OzJ4PJ7DqP7w5ebmSJJiYmLUr1///c4bMuRIbdy4wR8eORTR0dEqK9ur77/fqLy8PBUWFigvL0dbtmxWVVWlpMN7Vp0dgRBErKYdQpzVhiy+2EaBEDqEAAAAAAAAAAAAAO0p2TNY51a+F+4yOrR9QYyWfP/9Js2aNV3r1n0XcDw5OVknn3yKtmzZosLCXYd1P4slqlV1BktNTf1/2B8XF3fAebGxcQHzD2bPnlI999yz+vzzT+V2u/3HY2JidOSRR8nj8WjDhnWtK7qTIhCCiNU0EOKwG7L44uRQmSS2jAEAAAAAAAAAAADQceXk7NCtt/6PHA6H+vUboPPPH6dBgwarX7/+SkvrIUmaNOn3hx0ICbfY2PqtW6qrDxz02Ld1zb5gyIE4HA7ddtsk5eTsUGJiksaPv1RDhx6prKz+6t07U2azWS+88ByBkCYIhCBiWZt87p12KcoX6x/TIQQAAAAAAAAAAABARzVv3lw5HA5lZfXTyy+/ppiY5tvK7N69OwyVtU3fvv0kSXV1dcrJ2bHfbWN++ukHSVKfPn0OuuayZUuVk7Pj5+DHK+rTp2+zOSUlHe9ZtTdTuAsA9sdklqJiG7qEOH/uELIPgRAAAAAAAAAAAAAAkcAw6v/07vP5DjKzQWFhgSQpK6t/i2GQ1atXqri4SJLk8XiCUGVw7XvPUuD77ts3S337ZkmS3nnnzRav3bhxvX788XtJ0kknneI/bjK1HGHY96xiY+NaDIPs3btH33zztaTIfFbhQiAEEa3xtjHOKkMWn80/dhs18skbjrIAAAAAAAAAAAAAwC82tv7vmEVFhYd8zb5OGqtXr9T69ev8x91utz799GNNnvyA/5jDUReUOoNp33uWmr/viRMnSZIWLJiv2bNfkNPp9J9bs+ZbPfTQPZKkE088RSNHnug/Z7M17BjReM19z6qqqlLvvDM3IICyadNG3X77zaqsrJAUmc8qXNgyBhHNGi9V/9zZx2mXohQbcN6tWkXp4HtKAQAAAAAAAAAAAEB7GTx4iLKzt+mNN+Zo5crlOv30M9SjR/oBr7nyymv02Wcfq7y8XLfcMlGZmX0VFxengoJdqqqqlM0Wq2HDjtGmTRsicuuYxMQkZWT0VFFRoR544G5lZfXTxIk36aSTTtGZZ56lXbtu0Ysv/kOvvPKS3nnnTfXtm6Xy8nJ/t4/jjhuuyZP/IsMw/Gv26dNXNptNtbW1uvHG69SrV2/dd9/DGjXqdB199DHauHGDpk9/Wm+8MUdpaT20Z0+pSkp2yzAMjRz5S3377X9VWloin88XsG5XRYcQRLSADiHVgVvGSGwbAwAAAAAAAAAAACD8brnldo0Zc6ZsNptyc3OUm5tz0GsyMjL06qtv6Te/uVR9+vTV7t3FysvLUWpqqi699ArNmTNXN954syTpu+++VW1tbTu/i8M3Zco0DRt2jLxej3buzNWuXTv953772+v1wguv6Fe/OkdxcfHatm2r6urqNHLkL/Xgg49o+vTnlZSUHLBebGysHntsmgYNOkJ1dbUqKNilwsJdMpvN+r//+4cmTfqTBg+uP7d9+zaZzWaNHfsrzZz5kp588hlZrdGqqKjQxo3rQ/wkIpPhO5xNjLqYkpKqcJfQ5aSlJUhqePZv/Mamnd80NLL5pf1qbYtr2Gfqyj0blOjtF9IaAQDto+l3AACg6+A7AAC6Jn7/A0DXxXcAEDn27CmWy1WnqKgYpaYeuJsFEAwWS33PCrfbG+ZKQqc1n7N935VtRYcQRDRrfODYVNUtYOw2Ii8FBwAAAAAAAAAAAABAuBEIQUSLjm/SwKYqJWDoNqpDWA0AAAAAAAAAAAAAAB2D5eBTgPCxxgUGQozKpICx26gJZTkAAAAAAAAAAAAA0Cm89to/tWLF8lZdO2vW7CBXg/ZAIAQRremWMb6q5ICxiw4hAAAAAAAAAAAAAHDYdu7M08aN68NdBtpRyAIhb731lv785z9rypQpuuyyy9q8XkFBgS688ELZ7XYtWbJEmZmZQagSkcaa0GTLmMrEgCEdQgAAAAAAAAAAAADg8D344CN68MFHwl0G2pEpFDfZsGGDnnrqqaCt5/P59MADD8hutwdtTUQma3xgIMRXlRAwdhEIAQAAAAAAAAAAAACgmXYPhKxatUoTJ05UdXXwtvZ44403tGLFiqCth8gV3WzLmMADbhEIAQAAAAAAAAAAAACgqXbbMsbhcOjFF1/UrFmz5PF4grZubm6u/va3v8lms6m2tjZo6yIyNe0Q4rXHBYzZMgYAAAAAAAAAAAAAgObapUNIbm6uzj77bM2cOVOSdPvtt6t3795tXtfr9eq+++5TbW2t7rjjjjavh8jXNBDiqbIFjN1G8DrPAAAAAAAAAAAAAADQWbRLIKSoqEiFhYU67rjj9M4772jSpElBWXf27Nn67rvvNG7cOI0dOzYoayKyWZtsGeOxBwZCXHQIAQAAAAAAAAAAAACgmXbZMiYjI0MvvviiRo8eHbQ1t27dqunTpystLU0PPfSQqqqqgrY2Ipc1rsmWMVUxAWO2jAEAAAAAAAAAAAAAoLl2CYRkZWUpKysraOu53W7de++9cjqdeuyxx5SUlEQgpItoumWMu8oaMHaxZQwAAAAAAAAAAAAAAM20SyAk2GbNmqXvv/9e48eP1xlnnBGy+6alJYTsXgi079nHBOZBZHLGBozNMU6lxfDvBACdCd+/ANB18R0AAF0Tv/8BoOviOwAIv5qavfJ4nDKZJIvFFO5y0IV0pZ83k0kymUyy2aJC/t0X8U/5+++/1/PPP6/09HQ98MAD4S4HIRaTGDh2VQVmmFyiQwgAAAAAAAAAAAAAAE1FdIcQp9Ope++9V263W4899pgSExMPflEQlZSwLU2o7UtE7Xv2Pp9kssTL6zYkSdV7fTJ8JvkMb/3YVaGScv6dAKAzaPodAADoOvgOAICuid//ANB18R0ARI7aWpe8Xq+8Xsnt9oa7HHQB+zqDdKWfN69X8nq9qq11HfJ3X7A6iUR0h5Bnn31WW7du1SWXXKLRo0eHuxyEgWFI1viGsbPaUJSvIRjkNPh/FgEAAAAAAAAAAAAAaCqiAyGLFi2SJL333nsaMmRIwP/Gjh3rnzd27FgNGTJEM2bMCFepaEfWeJ//tdNuKMrXkBBxGfZwlAQAAAAAAAAAAAAAAerq6lRYWNDu9yksLNCoUSM1atRI5efvbPf77c/jjz+iUaNG6i9/efiQ5hcWFuikk4brpJOGH1bd3333rf/9ut3u1pbbJUX0ljHDhg1Tenp6i+ecTqc2bdrkn2e1WtWzZ89QlocQCQiEVBuy+hJU/fPYZVSGpygAAAAAAAAAAAAA+Nknn3ysWbOm6/e/v1EXXvibcJcDSIrwQMj06dP3ey4/P9/fJeTZZ59VZmZmqMpCiEXFNrx2VSugQ4jTsMsnnwwZYagMAAAAAAAAAAAAAKQXX3xOJSW7Q3KvtLQeeuONdyVJGRk0TcD+RUwgJC8vTy6XSwkJCerRo0e4y0EEscY1dAjxOA1ZHClSVP3YZ7jlUZ0ssoWpOgAAAAAAAAAAAAAIHYvFoqysfuEuAx2AKdwF7HPdddfpvPPO0zPPPBPuUhBhohoFQiTJbO8eMHYZ9lCWAwAAAAAAAAAAAABAxIuYDiHA/lhjA8emqu5SRsPYaVTK5ksLbVEAAAAAAAAAAAAAurzZs1/QK6+85B9PmzZF06ZN0fXX/0E9e/bS1KmPauzYX+mSS67QM888pdzcHUpMTNJVV/1WV1xxtSSptLRU7733tr79dpXy8/NVU1Ot2Ng4ZWX10+jRZ+riiy9VdHSM/x6FhQW67LJxkqS33pqvzMw+AbVcffW1mjDht3r11Ze1fPlXKi0tUXx8goYPH6Hf/e4GDRw4qF2ficfj0WOPTdZnny1WfHy8/va3GRo27OiDXvfdd9/q7bff0E8//SC73a4BAwbq8suvUmpq94Nei5aFLBDy+eeft+l8U5mZmdq8eXNbSkIHYW3WISQlYOwy2SVvKCsCAAAAAAAAAAAAACk9PUNHH32sNm/+UU6nU5mZfZSS0k3p6Q3/hXtubq7uvPNWmc0m9e8/QDk5Oerff6AkadOmjbrrrltlt1fJao1W7969ZbFkqLCwQJs2bdCmTRv09ddfavr052U2mw+ppqKiQl1//VUqLS1RRkZPZWX11/bt27RkyadavnyZnnvuZQ0ZMrRdnofX69XUqY/qs88WKyEhUX//+0wNHfqLg173r3+9qhdeeE4+n08pKd3Uv/9A7dyZp0cffUjHHz+iXWrtCugQgogXFRc4NuzdAsZOoyqE1QAAAAAAAAAAAABdxzaz9FC8SVvNRrhLCYrBHp+m2L0a5AnOehdccJEuuOAiXXrphSoqKtTVV1+rCy/8jSTpo48WSpK2bduio446Wk8/PUPx8fGqqChXYmLSz500HpbdXqXTThuj+++frMTEREmS2+3WW2/9S88/P1Pr16/VqlUrdMopow6ppiVLPlHfvll66aU5/jBGXl6Obr/9Fu3eXaxXX31JTzzxdHAeQCM+n09PPvmYFi/+SMnJyfr73/+hwYOPOOh1Gzas0/PPz5RhGLrlltt1xRVXyWQyyeFwaNas6Xr33beDXmtXQSAEEa9phxDZkwOGLgIhAAAAAAAAAAAAQLt4IN6kpVZTuMsImp1mQw/ES+9UhHYLghtvvFnx8fGSpKSkZEnStm1bVVFRIavVqvvue8gfBpEki8Wia665TgsWzFdBwS5lZ2875ECIJP35z48HdAHp27efLr98gmbO/D9t3Lg+OG+qEZ/Pp6eemqqPPlqobt1S9X//95wGDDi0rWnmzPmnJOnccy/QhAnX+I9HR0fr9tvv1o4d27Vmzeqg19wVdJ5PLjqtpoEQoyopYEwgBAAAAAAAAAAAAECkMplMGjbs6GbHhwwZqo8//kKLFn3uD4k05nQ6lZhY/7dRh6PukO/XvXtai1vCZGX1lyRVVQX/76tPPz1NCxfOV1JSkmbMeOGQwyB1dXVau/ZbSfWBkJZcdNElQauzq6FDCCJe0y1jZE8IGDoNe+iKAQAAAAAAAAAAALqQqXavHo6Ttlg6x5YxR7h9eqw6tN1B4uMTFB0ds9/z0dEx2rkzTz/99IN27cpXQcEu7dixXdnZ2+R0OiRJXu+h15yWlraf+0RLkjyeIO2X87Mvv/xcDkd9ndXV1f6aD0VRUaGcTqckacCAgS3OOZRtZ9AyAiGIeM22jKkODITQIQQAAAAAAAAAAABoH4M80tzK0AYoOpt9QYyWfP/9Js2aNV3r1n0XcDw5OVknn3yKtmzZosLCXYd1P4slqlV1tpbD4VCPHulKT8/Qxo3r9fjjj+rll1+TxXLwOEJVVaX/tc0W2+KchITEFo/j4AiEIOI17RDitQceIBACAAAAAAAAAAAAoKPJydmhW2/9HzkcDvXrN0Dnnz9OgwYNVr9+/ZWW1kOSNGnS7w87EBJq6ekZmjHjBZlMJv32t5dr27YtmjNntm644X8Oem1SUpL/dXV1taxWa7M5h7NdDgKZwl0AcDBNO4T47IHJMCeBEAAAAAAAAAAAAAAdzLx5c+VwOJSV1U8vv/yaJky4RieccKI/DCJJu3fvDmOFh+bYY49Xr169lZHRU3/4w82SpNdff0Vbt24+6LXp6Rn+Dir7m79jx/bgFdvFEAhBxIuKDQyEeO22gLHLsIeyHAAAAAAAAAAAAAAIYBj1f3r3+XwHmdmgsLBAkpSV1V8xMTHNzq9evVLFxUWSJI/HE4Qq29+ll16ho446Wm63W48//qjcbvcB50dHx+ikk06RJM2f/26LcxYunB/0OrsKAiGIeNYmW8Z47IF7bDlNdAgBAAAAAAAAAAAAED6xsfX/UXtRUeEhX9O3bz9J9cGP9evX+Y+73W59+unHmjz5Af+xjrJtislk0r33PqSoqCj/1jEHc8MN/6OoqCgtW7ZU//jHdLlcLkn1z+Hll5/Xl19+0b5Fd2KWcBcAHEzTLWM81YGBEBdbxgAAAAAAAAAAAAAIo8GDhyg7e5veeGOOVq5crtNPP0M9eqQf8Jorr7xGn332scrLy3XLLROVmdlXcXFxKijYpaqqStlssRo27Bht2rShQ2wds8+AAQN1zTXX6ZVXXtLrr7+i008fo8GDhxxg/iDdf/9kPfHEX/Tmm69p4cL3lZmZqYKCXaqoqNDpp5+hr74iFNIadAhBxItq0iHEXWOR4Wv40SUQAgAAAAAAAAAAACCcbrnldo0Zc6ZsNptyc3OUm5tz0GsyMjL06qtv6Te/uVR9+vTV7t3FysvLUWpqqi699ArNmTNXN954syTpu+++VW1tbTu/i+D53e9+r/79Bxzy1jG//vW5evHFV3XWWWcrJiZG2dnb1K1bqu64427ddtudIaq68zF8h7OJURdTUkLQINTS0hIkBT57V430dL8E/3jAmW7VfJomp6lckpTi/oUuK1sZ0joBAMHX0ncAAKBr4DsAALomfv8DQNfFdwAQOfbsKZbLVaeoqBilph64mwUQDBZL/X/473Z7w1xJ6LTmc7bvu7Kt6BCCiGexSTIackvOainKF+8f0yEEAAAAAAAAAAAAAIBABEIQ8QxDsjbaNsZZbcjqa0hEEQgBAAAAAAAAAAAAACCQJdwFAIfCGueT025IklzVRkCHEKdRJZ98MmSEqzwAAAAAAAAAAAAA6FBee+2fWrFieauunTVrdpCrQXsgEIIOISqgQ4gCOoT4DI88qpNFtjBUBgAAAAAAAAAAAAAdz86dedq4cX24y0A7IhCCDsEa5/O/ru8QkhBw3mlUyeIjEAIAAAAAAAAAAAAAh+LBBx/Rgw8+Eu4y0I5M4S4AOBRRjQIhzhrJ4k4MOO8yqkJdEgAAAAAAAAAAAAAAEYtACDoEa2yjgc+QpSYl4LzLRCAEAAAAAAAAAAAAAIB9CISgQ2jcIUSSjKrUgLHTsIeyHAAAAAAAAAAAAAAAIhqBEHQI1rjAscneLWDMljEAAAAAAAAAAAAAADQgEIIOoVmHEHtywJhACAAAAAAAAAAAAAAADQiEoEOwNg2EVCcGjJ0EQgAAAAAAAAAAAAAA8CMQgg6h6ZYxsicFDF2GPXTFAAAAAAAAAAAAAAAQ4QiEoENoumWMzx4fMHYalaEsBwAAAAAAAAAAAACAiEYgBB1C8w4hgYEQOoQAAAAAAAAAAAAAANCAQAg6BGuTDiFee2zA2GlUhbIcAAAAAAAAAAAAAAAiGoEQdAhNt4xpGghxmQiEAAAAAAAAAAAAAACwD4EQdAhNt4zx2GMCxk6jMoTVAAAAAAAAAAAAAECguro6FRYWtPt9CgsLNGrUSI0aNVL5+Tvb/X7ouAiEoEOIig3sEOKpjpLJZ/WPnUZFqEsCAAAAAAAAAAAAAEnSJ598rAkTLta33/433KUAfpZwFwAciqYdQpw1ktWXqDqjtH5ssGUMAAAAAAAAAAAAgPB48cXnVFKyOyT3SkvroTfeeFeSlJHRMyT3RMdEIAQdQlRcYIcQV7Uhqy9BddoXCGHLGAAAAAAAAAAAAACdn8ViUVZWv3CXgQ6ALWPQIVibBEKc1Yas3qSGsYlACAAAAAAAAAAAAAAA+9AhBB1CVGzg2FUjJfgS/WOPUSuvXDIpKsSVAQAAAAAAAAAAAOiqZs9+Qa+88pJ/PG3aFE2bNkXXX/8H9ezZS1OnPqqxY3+lSy65Qs8885Ryc3coMTFJV131W11xxdWSpNLSUr333tv69ttVys/PV01NtWJj45SV1U+jR5+piy++VNHRMf57FBYW6LLLxkmS3nprvjIz+wTUcvXV12rChN/q1Vdf1vLlX6m0tETx8QkaPnyEfve7GzRw4KCgvPfHH39EixZ9oLvuuk+GYdKcObNVXl6mHj3Sdc89D2r48JGSpOpqu+bNe0tffvm5du3Kl9frVa9evTV69Jm6/PKrlJCQ0OL6e/aUav78d7Vs2VIVFBTI5/Oqb98snXXW2br00itltVqbXbN06RItXPgfbd78g+x2u5KSkjRs2DG6+OLLNWLECc3mjxpVX+OSJcu1cuU3evfdt7R162a5XC717Zulc845X5dccoUslo4ZreiYVaPLMUdJ5mifPA5DkuS0G7I2CoRI9dvGxPhSw1EeAAAAAAAAAAAAgC4oPT1DRx99rDZv/lFOp1OZmX2UktJN6ekZ/jm5ubm6885bZTab1L//AOXk5Kh//4GSpE2bNuquu26V3V4lqzVavXv3lsWSocLCAm3atEGbNm3Q119/qenTn5fZbD6kmoqKCnX99VeptLREGRk9lZXVX9u3b9OSJZ9q+fJleu65lzVkyNCgPYPFixdp48b16tEjXZmZfVRYWKDBg4f8/N5zdNddt6qwsEBms1m9evVWTEyMtm/P1iuvvKRFiz7Q00/PaLYFzoYN6/Tgg/eorGyvzGaz+vUbIJfLqa1bt2jLls1asWK5nnlmpqKi6hsGuN1uTZ58v7766gtJUmpqdw0efIQKCwv05Zdf6Msvv9Dll0/Qrbfe2eJ7ePnl5zV37uuy2WzKzOyj0tJSbd26RVu3btEPP2zSo48+EbTnFUoEQtBhWON9qvUHQtRCIKSCQAgAAAAAAAAAAACAkLnggot0wQUX6dJLL1RRUaGuvvpaXXjhbyRJH320UJK0bdsWHXXU0Xr66RmKj49XRUW5EhOT5PF49NhjD8tur9Jpp43R/fdPVmJi/d9A3W633nrrX3r++Zlav36tVq1aoVNOGXVINS1Z8on69s3SSy/N0dChv5Ak5eXl6Pbbb9Hu3cV69dWX9MQTTwftGWzcuF6XXHK5br31TpnNZpWVlSkhIUG1tbW69947VFhYoNNOG60777xPGRnpkqTi4t2aNm2Kvvnma9177/9qzpw3/V1QKisr9fDD96msbK9OOukU3X//ZKWmdpck/fTTD7rrrtu0du0a/fOfL+p//ucWSdKMGc/oq6++kM0WqwcemKwzzjhLkuTxePT+++9p+vSn9c47c5WenuHvzNLY3Lmv67e/vV7XXTdR0dHR8ng8eumlWfrXv17VkiWf6pprrvOHXDoSAiHoMKLjpdo99a8dVYas3iaBEFOl5A1DYQAAAAAAAAAAAEAntWeboSUPxah0qyncpQRF98FejZ1Sp9RBvpDe98Ybb1Z8fLwkKSkpWZK0bdtWVVRUyGq16r77HvKHQSTJYrHommuu04IF81VQsEvZ2dsOORAiSX/+8+MBXUD69u2nyy+foJkz/08bN64Pzpv6mdUarZtu+pO/g0lKSookaeHC95Wfv1NHHDFUU6Y8FdDhJDW1ux577EldffVlys/P00cffaDx4y+VJC1Y8G/t2VOqnj176fHHnwrYLmfo0F/o1lvv1F/+8pA+/vhD/eEPk1RaWqL3339PknTPPQ/4wyCSZDabdckll6umpkYvvDBTr7zyki68cLxiY2MD3sOpp57mD5fsu+4Pf5ik//zn36qqqtSGDesJhADtyZrQ8Et5f1vGAAAAAAAAAAAAAAiezx6I0Y6lnefPypU7TfI9EKMr3qkN2T1NJpOGDTu62fEhQ4bq44+/kMNRFxB62MfpdCoxMUkFBbvkcNQd8v26d09rcUuYrKz+kqSqqqrDqP7gjjhiiGw2W7Pj+7ZvOeusX7e43U10dIzGjBmrt976l5Yv/8ofCFm+fJkk6eyzz2vxuZxxxlj1799fffpkyWQyaeXKb+TxeJSamqqxY3/dYo2XXnqF/vnPF2S327V27RqdeuppAedPPfX0ZteYzWZlZvbRjz9+L7s9uM8sVDrPJxedXnSTQEiUJyngPIEQAAAAAAAAAAAAAJEmPj6hxWDDPtHRMdq5M08//fSDdu3KV0HBLu3YsV3Z2dvkdDokSV7voW+VkJaWtp/7REuq30YlmFJTU1s8vmNHtiRpwYL3tWzZl5IkwzAkST5f/d9+9+6t3yIiNzfXf92uXfmSpEGDBre4blRUVEC3jtzcHEnS4MFDZDK13MnGZrOpb98sZWdvU15ebrNASKifWagQCEGHYY0PHJuqUqVGTUKcRkVoCwIAAAAAAAAAAAA6ubOm1mnJwzEq3dJJtow5wquxjx16t41g2BcqaMn332/SrFnTtW7ddwHHk5OTdfLJp2jLli0qLNx1WPezWKJaVWdr7S/sYrfbJUn5+XnKz8874BqNO3BUVJRLkmy22P3MDlRTUy1JiouLP+C82Ni4gPmNHeyZ7QuwdDQEQtBhNO4QIklGRarUu2HsNNEhBAAAAAAAAAAAAAim1EE+XT43dNurdCU5OTt0663/I4fDoX79Buj888dp0KDB6tevv9LSekiSJk36/WEHQiKFzWaT3W7XtGl/93fksFjqg0Vu9/47nuy7rqXgRktiY+uDI9XV9gPO27dVzr5gSFfQOWJc6BKs8U1SV5UpAUO2jAEAAAAAAAAAAADQUcybN1cOh0NZWf308suvacKEa3TCCSf6wyCStHv37jBW2DZ9+mRJatg6piX7tsopKytrdF1fSdL27S1f53a7NWnSDXrooXu0a1e++vbtJ0naunXzfrfWqa62a+fO3J/X73PY76WjIhCCDiM6ocmByqSAodOoEgAAAAAAAAAAAACEmmHU/+n9cLYWKSwskCRlZfVXTEzzbVdWr16p4uIiSZLH4wlClaG1ryvIBx/8Rw5H82163G637r//Tk2c+Ds999z/+Y+fdNKpkqRPP/1YLper2XUrVy7Xxo3rtWrVCnXrlqqTTjpFZrNZe/bs0ZIln7RYy3vvvSOPx6OYmBgdd9yIILy7joFACDoMa5MtY3xViQFjOoQAAAAAAAAAAAAACIfYWJskqaio8JCv2dfZYvXqlVq/fp3/uNvt1qeffqzJkx/wH2spUBHpLr74cqWmdld+/k7de+//qqioyH+urKxMkyffr5ycHYqKitKECb9tdN1lSkpKUn7+Tj366IOqrKzwn/vhh03661+fkCSNH3+pbDab0tMzNG7cxZKkp56aqi+++Mw/3+v1av78dzV79guSpGuvnaj4+Ph2fd+RxBLuAoBDFd0kEOKtDGwZQiAEAAAAAAAAAAAAQDgMHjxE2dnb9MYbc7Ry5XKdfvoZ6tEj/YDXXHnlNfrss49VXl6uW26ZqMzMvoqLi1NBwS5VVVXKZovVsGHHaNOmDR1y65jExERNm/aM7r33Dn377X91+eXj1L//ABmGoby8XDmdTpnNZj3yyOMaOHCQ/7qUlG56/PG/6r777tTSpZ/rm2++Vr9+A2S3V6mwsEA+n0+//OXJuuGGm/zX/OlPd6i0dLeWLftSDz98n7p3T1NaWg8VFu5SeXm5JOmSSy7XNddcG+rHEFYEQtBhNO0Q4q2MDRg7TRUCAAAAAAAAAAAAgFC75ZbbVVdXq2+//a9yc3OUm5tz0EBIRkaGXn31Lb366stas+a/Ki4ultlsUnp6hs4++1xdfvlVKioq1K233qTvvvtWtbW1stlsIXpHwTF06C/02mtv691339bXX3+pXbvy5XQ6lZraXccfP0JXXnmNBg0a3Oy6444brtdff1tz576uFSuWKydnh8xmk4488iidf/44XXjhb2QyNWyIYrVaNXXq3/T555/pww//o82bf9S2bVvUrVuqxo79tS666GINHz4ylG89Ihi+w9nEqIspKakKdwldTlpafdePlp795g8smv/7hl9wY6fUad0D8fIZ9ftl9XCdoN+ULwlNoQCAoDvQdwAAoHPjOwAAuiZ+/wNA18V3ABA59uwplstVp6ioGKWmHji8AASDxVIf4nC7vWGuJHRa8znb913ZVqaDTwEiQ9MtY5xVhqy+xIYxW8YAAAAAAAAAAAAAACCJQAg6kKZbxjiqDFl9Sf4xgRAAAAAAAAAAAAAAAOpZwl0AcKiim3TFcVZJVm+iZP55bCIQAgAAAAAAAAAAAACH4rXX/qkVK5a36tpZs2YHuRq0BwIh6DCs8U06hNgDt4xxG9Xyyi0TP9YAAAAAAAAAAAAAcEA7d+Zp48b14S4D7Yi/nKPDaLpljLPKUGKjQIhUv21MjK9bKMsCAAAAAAAAAAAAgA7nwQcf0YMPPhLuMtCOTOEuADhU1lhJRkMoxFGlgA4hUn0gBAAAAAAAAAAAAACAro5ACDoMwyRFxzeMHVUGgRAAAAAAAAAAAAAAAFpAIAQdSuNtY5x2Q1ZvYCDEZaoKdUkAAAAAAAAAAAAAAEQcAiHoUKIDAiGS1ZcUcJ4OIQAAAAAAAAAAAAAAEAhBB2NtsmVMlLfpljEVIa4IAAAAAAAAAAAAAIDIQyAEHUrjDiFelyFLXUrAeTqEAAAAAAAAAAAAAABAIAQdjLVRIESSjIpuAWOniUAIAAAAAAAAAAAAAAAEQtChRDcJhKgyOWBIhxAAAAAAAAAAAAAAAAiEoIOxJgSOfU0CIQ6jInTFAAAAAAAAAAAAAAAQoQiEoEOxxjXtEJIUMHQSCAEAAAAAAAAAAAAAgEAIOpamW8Z4K+MDxg5TWSjLAQAAAAAAAAAAAAAgIhEIQYcS3WTLGHdVtMy+aP/YaZSHtiAAAAAAAAAAAAAAACIQgRB0KNYmHUKcVYas3mT/2GEqD21BAAAAAAAAAAAAABACo0aN1KhRI7V69Sr/sY8+WqhRo0Zq/PjzDmut2bNf0KhRIzVp0g1BrTEnZ4d8Pt/BJx6G7duzg7ZWS8+wMyMQgg6l6ZYxTruhaF+yf+ygQwgAAAAAAAAAAAAAhFR1tV3PPDNN1157pTweT1DWLC0t1SOPPKi77ro1KOt1RZZwFwAcDmt84NhhV0AgxGlUyCevDLJOAAAAAAAAAAAAADq5008/Q0cddbQslvD+6X/z5p/073/PC+qa//3vCn322WKlpfUI6rpdCYEQdChNO4Q4qgxFN9oyxmd45TKqZPUlhbgyAAAAAAAAAAAAAAit+Ph4xcfHH3wiuiTaKKBDsTbdMqbKkLVRhxCJbWMAAAAAAAAAAAAAAKBDCDqU5h1CpJRGHUIkyWEqV4I3K4RVAQAAAAAAAAAAAOiKZs9+Qa+88pIGDhysOXPmtjhnw4Z1uvnmiYqJidGCBYsVGxunqqoqvf/+e1q5crlycrbLbrfLZrOpV69MnXrqabrssglKTEw86P0/+mihpk59VGlpPTR//kcB57xerxYt+kALF76vnJztkqSjjz5WN9zwP21/441ceumFKioq9I/HjDlJkjRv3gL17NnLf/zbb/+r999/Vxs3blBFRbni4+M1ZMgvNG7cbzR69JkBa44aNdL/uqRkt3/89dff+o8H6xl2ZgRC0KFYm3Q7clQaiqZDCAAAAAAAAAAAAIAwOO+8C/Xqqy8rO3ursrO3aeDAQc3mLF5cH9QYPfpMxcbGaefOPN122yTt3l0ss9mszMw+Sk/vqeLiQm3Z8pO2bPlJS5Z8opdffl2xsbGtqsvlcmny5Pu1bNlSSVLv3pmKi4vT6tWrtHr1Kh155FGtfMfNDR36C8XGxmr79mxJ9aETSbJarf45f//7U3rvvXckSUlJyRo8+AiVlJRo1apvtGrVNzrzzF9p8uTHZLFY/GuUlZUpPz9PUVFRGjr0FwH3DMUz7AwIhKBDMUdJ1jifnNWGJKmu3JDVlxQwx2lUhKM0AAAAAAAAAAAAoNMpN2/Vivh7VWbeEu5SgiLFc4ROtk9TsmdwUNbr2bOXhg8fqTVrVuuTTxZp0qQ/BZx3uVz6/PPPJNWHRyRp2rQp2r27WEcddbQef/yv6t69uyTJ5/Np8eKPNHXqo8rLy9WiRR/okksub1Vdb775mpYtW6r4+Hg99tg0nXDCiZKk0tISPfLIg1q37rvWveEWTJkyTd99961uvfUmSdKMGS/4gx2SNHfuv/Tee+/IbDbrjjvu1sUXXyqvt/7c559/piee+Is+//xTdeuWqttvv0uSNGvWbH/3k+TkFM2aNTvgnqF4hp0BgRB0ODEpjQIhFYaivSkB5x2m8jBUBQAAAAAAAAAAAHQ+38TfrXzr5+EuI2js5jz54u/WeRXvB23N8867UGvWrNann36sm276owzD8J/75ptlqqqqVEZGTw0fPlJlZXv927fce++D/iCDJBmGoXPOOV+LFn2gNWtWa/v2ba2qx+12a+7c1yVJt912lz8MIkndu6dp6tS/acKE8aqoaP//0N7hcGjOnPowx8SJN+nSS+vDGd6fEyFnnnmWvF6PHnnkQc2fP09XXHFVwDYzLQnFM+wsTOEuADhcMck+/+u6MraMAQAAAAAAAAAAABA+Y8acqbi4OO3eXdys88bHH38oSTr33AtkGIZSUrrpgw8+05IlX2vAgObby3g8HsXGxkmS6urqWlXP+vVrZbfbZbVGa+zYXzc7n5iYqDPPbH68PWzYsFZ2e5XMZrMuvviyFueMHftrpaX1kMfj0TffLDvomqF4hp0FHULQ4TQOhDirDUU5m3QIIRACAAAAAAAAAAAABMUp9r9qRdx9KrNsDncpQZHiHqKTq58M6prR0TEaO/bXWrBgvj75ZJGOP36EJKmiolwrViyXYRg699wLml1TXFykH37YpPz8fBUU7FJu7g5t3bpFtbU1kuq3P2mNvLxcSVJmZqasVmuLcwYPPqJVax+u3NwcSVKfPn0VFxff4hzDMHTEEUNUUrLbX/uhaM9n2FkQCEGHY0tu8qHdmyqlNQydprLQFgQAAAAAAAAAAAB0UsmewTq38r1wlxHxzjtvnBYsmK8vvliiO+64R1arVUuWfCq3263jjx+hXr16++fm5eXoueee1YoVy/1bp0hSXFycjj32OJWWlmrbti2trqWqqlKSZLPF7ndOQkJCq9c/HNXV1ZK03zDIPvvO19TUHNK67f0MOwsCIehwYlICAyHesm4BYzqEAAAAAAAAAAAAAAilYcOOVr9+/ZWTs0MrVizX6NFnaPHijyRJ5513oX9eWdle3XLLjSor26v09AyNGzdeRxwxVFlZ/dSzZy8ZhqFHH32oTWGGxMQkSQ1hjJY4HI5Wr3849m3dUl1tP+C8fSGW2Nj9h1j2CcUz7CwIhKDDiUkKDIR49iQFjB2m8hBWAwAAAAAAAAAAAADSuedeoFmzZmjp0iUaNGiwvv9+o2y2WI0ZM9Y/54MPFqisbK8SE5M0e/a/lJyc3GydkpLdbaqjb98sSVJ+fp5qa2tls9mazdmxY3ub7nGosrL6SZJ27sxTdbVdSUmJzeZ4vV5t2VK/JVFmZt+DrhmKZ9hZmMJdAHC4bMmBY3eFTSZflH9MhxAAAAAAAAAAAAAAoXbOOefLbDZrxYqv9emnH0uSzjzzrIBARmHhLklSRkZGi0GGHTu2a9OmDZIkj8fdqjqOPfZ4paR0k9vt1sKF7zc7X1dX568vWEymlqMHxxxznBISEuXxePTvf89rcc5nn32iPXtKZRiGTjzxZP9xwzAkST5fYMOAUDzDzoJACDqcplvG1JUZsvoauoQ4CYQAAAAAAAAAAAAACLHU1O468cRTZLfb9eabr0sK3C5GauiYsW3bVi1dusR/3OfzaeXKb3TnnX+S210fYqirq2tVHWazWRMn3iRJev75mfrss8X+cxUV5Zo8+T7t3l3cqrX3x2Zr2OqlqKjQ/zomJka//e11kqSXX35e7777jrxer//80qVL9Ne/TpUkjRs33t/dRGrYPqaqqlI1NQ3b34TiGXYWbBmDDicmOTAQUltuKNqbrDpTqSS2jAEAAAAAAAAAAAAQHueff6G++WaZamqqlZnZR8cee3zA+QsuuEjz57+r/Pydeuihe5WR0VPJySkqLi5SWdleWSwWHX/8CK1du6ZN255cdNHF2r59m9577x098siDmjVrhlJSumn79my5XE6ddtoYLVu2tE3vtbE+ffrKZrOptrZWN954nXr16q377ntYgwYN1oQJv1VBQYHef/9d/e1vT+rll59Xz569VVKyW6WlJZKkMWPG6tZb7wxYc+DAwTKZTHI4HJow4RJ1756mZ56ZEbJn2BnQIQQdTtNASF25oWhfsn/sMCrkk08AAAAAAAAAAAAAEEqnnnq6fxuTc845v9n5uLh4vfTSa7rmmuvUv/8AlZeXafv2bMXFxen888dp9ux/6f77J0uq74BRVFTU6lruuOMeTZ36N40YcYJqa2uVm7tDRx75Cz3zzAyNGXNmq9dtSWxsrB57bJoGDTpCdXW1KijY5d/axTAM3XXXfXrmmZkaPfoMmUxmbd26WZJ06qmn6Ykn/qYpU6YpOjo6YM3MzD564IE/q0+fvqqsrFBxcZEKCwtD+gw7OsPXdMMd+JWUVIW7hC4nLS1B0oGfffFGk14ZG+cfj5jolPu587XT+pn/2PUlBYpSfPsVCgAIukP5DgAAdE58BwBA18TvfwDouvgOACLHnj3FcrnqFBUVo9TU9HCXgy7AYqnvWeF2ew8ys/Nozeds33dlW9EhBB1Osw4hZYas3uSAY2wbAwAAAAAAAAAAAADoygiEoMOJSWkSCKkI3DJGkhxGeegKAgAAAAAAAAAAAAAgwljCXQBwuKxxksnik9dtSJJqywxFt9QhxBP62gAAAAAAAAAAAACgo/n735/Sli2bD/u61NTumjJlWjtUhGAgEIIOxzDqt42pKa0PhNSVG4r2pQTMcdIhBAAAAAAAAAAAAAAOSXb2Nm3cuP6wr8vI6NkO1SBYCISgQ6oPhNS/rquQrGwZAwAAAAAAAAAAAACtMnPmi+EuAe3AFO4CgNawJTe8ri0zZPUkBZx3mMpDWg8AAAAAAAAAAAAAAJGEQAg6pJhkn/+1z2PIVJkWcN5hlIW6JAAAAAAAAAAAAAAAIgaBEHRIjQMhkqSy1IChw7Q3hNUAAAAAAAAAAAAAABBZCISgQ4pJaRII2RsYCKmjQwgAAAAAAAAAAAAAoAsLWSDkrbfe0pAhQzRv3rxWXb9hwwbdeeedGjNmjIYNG6YRI0boiiuu0GuvvSan0xnkahHpbE06hHj2JgWMHSYCIQAAAAAAAAAAAACArssSipts2LBBTz31VKuvnzNnjp588kl5vV7FxMRowIABKisr07p167Ru3Tp98MEH+uc//6n4+PggVo1I1nTLGFdFjMw+mzxGrSSpzmDLGAAAAAAAAAAAAABA19XuHUJWrVqliRMnqrq6ulXXr1mzRk888YS8Xq8mTpyo1atXa8GCBVq2bJnmzJmjHj16aP369Zo8eXKQK0ckaxoIqSszFOPt5h87TARCAAAAAAAAAAAAAABdV7sFQhwOh2bMmKHrr79eFRUVrV5n9uzZ8vl8OuOMM3T33XfLarX6z5100kmaNm2aJOnDDz9UYWFhm+tGx2BLCQyE1JYbivE1CoQYbBkDAAAAAAAAAAAAAOi62iUQkpubq7PPPlszZ86UJN1+++3q3bt3q9ZatWqVJOmCCy5o8fzJJ5+suLg4SdKmTZtadQ90PDFJTTqElEvRjTqEuEx2eeQMcVUAAAAAAAAAAAAAAEQGS3ssWlRUpMLCQh133HF6+OGHNWzYMM2bN++w1/F6vfr73/+uoqIijRw5ssU5Pl9DMMDj8bS6ZnQsMSlNAyGGon0pAcccpr2K9WaEsiwAAAAAAAAAAAAAACJCuwRCMjIy9OKLL2r06NFtWsdkMun0008/4Jxly5apurpakjR48OA23Q8dhy05cFy711Baow4hklRn7FWsCIQAAAAAAAAAAAAAALqedgmEZGVlKSsrqz2WDlBdXa0nnnhCkjRs2DANHDiw3e+JyBCTHNghpLbMCNgyRpIcpjKJpjEAAAAAAAAAAAAAgC6oXQIhoeB0OnX77bdrx44dMpvNeuCBB4J+j7S0hKCviUNzKM/eliLVltW/dlZY1D2uZ8B5a3Kt0sS/IQB0NHz/AkDXxXcAAHRN/P4HgK6L7wAg/Gpq9srjccpkkiwWU7jLwUGcdNJwSdL06bP0y1+eKEn64IMFmjLlEaWl9dDChR8f8lovvfS8Zs9+Ucccc5xefPGfba5t0qQ/aO3aNbruuht00023HHT++vXf6ZZbbpQkff31f2WxHFp0obXvN5xMpvrdUWy2qJB/93XIT3VdXZ3++Mc/6quvvpIk3X333RoxYkSYq0KoxaU2vK7ZI8UosENIrfaEuCIAAAAAAAAAAAAAACJDh+sQsmfPHt18881at26dJOmWW27R9ddf3y73Kimpapd1sX/7ElGH8uytSbGSzJKk6j0+OctjpeSG86X2ApXU8m8IAB3F4XwHAAA6F74DAKBr4vc/AHRdfAcAkaO21iWv1yuvV3K7veEuB4fI4/H6/71GjRqjN954VxaL5bD+Db1enyTJ5/MF5d/e5/P51z3Qevs60Xg8DXPq5x9aDfvqbrgu8nm9ktfrVW2t65C/+4LVSaRDBUKys7P1hz/8Qbt27ZJhGLrvvvt03XXXhbsshImtW8OH3es2ZKpICwiEOExloS8KAAAAAAAAAAAAAEIkPj5e8fHx4S4DEarDBEJWrVqlP/7xj6qsrFR0dLSeeuopnXPOOeEuC2HUOBAiSb7SNCmrYVxn7A1xRQAAAAAAAAAAAAAARIYOEQj573//qxtvvFF1dXVKTk7W888/r+OPPz7cZSHMbClNAiF7ugWMHSYCIQAAAAAAAAAAAADaz+zZL+iVV17SwIGDNWfO3BbnbNiwTjffPFExMTFasGCxYmPjVFVVpffff08rVy5XTs522e122Ww29eqVqVNPPU2XXTZBiYmJB73/Rx8t1NSpjyotrYfmz/8o4JzX69WiRR9o4cL3lZOzXZJ09NHH6oYb/qftb/wQ1dXV6e67b9PatWvUo0e6Zs58QX379j3odV9++YX+/e93tG3bFjmdLg0ZMlS/+93vQ1Bx5xLxgZCdO3fq5ptvVl1dnTIyMvTKK69owIAB4S4LESA2NTAQ4toTuI8SHUIAAAAAAAAAAAAAtKfzzrtQr776srKztyo7e5sGDhzUbM7ixfVBjdGjz1RsbJx27szTbbdN0u7dxTKbzcrM7KP09J4qLi7Uli0/acuWn7RkySd6+eXXFRsb26q6XC6XJk++X8uWLZUk9e6dqbi4OK1evUqrV6/SkUce1cp3fOgcDofuvfd/tXbtGmVk9NSzz846pDDI009P0/z58yRJPXqkKyOjl3788Xv97//+UccfP6K9y+5UIiYQkpeXJ5fLpYSEBPXo0cN//KGHHlJVVZViYmL0wgsvEAaBX9MOIY69Flm9yXKayuvHprIwVAUAAAAAAAAAAAB0HtvM0kPxJm01G+EuJSgGe3yaYvdqkCc46/Xs2UvDh4/UmjWr9cknizRp0p8CzrtcLn3++WeS6sMjkjRt2hTt3l2so446Wo8//ld1795dkuTz+bR48UeaOvVR5eXlatGiD3TJJZe3qq4333xNy5YtVXx8vB57bJpOOOFESVJpaYkeeeRBrVv3Xeve8CFyOp26//67tGbNf9W7d6aeffZ5ZWRkHPS6Tz5ZpPnz5ykqKkr33/9n/frX50iSqqqq9NRTj+uLLz5r17o7m4gJhFx33XXatWuXxo8fryeffFKStHHjRq1cuVKSFBMTo0cfffSAa9x0000aPXp0u9eKyGDrFhgIqdlrKMaXIqfKJdEhBAAAAAAAAAAAAGirB+JNWmo1hbuMoNlpNvRAvPROhTdoa5533oVas2a1Pv30Y9100x9lGA3hmW++WaaqqkplZPTU8OEjVVa21799y733PugPg0iSYRg655zztWjRB1qzZrW2b9/Wqnrcbrfmzn1dknTbbXf5wyCS1L17mqZO/ZsmTBivioqKVq1/MC6XSw8+eLf++98V6tOnr6ZPf15paT0OfqGkOXNmS5J++9vr/WEQSUpISNDkyY8pO3ur8vJy26XuzihiAiEtWb16tf91eXm5vvvuwCmlPXv2tHdJiCBNAyF1ZYaivd0k8w5JksO0Vz75ZKhzpBUBAAAAAAAAAAAARJ4xY87UM89M0+7dxVq37ruAbU0+/vhDSdK5514gwzCUktJNH3zwmRyOOkVHxzRby+PxKDY2TpJUV1fXqnrWr18ru90uqzVaY8f+utn5xMREnXnmr/3bsgST2+3Www/fqxUrlqt370zNnPmiUlO7H/xCSbt25Ss3N0dSQzeVxqKionTBBRfpH/+YHsySO7WQBUI+//zzwz7/+9//Xr///e/bqyR0cM06hOyp7xCyj9dwyWXYZfUlhLo0AAAAAAAAAAAAoFOYavfq4Thpi6Vz/EfYR7h9eqw6eN1BJCk6OkZjx/5aCxbM1yefLPIHQioqyrVixXIZhqFzz72g2TXFxUX64YdNys/PV0HBLuXm7tDWrVtUW1sjqX4LmdbY10EjMzNTVqu1xTmDBx/RqrUP5r333pbD4ZAkVVRUyOs99Ge9r+7Y2DhlZPRscU571d1ZRXSHEOBAbCmBvwBrywwlebsFHHMYZQRCAAAAAAAAAAAAgFYa5JHmVgY3QNEZnXfeOC1YMF9ffLFEd9xxj6xWq5Ys+VRut1vHHz9CvXr19s/Ny8vRc889qxUrlgcEJuLi4nTssceptLRU27ZtaXUtVVWVkiSbLXa/cxIS2udvqA6HQwMGDJTP59OOHdv11FOP669/ffaQrq2qqpIk2WzNO6fsk5CQGJQ6uwoCIeiwmnYIqd1rKL1pIMS0VwnevqEsCwAAAAAAAAAAAEAXM2zY0erXr79ycnZoxYrlGj36DC1e/JGkwO1Pysr26pZbblRZ2V6lp2do3LjxOuKIocrK6qeePXvJMAw9+uhDbQqEJCYmSZKqq6v3O2dfF49gGzToCD377D+Un5+vSZN+rxUrluujjxa2uAVMU0lJ9XXX1NTsd0571d1ZmcJdANBa5igpOqEhFFK711C0LzAQUmfsDXVZAAAAAAAAAAAAALqgfdvCLF26RLt25ev77zfKZovVmDFj/XM++GCBysr2KjExSbNn/0vXXnuDTj75VPXq1VuGUb8tT0nJ7jbV0bdvliQpPz9PtbW1Lc7ZsWN7m+6xP6ecMkpJSck66qhhuvTSKyRJ06c/c0jvaV/dtbW1/u1jmtqxIzt4xXYBBELQoTXuElK711C0NyXgfJ2JQAgAAAAAAAAAAACA9nfOOefLbDZrxYqv9emnH0uSzjzzLNlsNv+cwsJdkqSMjAwlJyc3W2PHju3atGmDJMnjcbeqjmOPPV4pKd3kdru1cOH7zc7X1dX562tPf/jDzerZs5fs9io99dTjB53fs2cvHXHEUEnS+++/2+y81+vVhx8uCHqdnRmBEHRojQMhNXsNRTfdMoYOIQAAAAAAAAAAAABCIDW1u0488RTZ7Xa9+ebrktRsq5SsrH6SpG3btmrp0iX+4z6fTytXfqM77/yT3O76IEhdXV2r6jCbzZo48SZJ0vPPz9Rnny32n6uoKNfkyfdp9+7iVq19OGw2m+655wFJ8m8dczA33fRHSdK7776td955U16vV1L9s3jqqcf1448/tF/BnZAl3AUAbdE4EOJ1GbJUpktJDefrTHvCUBUAAAAAAAAAAACAruj88y/UN98sU01NtTIz++jYY48POH/BBRdp/vx3lZ+/Uw89dK8yMnoqOTlFxcVFKivbK4vFouOPH6G1a9e0aeuYiy66WNu3b9N7772jRx55ULNmzVBKSjdt354tl8up004bo2XLlrbpvR6KE044Seeee4EWLfpA06c/o5NOOlk9evTY7/xf/vIkTZr0Jz3//ExNn/6M/vWvOUpPT1dubq5qaqp1+uln6Kuvvmj3ujsLOoSgQ7Ol+AIPlKYHDAmEAAAAAAAAAAAAAAiVU0893b8VzDnnnN/sfFxcvF566TVdc8116t9/gMrLy7R9e7bi4uJ0/vnjNHv2v3T//ZMl1XcRKSoqanUtd9xxj6ZO/ZtGjDhBtbW1ys3doSOP/IWeeWaGxow5s9XrHq4//ekOdeuWKru9Sk8+OeWg86+++lpNn/68Tj31NEnS9u3Z6ts3S4888riuuOLq9i63UzF8Pp/v4NO6ppKSqnCX0OWkpSVIOvRn/9nD0fr2Bat/fNmnOVpyVn//eGDdJRpb9UpwiwQAtIvD/Q4AAHQefAcAQNfE738A6Lr4DgAix549xXK56hQVFaPU1PSDXwC0kcVS37PC7faGuZLQac3nbN93ZVvRIQQdWtMOId49yQFjOoQAAAAAAAAAAAAAALoiAiHo0GK7BQZCnHujFeWN948JhAAAAAAAAAAAAAAAuiJLuAsA2sLWJBBSs9dQjC9VLtklSbVGaTjKAgAAAAAAAAAAAIAO4+9/f0pbtmw+7OtSU7trypRp7VARgoFACDq0poGQ2r2GYrypqjLnSqrvEOKTT4aMcJQHAAAAAAAAAAAAABEvO3ubNm5cf9jXZWT0bIdqECwEQtChNQuE7KkPhOzjNZxyGXZZfQmhLg0AAAAAAAAAAAAAOoSZM18MdwloB6ZwFwC0RVz3wEBIdWn9ljGN1bFtDAAAAAAAAAAAAACgiyEQgg7N1s0nGQ2hkJpSQzHe7gFz6kx7Ql0WAAAAAAAAAAAAAABhRSAEHZrJErhtTHWJKWDLGEmqM9EhBAAAAAAAAAAAAADQtRAIQYcXlxbYIcTma9IhxKBDCAAAAAAAAAAAAACgayEQgg4vtntDIMRRaSiqJi3gfC1bxgAAAAAAAAAAAAAAuhgCIejwGncIkSTtzggY1hEIAQAAAAAAAAAAAAB0MQRC0OE17hAiSd6SHgFjtowBAAAAAAAAAAAAAHQ1BELQ4TXtEOIu7hYwrjOVhrIcAAAAAAAAAAAAAADCjkAIOry4Jh1CXKUJMnwNP9psGQMAAAAAAAAAAAAA6GoIhKDDi+3uDRjX7DYr2tfQJaTOoEMIAAAAAAAAAAAAAKBrIRCCDq/pljHVpYZivKn+MR1CAAAAAAAAAAAAAABdDYEQdHixTbaMqSkxZPN2948dpnJ55Qp1WQAAAAAAAAAAAAAQNKNGjdSoUSO1evUq/7GPPlqoUaNGavz48w5rrdmzX9CoUSM1adINwS4TEYRACDq8poGQ6lJD0b7UgGN1RlkoSwIAAAAAAAAAAAAAIKws4S4AaCtrnBQV65OrxpAk1ZQa6t2oQ4hUv21MrKdHOMoDAAAAAAAAAAAAgHZx+uln6KijjpbFwp/+0Rw/FegU4tJ8Ks+tD4RUlxiK8XYLOF9nKpU84agMAAAAAAAAAAAAANpHfHy84uPjw10GIhRbxqBTaLxtTM0eQ9GewC1jak0loS4JAAAAAAAAAAAAAICwoUMIOoXYNK8ksyTJ5zFk2dNPSmw4X20qCEtdAAAAAAAAAAAAADqv2bNf0CuvvKSBAwdrzpy5Lc7ZsGGdbr55omJiYrRgwWLFxsapqqpK77//nlauXK6cnO2y2+2y2Wzq1StTp556mi67bIISExNbXK+xjz5aqKlTH1VaWg/Nn/9RwDmv16tFiz7QwoXvKydnuyTp6KOP1Q03/E/b33gTf/zjjVq37jv99a/PavPmH/Xee++opqZavXr11pQpTykrq58kac+ePXrzzde1fPkyFRYWyGQyKyurn8aO/bUuvvgyRUdHt7j+rl35+ve/39GKFctVXFwks9miAQMG6rzzLtQFF1wkkymwF4bH49GiRQu1ePEibd26RQ5HnVJSuun444fr8suv1pAhQwPmFxYW6LLLxqlbt1T95z8f68MP/6P//Ge+cnJ2SJIGDBiocePG67zzLpRhGEF/fu2FQAg6hbg0X8DYXJQl9W8YV5t2hbgiAAAAAAAAAAAAAJ3deeddqFdffVnZ2VuVnb1NAwcOajZn8eL6oMbo0WcqNjZOO3fm6bbbJmn37mKZzWZlZvZRenpPFRcXasuWn7Rly09asuQTvfzy64qNjW1VXS6XS5Mn369ly5ZKknr3zlRcXJxWr16l1atX6cgjj2rlOz6w1177pzZuXK/evTMVHx+v6upqZWb2kVQfjLn//rtUUVEui8WiPn36yueTNm/+UT/99IMWL/5ITz89Xamp3QPW/PLLLzRlyp9VW1sjqzVa/fr1l91epU2bNmjTpg1at+47PfzwX/xBjepqu+6661Zt3LhBktSzZy9lZvbRzp25Wrx4kT79dLFuvvlWXXnlNc3q9/l8mjLlz1q8+CPFxyeoT5++KijYpe+/36jvv9+ovLxcTZr0p3Z5du2BQAg6hbjugYEQFfcOGNrN+SGsBgAAAAAAAAAAAOgc9mwztOShGJVuNR18cgfQfbBXY6fUKXWQ7+CTD0HPnr00fPhIrVmzWp98sqhZWMDlcunzzz+TVB8ekaRp06Zo9+5iHXXU0Xr88b+qe/f6AITP59PixR9p6tRHlZeXq0WLPtAll1zeqrrefPM1LVu2VPHx8XrssWk64YQTJUmlpSV65JEHtW7dd617wwexceN6TZr0J1199bWSpLKyMpnNZpWU7Nb999+piooKXXTReN10061KSEiQVN/949FHH9IPP2zS5Mn367nnXvKvt2tXvqZMmaza2lqde+4Fuu22uxQfHy9JWrnyGz344N365JNFOu644Ro3brwk6dFHH9bGjRuUmpqqP//5cQ0fPlKS5HQ69dpr/9Srr76smTP/Tz179tLo0WcG1F9WtleffbZYt912ly6++DKZzWY5HA499dQULV68SG+99S9deeU1SklJaZfnF2yd41OLLi+2SYcQd0mqDF9D3sluIhACAAAAAAAAAAAAHK7PHojR9s8tqtxp6hT/2/65RZ89EBPUZ7Qv6PHppx/L5wv8u+U33yxTVVWlMjJ6avjwkSor2+vfvuXeex/0h0EkyTAMnXPO+Tr++BGSpO3bt7WqHrfbrblzX5ck3XbbXf4wiCR1756mqVP/pqSkpFatfTAZGT111VW/84/3BSfmzn1dFRUVOu200br//of9YRCpvnvJk08+rbi4OK1fv1YrViz3n5s793XV1tbqqKOO1v33T/aHQSTppJNO0bXX3iBJ+vDDBZKkTZs26ptvlkmSpkx5yh8GkSSr1aqJE2/SRRddLEmaNWtGi+9h/PhLddllV8psNkuSoqOjdeutd8owDHk8Hv344/etf0AhRiAEnULTDiG1pWbFeXv5xwRCAAAAAAAAAAAAALSHMWPOVFxcnHbvLm7WeePjjz+UJJ177gUyDEMpKd30wQefacmSrzVgQPPtZTwej2Jj4yRJdXV1rapn/fq1stvtslqjNXbsr5udT0xM1JlnNj8eDMOGHePfuqWxr75aKkk655zzWryuW7dUf3Bl+fJl/uP7Xl944W9kMjWPN1xyyeV67bW3NWPGCz/P/0qSdOSRR+noo49t8V77torJz9/ZYujm1FNPa3YsKSlZycn14Ra7varFdSMRW8agU2jaIaR6t6F4b2/ZzXmSpFpzsTxyyKzocJQHAAAAAAAAAAAAdEhnTa3TkodjVLqlc/Qa6H6EV2Mfa13QYn+io2M0duyvtWDBfH3yySJ/h4+KinKtWLFchmHo3HMvaHZNcXGRfvhhk/Lz81VQsEu5uTu0desW1dbWSFKzbiOHKi8vV5KUmZkpq9Xa4pzBg49o1doHk5ravdmxmpoaFRUVSpL++c+X9Pbbc1t8b/vm5OXlSJIcDodKSnZLkgYNarneuLh4DRjQ0DVk37VDhhy53xr79OmruLg4VVdXKy8vt1kwp3v3Hi1eFx1d/7dmj8ez37UjDYEQdArxPQJ/YdiLTYr3ZEpRDceqTQVK9PYPcWUAAAAAAAAAAABAx5U6yKfL59aGu4yId95547RgwXx98cUS3XHHPbJarVqy5FO53W4df/wI9erV2z83Ly9Hzz33rFasWC6v1+s/HhcXp2OPPU6lpaXatm1Lq2upqqqUJNlssfud03jLlmDaF5porLra7n+dnX3wbXD2deCorKzwH4uNtR3S/aurqyUpYGuZlsTG1gdCampqmp2Liopq4YoGrQ3qhAOBEHQKcenegHF1saE0b5+AY3ZzPoEQAAAAAAAAAAAAAEE3bNjR6tevv3JydmjFiuUaPfoMLV78kSTpvPMu9M8rK9urW265UWVle5WenqFx48briCOGKiurn3r27CXDMPToow+1KRCSmJgkqSEc0RKHw9Hq9Q9XTExDmOONN97RwIGD5HZ7D3BF8+taCm60ZN92O3a7/YDz9oVOYmP3H5rpDDpHXx90edEJksXWkMSyFxuK8/QOmGM35Ye6LAAAAAAAAAAAAABdxL5tYZYuXaJdu/L1/fcbZbPFasyYsf45H3ywQGVle5WYmKTZs/+la6+9QSeffKp69eotwzAkyb9NSmv17ZslScrPz1NtbcvdXXbs2N6mexyOhIQEdeuWKknavj17v/Oys7dp69bNqqys9F+XktLtgNeVlpbqxhuv05//fL/sdruysvpJkjZv/nG/98nJ2eF/LpmZfQ/7/XQkBELQKRhG4LYx9mJD8d7MgDl2M4EQAAAAAAAAAAAAAO3jnHPOl9ls1ooVX+vTTz+WJJ155lmy2Ro6XRQW7pIkZWRkKDk5udkaO3Zs16ZNGyRJHo+7VXUce+zxSknpJrfbrYUL3292vq6uzl9fqJxyyihJ0rvvvh2wTc4+drtdt912k66//mrNmzfXf/ykk06RJH344YIW1/3ii8/0ww+b9P33mxQfH69TTz1NkvTjj99r48b1LV7z9ttvSpJ69EjXwIGDWv+mOgACIeg04httG1Ozx1CsIzAQUm3aFeqSAAAAAAAAAAAAAHQRqanddeKJp8hut+vNN1+XFLhdjCR/B4tt27Zq6dIl/uM+n08rV36jO+/8k9zu+iBIXV1dq+owm82aOPEmSdLzz8/UZ58t9p+rqCjX5Mn3affu4lat3VrXXHOdbLZYrV+/To888pDKy8v954qKCnX33bepvLxc8fHxuvjiy/znrrrqd7Jao7V+/Vo988y0gGeycuU3evHFf/jnSdKwYcfo5JNPlSQ99NA9+u67b/3znU6nZs9+QQsXzpck3Xzzrf6uLJ2VJdwFAMESn9HQIUQ+Q6biLKlHwyG7aWfoiwIAAAAAAAAAAADQZZx//oX65ptlqqmpVmZmHx177PEB5y+44CLNn/+u8vN36qGH7lVGRk8lJ6eouLhIZWV7ZbFYdPzxI7R27Zo2bR1z0UUXa/v2bXrvvXf0yCMPatasGUpJ6abt27Plcjl12mljtGzZ0ja918ORmdlHf/nLE/rznx/QJ598rM8//0z9+w+Qy+XWzp258ng8stls+utfn/VvEyNJ/fsP0EMPPaopUybr3/+ep0WLPlRWVj/t3bvHH2o577wLNX78pf5rHn74L7r33ju0ceMG3XrrTerZs5eSk5OVl5er6upqmc1m/eEPk3TWWWeH7P2HC4EQdBpx6b6AsaMoSZZhcXIb1ZIku5kOIQAAAAAAAAAAAADaz6mnnq7k5GSVl5frnHPOb3Y+Li5eL730mt54Y46WL/9KhYUF2rt3r3r06KFTThmlyy+/SjabTZdffpG2bduqoqIiZWRktKqWO+64RyNG/FLvvfe2tm7dosrKHTryyF/ouutu0N69e0MaCJGkk08+VXPnztPcuW9o5cpvlJeXK6/Xq549e+mEE07ShAnXqFev3s2uO/PMszRo0CDNnfsvrV69StnZW2W1WjV8+Ej95jeX6swzzwqYn5iYpBkzXtRHHy3UJ58s0rZtW7VnT6m6d0/TGWecpYsvvkxHHDE0VG87rAyfz+c7+LSuqaSkKtwldDlpaQmSWvfsV0y36ssp0f7xJa/XaO2Vw1Vu2SxJivIm6vo9+cEpFAAQdG35DgAAdGx8BwBA18TvfwDouvgOACLHnj3FcrnqFBUVo9TU9HCXgy7AYjFJktxub5grCZ3WfM72fVe2lSkoqwARIL5H4C+N6mKT4r0NCTKXqVJOoyLUZQEAAAAAAAAAAAAAEHIEQtBpxDfZMsZebCje0yfwmIltYwAAAAAAAAAAAAAAnZ8l3AUAwRLXQiCkhzcz4FileYe6eX4RyrIAAAAAAAAAAAAAIKL9/e9PacuWzYd9XWpqd02ZMq0dKkIwEAhBp9GsQ8huQ0e4jwg4Vmb+Sf10fijLAgAAAAAAAAAAAICIlp29TRs3rj/s6zIyerZDNQgWAiHoNGzdfDJF+eR1GZIke5FJKZ4jA+aUWX4IR2kAAAAAAAAAAAAAELFmznwx3CWgHZjCXQAQLIYhxfdo6BJSXWwoyTNQJl+U/1iZ5adwlAYAAAAAAAAAAAAAQEgRCEGnEtdo25jqEkOGN0pJnsH+Y2XmzfLKHY7SAAAAAAAAAAAAAAAIGQIh6FTie3j9r71uQzV7DHVzN2wb4zWcqjTvCEdpAAAAAAAAAAAAAACEDIEQdCrxjTqESJK92FCK58iAY3vNP4SyJAAAAAAAAAAAAAAAQo5ACDqVuCaBkOrdhrq5fxFwrMzyYyhLAgAAAAAAAAAAACKSYex75TvQNABtUv/5avi8hQ6BEHQqLXcIGRpwrMxMIAQAAAAAAAAAAAAwfv4Ltdfrkc9HKAQINp/PJ6/XI0kyjNDHMwiEoFOJz/AGjO1FJiV4+svsi/Efo0MIAAAAAAAAAAAAIEVFRUuSPB633G5XmKsBOh+32yWPxy1Jioqyhvz+BELQqSRkBCYXqwoMmWRWsnuI/1i5eZs8coa6NAAAAAAAAAAAACCixMTE+l+Xl5fK5XLSKQQIAp/PJ5fLqfLyUv+xxp+3ULGE/I5AO0ro1SQQUlSfeermOVJ7otZLknyGWxXmberm+UXI6wMAAAAAAAAAAAAihcUSJZstXrW1dnk8Lu3ZUyiz2SKTySzJCHd56IRMP7es8HoPPK9jq98mZl9nEEmy2eJlsUSFvBICIehUbN18Mkf75HHUf0FVFdb/3xT3kQHz9lp+IBACAAAAAAAAAACALi8xsZskqbbWLql++5jGf8gGgsn0cyLE27kTIQFstnj/5yzUCISgUzEMKT7dp4q8+iCI/edASDdPYCCkzPxjyGsDAAAAAAAAAAAAIo1hGEpKSlVcXKLq6mp+3jbGK3aOQXuw2eq7ZNTWusJcSfsxDMkwTIqKsiomJjYsnUH2IRCCTiehl1cVefXJsuoSkzxOKcXUJBBi+SkcpQEAAAAAAAAAAAARyWKJUnx8UrjLQCeXlpYgSSopqQpzJV2DKdwFAMGW0DMwrmgvNhTv7aMob7z/WJn5h1CXBQAAAAAAAAAAAABAyBAIQaeTkBEYCKkqNGTIpGTPEP+xSvMOuVUb6tIAAAAAAAAAAAAAAAgJAiHodBJ6eQPGVYX1P+bd3L/wH/MZXpVbtoS0LgAAAAAAAAAAAAAAQoVACDqdplvGVBUakqQUz5EBx8vMP4asJgAAAAAAAAAAAAAAQolACDqd+IzADiH2nzuEpLiHBhzfayEQAgAAAAAAAAAAAADonAiEoNNJ6NVyh5Bunl8EHKdDCAAAAAAAAAAAAACgsyIQgk4nPt0nGQ2hkH2BkFhvT1m9Sf7jZXQIAQAAAAAAAAAAAAB0UgRC0OmYo6S47o0DIfU/5oYMpXiObDhuzpVL9pDXBwAAAAAAAAAAAABAeyMQgk4pvmdDIMReZMj38zDFfWTAvDLLT6EsCwAAAAAAAAAAAACAkCAQgk4poZfX/9rjNFS7p37bmG5NAyFmAiEAAAAAAAAAAAAAgM6HQAgihtOo0kJdp1d1ivKsi9u0VkKGL2BcVVgfCGm8ZYwklVl+bNN9AAAAAAAAAAAAAACIRARCEDG+t72gDZqjXVqhLxL+ILfqWr1WQs+WAyHd3L8IOL7X8kOr7wEAAAAAAAAAAAAAQKQiEIKIUWXK9b92mMplN+9s9VqNt4yRpKqC+h91my9NMd5U/3G2jAEAAAAAAAAAAAAAdEYEQhAxrL7kgHGdsbfVayVmBnYIqcg3/K9TGnUJqTbvksMob/V9AAAAAAAAAAAAAACIRARCEDFivN0Cxg5T6wMhSb0DO4RU5jf8qKd4hgacKzNvbvV9AAAAAAAAAAAAAACIRARCEDGifSkBY4dR1uq14nv6JKOhS0jlroYOId0adQiRpDLLD62+DwAAAAAAAAAAAAAAkYhACCJG0w4hdW3oEGKJluJ7NA6ENO4QcmTA3DLzj62+DwAAAAAAAAAAAAAAkYhACCJGMDuESFJiZkMgpKrQkNdd/zrFHbhlzF4LgRAAAAAAAAAAAAAAQOdCIAQRI9rbJBBiamsgxOt/7fMYshfXbxsT4+umWE+G/1wZgRAAAAAAAAAAAAAAQCdDIAQRI8bXZMsYo/VbxkhSYm9fwLgyv+VtY2pNu1VtKmzTvQAAAAAAAAAAAAAAiCQEQhAx2rNDiCRV5Bv+12muEQHnii3/bdO9AAAAAAAAAAAAAACIJARCEDEsssmiGP/YYbQxENK0Q8iuhh/3dPcJAed2RxEIAQAAAAAAAAAAAAB0HgRCEFFsSvW/DnaHkMpGHULSXb8MOFcctapN9wIAAAAAAAAAAAAAIJIQCEFEsamb/3VdGzuEJPVuEghp1CEkxpeqJPcg/7jEsk4eOdp0PwAAAAAAAAAAAAAAIgWBEESUxh1CXKZKeeVq9VoxKVJUbMO2MY07hEhSuruhS4jXcKrUsq7V9wIAAAAAAAAAAAAAIJIQCEFEiWnUIUSSHEZ5q9cyjMBtYxp3CJGkdNeJAePiqNWtvhcAAAAAAAAAAAAAAJGEQAgiiq1pIMTUtm1jEns3dAhxVBqqq2w4l+76ZcDc4qj/tuleAAAAAAAAAAAAAABECgIhiCiNt4yRpDpjb5vWa9whRJIq8xt+5JM9QxXlTfCPiy0r5ZNPAAAAAAAAAAAAAAB0dARCEFGC3SEkKTMw4FGe2/Ajb5JZ6e4T/OMac5EqzNvadD8AAAAAAAAAAAAAACIBgRBElGB3COk2MLBDyJ6tgT/yvZyjA8a7opa26X4AAAAAAAAAAAAAAEQCAiGIKDFB7hCSOrhJIGRL4I98b1eTQIj1yzbdDwAAAAAAAAAAAACASEAgBBGl2ZYxbewQkjLAK8PcsG1MaZNASKr7WFm9yf5xYdRX8srTpnsCAAAAAAAAAAAAABBuIQuEvPXWWxoyZIjmzZvXqusLCgr04IMP6vTTT9ewYcN02mmn6Z577lF2dnaQK0U4Ndsypo0dQizRUkq/hkDI3q0m+RqGMsmsXq7T/GOHqVx7LBvadE8AAAAAAAAAAAAAAMItJIGQDRs26Kmnnmr19du3b9f48eP17rvvqqamRkOGDJHT6dR//vMfjR8/XsuWLQtitQin5h1C2hYIkaTUwQ0dP5zVhqoKjIDzvZ1Nto2JYtsYAAAAAAAAAAAAAEDH1u6BkFWrVmnixImqrq5u1fVut1s33XSTysvLNW7cOH399dd67733tGzZMl1zzTVyOBz63//9X5WVtT04gPBrFghpY4cQSUo9whsw3tNk25jerjEB413Wz9t8TwAAAAAAAAAAAAAAwqndAiEOh0MzZszQ9ddfr4qKilavs2DBAuXm5qpXr156/PHHFRMTI0myWq166KGHNGLECFVWVurVV18NUuUIJ4tiFKVY/7jO2NvmNVMHBwZCSrcG/tgneQYrztPLP94V9aXKzVvbfF8AAAAAAAAAAAAAAMKlXQIhubm5OvvsszVz5kxJ0u23367evXu3aq358+dLksaNGyer1RpwzjAMXXnllZKkDz/8sA0VI5LENOoSEowOId2HNOkQsjnwx96QocGOKxsd8GmDbXqb7wsAAAAAAAAAAAAAQLi0SyCkqKhIhYWFOu644/TOO+9o0qRJrVrH6/Vqw4YNkqQRI0a0OGf48OGSpJ07d6qwsLB1BSOi2JTqf+0wgrBlzKADdwiRpGG1N8nkawgcbYmZqxpTUZvvDQAAAAAAAAAAAABAOLRLICQjI0Mvvvii3n77bQ0bNqzV6xQXF6uurk6S1Ldv3xbn9OzZU2azWZKUk5PT6nshctgadQhxmarkkbNN61njpcTeDaGQPS0EQmK9GTqi7ir/2Gs4tdE2q033BQAAAAAAAAAAAAAgXNolEJKVlaXRo0e3eZ09e/b4X3fr1q3FOWazWQkJCZKksrK2d5NA+MUqLWBcZ9qzn5mHLnVwQyCkdo9JNXuMZnOOqf2T5Gs4vi16nnzytfneAAAAAAAAAAAAAACEmiXcBRzIvu4gkhQdHb3fefvO1dbWBvX+aWkJQV0PhyZOPQLGMak1SlPb/i36HCftWNowdhfHK21o4Jw0DVc/naEcfS5Jqjbny5S2S911ZJvuDQA4PHz/AkDXxXcAAHRN/P4HgK6L7wAA6Lr4DgiNdukQEiwmU0N5htG8o8M+Pp+v2Xx0XLFNAiE12t3mNdOPChwXfd/yvAE6J2C8XYvbfG8AAAAAAAAAAAAAAEItojuExMbG+l87HA5ZrdYW5zmdTkkH7iLSGiUlVUFdDweXlpbQrENIUWWuEhxt+7eI6WWSFOcf53zr1BEljmbzUsyjpEa7E/3k/FD9K25o070BAIdmXxqY718A6Hr4DgCAronf/wDQdfEdAABdF98BhyZYHVQiuqVGSkqK/3V5eXmLc9xut6qq6n9YUlNTQ1EW2lnTDiG1ptI2r9l9qDdgXPJTyz/63TxHKdaT4R8XRH0tt4K7FREAAAAAAAAAAAAAAO0togMh6enpSkioT77k5+e3OKewsFAej0eS1K9fv1CVhnbUtENIrantW8ZEJ0iJvRtCIaWbTfp5p6EAhgxlusb6xx6jTkVR37T5/gAAAAAAAAAAAAAAhFJEB0Ik6ZhjjpEkrV27tsXz+4737t1b6enpIasL7ad5IKQkKOs27hJSu9ek6t1Gi/P6OM8KGP837i8qM28OSg0AAAAAAAAAAAAAAIRCxAdCzj33XEnSv//9bzmdzmbn33rrLUnS+PHjQ1oX2k+zLWOMIAVChgRuG1O6ueUf/97OMTJ8DedKo9bq3ymjVGRZEZQ6AAAAAAAAAAAAAABobxETCMnLy1N2drZ27w7cHmTcuHHq27evdu7cqbvuukt2u12S5HQ6NWXKFK1Zs0YJCQm65pprwlE22kG0EmXyWf3juiB1CEkb6gkYl/zU8o9/jC9VQ+uuDzjmMRzaEDsjKHUAAAAAAAAAAAAAANDeIiYQct111+m8887TM888E3A8OjpaTz/9tBISErR48WKddtppuuSSS3Taaafp9ddfV1RUlGbOnKmUlJQwVY5gM2TI5k3zj2tNpUFZt/GWMdL+O4RI0ij70zqt6lmZfTb/sYKor+WVZ7/XAAAAAAAAAAAAAAAQKSImEHIgxxxzjP7zn//o0ksvVWJiojZv3izDMHT22Wdr3rx5Oumkk8JdIoIsMBBSIp98bV4zdbBXMhrWKf3RvN+5hkw6su569XH+yn/MaSrXHsuGNtcBAAAAAAAAAAAAAEB7s4TqRp9//nmbzvfu3VuPP/54MEtCBLP5uvtfe4w6uQy7rL6ENq1pjZOS+/pUnmtIqu8Q4vNJhrH/a3o7T1dO9AL/uCBqmdLcx7epDgAAAAAAAAAAAAAA2luH6BCCriemUYcQSaozSoKybuNtYxxVhqoKDpAGkdTLdXrAuCDqy6DUAQAAAAAAAAAAAABAeyIQgohkaxIIqTUFJxCSNtQTMC7dfOCPQLJniGzeHv5xUdQKeeUKSi0AAAAAAAAAAAAAALQXAiGISM0DIaVBWbf7EG/AuOTHA38EDBnq5TzNP3aZ7CqxfBeUWgAAAAAAAAAAAAAAaC8EQhCRbN7uAeNgdQhpvGWMJJX8ZD7oNb1cowPG+dYvglILAAAAAAAAAAAAAADthUAIIlKMr322jEkd5JVh8vnHB9syRpJ6OwMDIdui58kn335mAwAAAAAAAAAAAAAQfgRCEJGabhlTZwQnEGKJkVIGNHQJ2bPFJJ/3ABdISvT2V5prhH9cYdmq3ZbVQakHAAAAAAAAAAAAAID2QCAEEcnm7REwDlaHEEnqPqQhAeKqMVSRZxz0miF1VweMt8S8GbR6AAAAAAAAAAAAAAAINgIhiEg2b/eAcTADIWlDA1uClPx08I/BQMclMvms/nF29HtyqzZoNQEAAAAAAAAAAAAAEEwEQhCRzIqW1ZvkHwe1Q0iTQEjpZvNBr4n2paif4wL/2Gmq0LKE2+WRI2h1AQAAAAAAAAAAAAAQLARCELEadwmpNe0O2rrNOoT8eGgfgyMcVwWMt8bM1YdJF8mtmqDVBgAAAAAAAAAAAABAMBAIQcSK8aX6XzuMcvnkC8q6KQO8MkU1rFW6+dA+Bn2cv1KW4/yAY0XWb7Qq7s9BqQsAAAAAAAAAAAAAgGAhEIKIZfUl+l/7DI/cqg7KuuYoKXVQQ5eQPVtN8rgOfp0hQ7+qfE1H1v4+4Pj3sS+oIGpZUGoDAAAAAAAAAAAAACAYCIQgYkV5EwPGTlNl0NZOO7IhEOJxGir54dA+CiZFaZT97xpS+9uA418m3CyX7EGrDwAAAAAAAAAAAACAtiAQgohl9SUFjJ1G8AIhvUZ6Asa7vjUf8rWGDJ1c/YTiPX38x6rMuVoX+/eg1QcAAAAAAAAAAAAAQFsQCEHEarxljCQ5jYqgrd27aSBk9aEHQqT62k6vmhlwbEPsDNlNO9tcGwAAAAAAAAAAAAAAbUUgBBGreSAkeB1CehzllcXm848Pp0PIPpmuM9Tf8Rv/2GPU6c3Uo/RV/G3Kjn5PTqMqGKUCAAAAAAAAAAAAAHDYLOEuANgfq7dJIMQUvECIOUrKONaj/JX1H4GKPJOqdxuK6+E7yJWBTrQ/qlzrR/IaTv+xn2yv6CfbKzL7ojW47kp1cx+lrTFz5TZqdWL1FPV1/jpo7wMAAAAAAAAAAAAAgJbQIQQRqz07hEgtbBvTii4hid7+Orr2lhbPeQyHfrLN0TcJ96gkaq3KLD/pk8QJ2hX1ZavqBQAAAAAAAAAAAADgUBEIQcSy+pICxsEPhHgDxru+bd3HYUT1fRpQN16G7+ANd7yGSx8mX6iFSedqRdwD2mPe2Kp7AgAAAAAAAAAAAABwIGwZg4jVvENIRVDXD0aHEEmyyKazqubIV+WVwyhTieU7Zcf8W9ui35HXcLV4TaF1uQqty7UxdqYyXCfr9KoZSvYcIadRJYsvVia1rhYAAAAAAAAAAAAAACQCIYhgVm+TQIgpuB1C4nr4lNTXq4q8+s4ghWvNctVKUbbWrWfIpBhfqvq4fqU+rl9pZPUD+t72smpNu5XiPlLfxT4pl8ne7LqiqBV6p9tI/zjB009jqmapp+vU1hUCAAAA/D979xll11meffz/7HLa9Kreu5ssS+69F1zpvScEAgQIob8EAiQEAgmhhtBCMR1sY3DBvXdZltX7qI2m19N2ed4PIx9pNKq2NBp5rt9aWmt2f/bRmb3PzL7mvkVERERERERERGTUUyBERqyhFUIObyAEYPJZEUt3BkKigmHzoy7TL4oOsNXBKY8ncXr/50vTjeFC7i//e3q89fvdrtfdyJ+qr8RYh4StpCKayvjgfGbn30htNO+wjE1ERERERERERERERERERF7eFAiREWs4AiHTLgxZ+iu/NL3hXu+wBUL2NC44m9d3PktMRM7ZwZrkb1ic+cpeq4YAWBNTMF0UnGdp859lafpbTCtcx7jgbCKTZ3rhesrjSUdkrCIiIiIiIiIiIiIiIiIicmxzjvYARPZlWAIh54dgbGl6w33uYT/GnhxcyuLxnJz7ENd13UVFNOWgtrMmYn3qDzxc8Y88Vv5pfltzBmuTvyFnWgnJH+FRi4iIiIiIiIiIiIiIiIjIsUQVQmTEcvDwbBmh6QeOTCAkXQvjFsRsf2YgCNK2yqVnq6Fygj3AlodHbXQcr+54lA3JP5GOG6iJ5nJPxbtpTjwC1lAeT6TP3bzXbQOnl3sq3w2AZzPMyb2F0/u/gEdqWMYuIiIiIiIiIiIiIiIiIiIjlwIhMqIl4kpCd2cgxDn8gRCAaReEpUAIDLSNmf/m4Igca298ypldeENp+pru2+hzmkjaWhK2gn5nG4+Uf5wNyZv3uY/QZFmW+R92+I+zIPtRxgZn4tk0HmUYzHCchoiIiIiIiIiIiIiIiIiIjCAKhMiIlrCVZNkOQNF0H5FjTL8w4pGv75pef487rIGQPRkMFfGuNjJl8Xgu6fkpmxN30Ow/RkSBNalfk3fahmzb5j/LX6veXJoujyZzSvbjzMm/WcEQEREREREREREREREREZFRRIEQGdEStrL0ddH0YLGHPdgwfmFEstJS6BnY79o7vWFtG3MwDIbJxSuYXLwCgEX9n+aZsq/S4S2lw11Jv7tlr9v1uU08UPH3rEv+jnN7v0FlPHUYRy0iIiIiIiIiIiIiIiIiIkeLAiEyou0eCLEmIiSLT9lhPYbjwdxrA5b8PAFAVDQ89t8JLvv3wmE9zuHkU87p/Z8vTa9O3sgj5Z+g6HTtdf2tiXv5de0CxgZnkbQ11IcncXzub0jammEasYiIiIiIiIiIiIiIiIiIDCcFQmRES8RVg6aLTg9+fHgDIQBnfrDIc7/0sdFAlZAlv/A544PFEVUlZH9mF97I9ML1bE3cz5bE3fQ722jxnibrbi+tY03E9sSDAGxM3sKzma/TGCzC4DKpeCkn5t6HwTlapyAiIiIiIiIiIiIiIiIiIoeRAiEyou1eIQQG2saUMe6wH6d6quXE1wU8d+NuVUK+leCyfxu5VUL25JFhSvFKphSvBCAm4vn0d3my7AtEJjdk/dBk2ZZ4ABioINLjruPsvq8f9pY8IiIiIiIiIiIiIiIiIiIy/FQOQEaUIA+9O3ZNDw2EdB+xY5/1oSLG3VURZOXNHjY+Yoc74hxcTsq9n9d0PMqE4gUHXH95+ofcUflalqd+SMF0HvkBioiIiIiIiIiIiIiIiIjIEaNAiIwYrSscvjgFPjcW7vnnJLD3CiFHSvVUy/SLotJ0ts2hecmx/y1SGU/nFd238Kb2Vbym40mu6rqZMcEZe123KXkHD1V8mF/XnsJ2/2HypuOIhnBEREREREREREREREREROTIUMsYGTGW/sqnr2Xg6ye/73PmPxRIpPYIhDhHLhACMOOSkHV/3fVtse4uj3ELikf0mMOlLB5HGeOoieYwsetC8qadyBR4oPyDbE7eOWjdvNPOn6qvLE2PCU7jlP6PUxMdR5+zhWb/YXJOGw4eVdEMZuRfhU/5cJ+SiIiIiIiIiIiIiIiIiIjsgwIhMmKEhV1f28jQs80hMW74KoQATL84HDS97m6Pc/7p5REI2VPK1oGFy3p+wWNln2Fl+idEprDXdXf4T3Bb9av2ua8ny77AKf0fY1bhDSRsxZEasoiIiIiIiIiIiIiIiIiIHKRjvx+GvGxk6uyg6WybIWGrBs070oGQ6smW+jm72sZsX+zQ32qO6DGPNpckZ/d/lbe3beWqrpvIRGMPeR85p4WHKz7Kz+tm8dua07mx9gTurngnPc4GACyWTYnbeKzs/9GUuB2LPcAeRURERERERERERERERETkpVCFEBkxMvV7BELaDTV2zwoh3Ud8HNMvjmhb5Q5MWMOKmz0WvTs44sc92lwSTAwu4oau+3mw/B9o9RaTsrX0uBuITP6g9hGaLJ3eCgD63CY2Jm9lSuFKut11tPvPAfAc32BMcBpn9n2ZxnDRETsfEREREREREREREREREZHRTIEQGTH2FggZMyQQcmQrhADMuDTkie8kStN3fSrFhns8rv5WjnTtET/8UVcWj+OKnt+UpkNyrEn9mjZvCQXTiUeascEZ1IbH0+Nu5Pn0d2nxn9zrviKTZ33qj0Pm7/Cf4Jbqy7mg97vMLLz2iJ2LiIiIiIiIiIiIiIiIiMhopUCIjBh7tozJtRsS8R6BEOfIB0ImnhaRro3JdezqqLTuLo9H/jPJxV8oHPHjjzQeaebl377XZY3hImYUXsV2/yFWpX5OU+IOQtNPZA78OsUm4J7Kd7O+cAtjgzOYnX8DKVt3mEcvIiIiIiIiIiIiIiIiIjI6OQdeRWR47BkI6W8zJI5ChRDXh6u/lSddFw+av/avyk/tjcEwPjiXC3v/h7e1b+Rdba28tuNpxgSnDVovGddyYvbvScX1g+ZvTN7CY+Wf4re1p7PNf2A4hy4iIiIiIiIiIiIiIiIi8rKlJ9wyYgxpGXOUAiEAMy6JeO+T/dx4Q4bmJS4AnesdurcYqibaA2wt1dEsruu6ix5nPXmnA4NLbXgcLklOzL2P26peRae3ctA2OaeFP1ddy6L+/8fJuQ9jlFcTEREREREREREREREREXnR9MRVRox0jcXs9o7MtRscfDybKc0brkAIQKIcZlwaDpq36UF32I7/clAZT6cxXERDuACXJADl8SSu7bqD2fk3Dvq/BbAm5snyz3N75WvJm46jMWQRERERERERERERERERkZcFBUJkxDAOlNXtmu5vG3h7JuJdVUKKTvewjmnqedGg6Y33q6jO4ZC0NVzQ+z3e3raVq7tupSyaOGj55uSd3FJ9OVmn+SiNUERERERERERERERERETk2KZAiIwo5Y27vs62G4BBbWOGs0IIwPiFEYmyXS1iNj3oYtUx5rBxcBkfnMerOh9kUvGSQcu6vFXcUnUFvU7TURqdiIiIiIiIiIiIiIiIiMixS4EQGVHKGnZ9Xeg2RMHQQIhl+BIZrg+TztxVJaS/1aFtpb5tDreUreOK7t+xqP8zg+b3eOv5Q815bPHvPUojExERERERERERERERERE5NunJtowo5Q2Dp3MdZlAgxJqQiNywjmnKeeGg6Q33ucN6/NHC4HBK9mOc3/NdjN11aSo4HdxWdQNrkr8+iqMTERERERERERERERERETm2KBAiI0rZHoGQ/lZDIq4aNK/oDG/bmKnnRYOml/3eH9bjjzZzCm/iot4f4tpkaZ41MQ+Vf5g+Z/NRHJmIiIiIiIiIiIiIiIiIyLFDgRAZUcobB0/n2gdXCIGBtjHDqWFeTN3sXaGQHc+57Fiqb50jaUbhVVzbdScV0ZTSvMDp48HyDw1ryyARERERERERERERERERkWOVnmrLiLJny5jsXgMh3cM4IjAGTnpjMGjec79UlZAjrSFcwJXdfxhUKWRz8q+sTf7mKI5KREREREREREREREREROTYoECIjCgHFwgZ3gohACe8OsTxdlWmWPY7nzA/7MMYdaqjWSzs/9SgeY+Uf5ycaT1KIxIREREREREREREREREROTYoECIjStmegZC2kREIKWu0zLg0LE3nuww3vydFvmvYhzLqnJT7APXByaXpgtPBI+UfO3oDEhERERERERERERERERE5BigQIiNKeePg6WybIRHvEQhxhj8QAjD/TYPbxqy5zefHl5TRt8MclfGMFg4e5/V+C2Pd0rx1qd+zMfGXozgqEREREREREREREREREZGRTYEQGVH23jKmatC8o1EhBGDGJRGzrhgcCulucvjT+1LY+KgMadSoj05ifvbDg+Y9VP5hiqb7KI1IRERERERERERERERERGRkUyBERpRMLZjdCm4MBEL2bBlzdEIAxoEbfpTnrI8UwNjS/E0Pejz2zcRRGdNockr2Y1SFs0rTWXc7D5T/Axa7n61EREREREREREREREREREYnBUJkRHFcyNTtms62OXsJhBydCiEAjgfnfaLIRZ8vDJr/wJcTNC/Rt9OR5JHi/N5vg92VGFqf+gM/q5vB42WfParvCxERERERERERERERERGRkUZPsGXEKW/c9XW23ZCI9wiEOEf/wf+p7wmYcUlYmraR4baPpIjD/WwkL9nY8AyOy7970Ly808aSzH/x25rTWZf8PRGFfWwtIiIiIiIiIiIiIiIiIjJ6KBAiI055w66v810Gt1g1aPlIqARhDLziv/Ok6+LSvB1LXb42tZz7vpBg+R88ujYNVLIo9MKSn/s8+X2fQu/RGvHLx2n9n6MymjZkfr+7lbsr38HP62bzUPlHaPGeUjsZERERERERERERERERERm1vKM9AJE9VYwdPB00V8O4XdMjIRACkKm3XPKFAn96X7o0LyoaHvtmsjTtZyxBdleLk8U/8XnD73NUjFNQ4cVK2Aqu67yLlamfsiVxD9sTDw5aXnA6WZ7+AcvTP2BC8QIu6vkRaVt/lEYrIiIiIiIiIiIiIiIiInJ0qEKIjDhVEwdP92/3ce2u0MVICYQAHPeqkGkX7rtPzO5hEICOtS7fnl/O98/K8KvXpHnmRz7ZNrOPrWVf0raBBbl/5JruP3N111+oCmfudb2tifu4qeYiOtwVwzxCEREREREREREREREREZGjS4EQGXGq9wiE9G53SMSVpemi0z3MI9o3Y+CKr+WpmhwfeOXddKx12Xi/x52fSPGdhWUs/j+f9fe43P7RJE/9r09UPEIDfhkaH5zDazof57LuG5lauAbH+oOW97obubn6EpoSdx6lEYqIiIiIiIiIiIiIiIiIDD+1jJERp2rC4OmebYaErSTHDmBkVQgBqJpo+dtH+mlb7WAjyLYbtj3tsvF+l61Pudh4/xVAwpzhjn9KDZr30FeTzL02IOg3pGosNdNjZl4WUj1ZrWb2xsFnavFqphavJm/aWZ7+AU9n/g1rBoI6gdPLHZWvZX7uw5yU/QApW3uURywiIiIiIiIiIiIiIiIicmQpECIjzp4tY/q2OyTsbhVCTA8Wi2HktFpxEzDmhF1VQqZfFHHOP0G+G4KcoWKsZdWtHrd/NEmuw8FNWqLCvsef7zI8+9PEoHn3/LNl+sUhNVMtYR5OeG3AhEWHVplkNEjZOk7Jfpz6cAF3V7yDwOkFwJqYZzNf4/n09xgTnEZjsJA5+bfg2TI2JG/GJcnM/GvwSB/gCCIiIiIiIiIiIiIiIiIiI58CITLi7BkI6d1uBgVCYhMQkT8mHtynqiBVNVDVY87VIbNfERIH4PiwfbHDXZ9Ose1p96D2FQeGtbfvaoey+P985lwdMuWciIrxMVPPjfAzR+Q0jkmTi5dxXddd3FH1OnrdjaX5oelna+Jetibu5dnMf+LgEZkCAM+nv8slPf9HdTT7KI1aREREREREREREREREROTwcI72AET2VDkWMLtao/RuMyRs1aB1RlrbmINlzEA1EWNg/Ckxb7o5y6nvKeKlLJUTYhrmRQe/M2tY9SefOz+e4vdvyfCD88rY/NjBhUtGi9poHjd03svUwrV7XW5NVAqDAHR4y/hDzXlsStxGTMQO73FWpv6PpzL/ymb/ruEatoiIiIiIiIiIiIiIiIjIS6YKITLiuD6UN1r6dgy0VOltdpgUVw5ap+j0kInGHI3hHVZuAi7+QoEL/l8Bd2eHmPa1hpW3+JSPsYxfGFHogSU/T7D01x7YfbeZ6W5y+MV1aaacEzHxtIg4hNqZMXOvCUd15ZCUreOynp/T5i3hufR/s81/kKzbvM/1Q5PljqrX7XXZKf2fYGH2kyOqXZGIiIiIiIiIiIiIiIiIyN4oECIjUvk4S9+Oga97txv8aM8KId1HYVRHzgthEIC6mZazP1IctHziaXnO/JCheYmL48KSn/tsuG8v377WsOlBj00P7lr28Ndirvx6ninnHEL1kZeh+nA+F/X+EIBeZxNL099lVeqnxCbEsT6B03vAfTxT9mWyzg7mZz9IVTzjSA9ZRERERERERERERERERORFUyBERqSKcTHNzw60P4kDAy1joXzX8mO1ZcxLUTvdUjs9BGDONSHbFztse8alZ7PD87/1yLbtvQNU10aHX74yw3GvDDjuVQH9LQ71cyImLIqHc/gjSkU8hbP6v8wZ/V/EEuOSoMtdw5+rrqPf3bLfbVemf8zK9I+ZXLiC0/u/wMbErbT4T+HbDJl4LOODcxlfPB+P9DCdjYiIiIiIiIiIiIiIiIjIUAqEyIhUMd4Omo63jYfpu6ZHYyBkd8bA+FNixp8yEOo480MFHv1GkhU3efRu23swZPkffJb/wS9Nz7oiYO61IcaFCadGVE20e93u5czZ7RJYHc3i2q7buLX6GnrdjQBUhbOYnX8jSzLfoOh0Ddq2KXk7Tcnbh+zzOb6Ja9NMKJ7P5OLlTC5eTnk88UiehoiIiIiIiIiIiIiIiIjIEAqEyIhUMW5wOCHcMmbQdNEZ3YGQPaVr4KLPFbjwnwu0LHPIthnaVjs8+b0EPVv2HhBZc7vPmtsHAiJ+xvKqn+WYeu7obitTEU/h1R2PsC71e9JxI5OKl+LgcWLufdxf8fesS/3uoPYTmdygwEh9cDKnZD/GmOAMtibuIxXXMiG4EIM5kqcjIiIiIiIiIiIiIiIiIqOYAiEyIlWMG9zOpLi1fvD0KK8Qsi/GwJgTBl67aRdEnPTGgPu/lOSZH/lg9x0+CLKG370pzXmfKjD3mpDKCaOvWsgLfMqZm3/boHkeaS7u/RGn9n+GVamfsyTz38SmeND7bPOf5c6qNw6aNzP/Gs7v/Q4uycMybhERERERERERERERERGR3e29dIDIUbZnhZDiturB06Z7GEdz7EqWw2X/VuAtt2Y54XUBJ72xyMTTQ4wzNPAR5g33fDbFdxaU89s3p8l1HIUBj3CV8XROzX6WGzrvpSFYCNYwrngOV3f9mTe0P8+l3T9ndu7NpOOGA+5rbeq33Fb1KoWbREREREREREREREREROSIUIUQGZEqxg+uEJLfVjloWg/RD82EU2MmnJovTbcsd1j3V4+WZQ4rbvKHrL/uTo9vzK2grDHGTcDxrw444wNFkhUQ5GDTgy59zQ7GgeppMZPPjDCjKF5WF53IDV33EpLHI1WaX1GczLTitdi+mFbvGZoSt7M2+Tt6vPV73c+2xAP8ueoaTu//Il3uKoqml9iE1IRzGB+cR9JWD9MZiYiIiIiIiIiIiIiIiMjLjQIhMiJVjB1cwSK3LTNouugoEPJSNB4X03jcQMuT0z9Q5DevS5NtG5ro6G8ZmPfofyVZ/JMENdNj2lc7FPsGt58Zc2LEeZ8sMO3CCMc98uMfKXYPg+zO4NAYLqIxXMSC7MdYlv4fnkt/E2tick7LoHVb/cXcWv2KofuwDg3hKUwoXsj0wg3URScckXMQERERERERERERERERkZcnY60d2jtCAGht7T3aQxh1GhoqgIHX/r9ml5PvGgge1MwsYtYkS+tNLVzDZT2/OCpjfDkq9sGa2z22Pu3yzI98sObAG+1Fpj6meoolKsKERRFnfrhIxVhLHMLKWzy2POEy6cyIudeEo6qiyO4slsWZr/BU2ZcOabvG4FTm5d7BjMIr8cgceAORY9Du9wARERlddA8QERmddP0XERm9dA8QERm9dA84OC+8Ti+VKoTIiFUxLibfNVBuom+bz+5vebWMObwS5XD8q0OOf3XI9AtD/vT3aQrdhx4KybY5ZNsGvt6x1GXJjT5Tzono3ODQuX4gAfLMj+CReREXfb7AtAuiw3kaxwSD4ZTsx/FsGY+Vf+qgt2vxn6TFf5JH408ys/Bajsu9k9ro+CM4UhERERERERERERERERE5likQIiNWWYOldcXA10HW4GRriDOdABRN91Ec2cvbzMsi3vt0H12bHGqnxbSucHjkv5JsX+yQ7zL4GZhxScjU80N6tzss+ZlPz9a9l/uICob1dw+9zLSucPn169Jc+q8FFr4rONKnNCKdlHs/mXgsS9PfJmVrmFi8mKpoJhEFtvsPsTVxH53eyiHbFZ1ulqf/l+Xp/2Vy4QoWZj9BTTgPlxSGF1fZRUREREREREREREREREReftQyZj9Upmb47V4i6Ob3pFjxR7+0bNzGRRSmPA1AeTSZN3Y8f1TGOJq9cLUwu+UOwvxAO5iVt3psesAjyB5aKGHKuSETT484+a0BFWN1Odpdn7OVtcnfsjL9Y3rcDftdtywaz0m5DzAr/3q2+Q/R526mYDqpD+czpXgVjvJ/cgxQmTgRkdFL9wARkdFJ138RkdFL9wARkdFL94CDc7haxigQsh96Ew6/3S8Ad34yyTM/TJSWTX7yNfQt+h0Ani3jnW3bj8oYZd9euJq0rXS4/aMptj7plpY1Hh8x7/qQxT/16dk8tKJIqtpy3f/kmHbh6GsjcyCWmG3+A6xI/ZgNyT9hTXhI2zcECzm/9zvURvMIydPrbqQymoZL8giNWOTF0YdAEZHRS/cAEZHRSdd/EZHRS/cAEZHRS/eAg3O4AiH6k3EZsTK1g7NKbuuU0teh6Sckh0d6uIcl+/FC5ZCGeTFv+XOW3mZDmINUlSVdO7Bs/psCfvvmNNufcQdtm+8y/OYNac7/dJHT318cVIVktDM4TAguYEJwAdm+Flakf8hz6W8SOH0HtX2r/zS/qz2dsmg8OaeV2ASk4npO6f8Y04rXkYxr8EgB0O2upSlxJ23es8SEzM/9A/Xh/CN5eiIiIiIiIiIiIiIiIiJyBCgQIiNWeo9AiGmdMGg673RQHg+eJyPL3lrAZOotb/h9lge+nGTFHzz6W3dVC7Gx4b4vJGl+zuGq/8qTKBvO0R4bMraRhdlPMi/3Tpalv0+Ht5wObxm97sYDbtvvbit9nXfaeKTiYzzCxzDWZXxwHo712JK4G2vi0nqbkrdxTdefaQhPORKnIyIiIiIiIiIiIiIiIiJHiAIhMmJl6vYIhLSNHTSdd9oUCDlGJcrgki8UuPhfCjQvcbjp3Wm6m3YFQ1be7NOzxeF1v8mSPDzVkF52MnYMp2b/HwAWy5rkL3k283UsMROLFzMuOIsW/xlWpH5E4Oy/5JY1EVsT9+51WWj6ua3q1VzX+Veq4hmH/TxERERERERERERERERE5MhwDryKyNGxZ4UQ29Y4aDpv2odzOHIEGAPjTo552x1ZppwbDlq27WmXX782Q0Htww7IYJhdeCOv7XyK13U+w9n9X2V68QbO6P8Cr+18klP6P8G44tkk42oqoqn4ceUh7T/vtHFzzaVs8x8iJsAytPILQIe7gifKPs/6xM37XEdEREREREREREREREREhocqhMiItWcgJG6rGzSddxQIebnI1Fle9+sc930hyRPfTZTmb3va5RfXZnj1z3NUTlDA4MUoi8ezKPupQfMsli3+3TQlbydn2tiSuJui011aPj3/SsaGp/No2SdL7WPyThu3Vl9VWsdYl9rweBZk/4lpxWtZk7yRByr+gdgUATg5+4+c2v9ZLBGObjUiIiIiIiIiIiIiIiIiw05P6WTE2rNlTNReNWg677QN53DkCHM8uOjzBVLVlgf+LVma37LM5f8uz/D63+ZomBcfxRG+fBgMk4JLmBRcAkDedLAs/T90eCuYkX8l04rXYTCUR1O4q/KtpZDH7qyJaPef466qt+z1GM9mvsazma9hrENteAIn5z7CtMJ1OLhH9NxEREREREREREREREREZIACITJipWsGB0KC9opB02oZ8/J01oeLGAfu/9KuUEh/i8NNf5PinfdkcRP72VhelJStZWH2k0PmTy1exbVdt3Fn5ZvJuttf1L6tiWn3n+Nu/+1Uh3M4IfdeAtNHu/ccHd4yXJtiWuEaTsi9DweP7f5DbE7chWdTnJB7LwlbSbP/GJl4LNXRrJd6qiIiIiIiIiIiIiIiIiKjhgIhMmJ5KUiUWYr9BoCgPT1ouVrGvHyd+Q9FysfE3PaPKeJg4P+/fbXLE99NcOY/DK1WIUdOY3gqr+x8gOfT/0O7/ywxITEBve5m+tymQ9pXl7eKhyo+NGR+q/80T5R/DtcmiUyhNP+Zsq8MWu+43Ls5o+9LeOy6FvQ5m2nxnqExXER5POHQTk5ERERERERERERERETkZUyBEBnR0nW7AiH59gTJ3ZblTcfRGZQMixNfH+Jn8tz07l0P/x/+WoJ51wVUT7X72VIOt4wdw2nZzw6aF1FkWfp/WJP8DUWni0w8jknFS5iXeycPVLyfTcm/4NkMocke9HF2D4PszfL0D9iYuJXa6Hgy8RjypoPNiTuxJsaxCU7IvZdZ+ddREU8hYSv2uy8RERERERERERERERGRlzsFQmRES9dauncWIch3eCQsmIF8CDmn7egNTIbFnGtCZlwasu6vA5eqMG/466dSvPoXudL7QI4OlwQn5T7ASbkPDFl2ec+vKJpePJsBYH3y9zyT+Spd3qpB6xnrYU14SMfNus1k3eYh82NT5LnMN3gu8w2MdZhSvJoz+/4V16bo8lbT7a4hMP1k4kYqoxk0hKdgGHgTRRTY4T9BJh5DdTT7kMYjIiIiIiIiIiIiIiIiMlIpECIjWqZ2VyWIqGDwe8cTVm4D1DJmNDAGLv3XPJseKiPMDTy8X3eXx+o/e8y5+tCCBDK8dq/QMbPwWqYXXsXmxJ10uivIxOOoD0+iOprNVv8+lmT+m253Hem4nspoGi5J1iR/DWbg+9+PKwic3oM+tjUxG5O3sDF5yz7XqQ5nMzf/NmCg8kiPuwGsYVH205yS/digdbf5D7ElcTcNwQKmFK/C0a1TREREREREREREREREjgF6qiUjWrp2cGsQv3X6rkCIUSBkNKieYjn7H4vc/8VdDYPu+nSSqReEJMuP4sDkkDi4TCleyRSuHDR/UnApk7ovHbL+wv5PsTz9AxK2iuNy72S7/wjPp79Dh7eMgtO1a7/WJzbBIY+ny1vNY+WfHjzTWJ4q+yLN/qPMyb+ZiCIbk39iY/LW0irl0SQmFy+nNjyB8ng84NDhLic0WRrCBYwLziJhqw55PCIiIiIiIiIiIiIiIiKHmwIhMqKl6wYHQrzWaTDjIWCgQojFlto+yMvXaX9XZNlvPdpWuQD0bnd46N+TXPyFwlEemRwplfFUzuj/Yml6WvEaphWvAaBguiiYTiJTpDKaikuSNvc5NifvpNtZz7rU74hM/kUfe0vibrYk7t7rsj53M8vTP9jntsZ6TCpeSn14Eh3eChzr0hguYkLxAuqiE1/0mEREREREREREREREREQOlQIhMqJl9qgQ4rZOLn1tTUjRdJO01cM8KhlubgIu+0qBG6/LlOY99b8+x782YOyJ8VEcmRwNSVs95Pu+PjqJ+uxJAJyS/SjPZP6Ddm8pmXgM1dEsqqPZJOMa+tzNrEn+mnb/uSMyNmtCmpK30ZS8rTRvPX8EYELxAqYUr8SxCfrczfQ5W0jHjdREc+ly19DpLicyBVwSTCpczrz823FJ7utQIiIiIiIiIiIiIiIiIvt1RAIhuVyOH/zgB/z5z39my5YtlJWVccIJJ/DWt76V888//0Xtc82aNXz/+9/nscceo7Ozk/LycubPn8/b3/52zjzzzMN8BjJS7NkyxrRNGDSdN+0KhIwSk8+MOPENAUt/6QNgY8OvXp3h7Xf0Uz3VHmBrGU0q4+lc0PedfS4/Mfd+2t2ltPhP0eEtIx3XMyf/Zu6ufBc7/Mf2uk1FNIVkXEubv/hFj2tr4j62Ju47qHU3J+5iaebbnJh7L5OKl5KMa0jYShz8F318ERERERERERERERERGV0OeyAkm83y9re/nSVLluD7PrNmzaKrq4uHHnqIhx56iA984AO8//3vP6R93n///XzgAx+gUCiQTqeZMWMGzc3N3Hfffdx333185CMf4T3vec/hPhUZAfYMhNA6ZtBk3mmjKp4xjCOSo+nCzxZYc7tHvnOgTVC+0/C908oZvzDi8q/kGaNqIXIQDGagokh00qD513T9me3+w2SdZgqmG7C4JBkfnEtVNBOAoumhw11Ot7uWrNtMRJ7qaDaOTbItcR/rkzeRd9oPyzh73Y08Uv5x4OMA+HEFc/NvZVH/Z2j1n2F98o9sSdyNxTK+eB5TilcwoXghPmVYLEXTTcF0AA6eTZO2DRicwzI2ERERERERERERERERGfmMtfaw/mn9Jz7xCf74xz8yb948vvvd7zJu3DgAbrrpJj796U8ThiE//vGPOeussw5qfz09PVx66aV0dXVx+eWX86UvfYmKigqiKOLb3/423/72twH4xS9+waJFiw7nqdDa2ntY9ycH1tBQAex67ZsedQe1CZn+Tw/S9ZXzStOXd/+aKcUrh3eQclQtudHjtg+lh8x3PMuivwlY8LYiNdNVMUSOjpiAZv8xiqaHmnAukSmwKfkXnkt/i4LTcViO4Vif2AR7XebaJMm4lrzTNmSdRFxFY7iQ6nAOlfFUxhbPpjKewobErfS7W5mZfy2V8dTDMsYXa897gIiIjB66B4iIjE66/ouIjF66B4iIjF66BxycF16nl+qwVghpamrilltuwXEc/uM//qMUBgG4/vrr2bBhA9/73vf45je/edCBkHvvvZeuri4qKyv593//d9LpgQfBruvywQ9+kCeeeIInn3yS3//+94c9ECJHX2aPCiFxe+2g6bw5PH+JL8eOk14fsu7OgNV/Gdw6Iw4NT3w3wRPfTTDpzJCT3hgw77oQL3WUBiqjkoPP+ODcQfNqs8dxfO49bPHvoeh04doUmXgM5fFEepyNdLvrKI8n0BicRsrWsjnxV54o+xyd3oq9HmNfYRCAyBTIutv3uqzodLMlcQ9bEvfsmmkNmIHr7JL0f3F+73fxSBKSZ2x4Bum4kT5n88CY7Zi97ndfYiI2Jf5Cl7eKGflXH/WwiYiIiIiIiIiIiIiIyGhzWAMhN998M1EUccoppzBz5swhy9/4xjfyve99j2eeeYZt27Yxfvz4A+6zubkZgMmTJ5fCILs78cQTefLJJ9m+fe8PwOTYtmfLmLCtctD04WrNIMcO48ANP86z7ekiG+71ePqHPrmOwW0wNj/qsflRj0e+HvPKn+RomKdWMnJ0JWwF04vXDZlfFc1kUnDJoHlTilcyuXgF7d5zbEr8hU53FZ3eSjq95UO2d6yPwSEyhRc3MLPrGhs4fdxV9ZZBi/24nMDpA6A2PIHG4FQMDtXRLGblX0/K1lI0vWz176PVW0x9eBKTi1ewPvkHFmf+g25vLQCLM1/j2q7bqQ/nv7hxioiIiIiIiIiIiIiIyCE7rIGQZ599FoCFCxfudfmYMWOYMGECW7du5YknnuD6668/4D5fqDKyadMmstksmUxm0PJVq1YBMGHChBc/cBmx0jV7BELaywdN55y24RyOjBDGwIRFMRMWFVnwtoC7PpNkxU3+kPU6Nzj89MoMV387z5xXhEdhpCIvjsFQH84fFKDYkLiFR8o/Qc5pYULxAqYXbmBq8Socm2Br4n6aErezzX8AayJScR3puIGUrQMgbzpp9Z4h5+44pHG8EAYB6PCep8N7vjT9aNknKYvHkXVasGbX95drk0MCKqHp5w8153J63xdIx/WMC86lIp48cAz6aPYfIzQ5Yk4gSyvr009QFo9nWuE6HNwh47JYDOaQzkVERERERERERERERGS0OayBkE2bNgED1Tz25YVAyMaNGw9qn5dccgmNjY20tLTwqU99ii9+8YuUl5djreXHP/4xDz/8ML7v8+Y3v/lwnIKMMI4HqWpLvmvgwV+hPTXoEaBaxkhZo+W67+c5/zMFlv7KZ+mvfHq27KoYEmQNf3xHmrP/scA5/1TEOPvZmcgINq14LVM7rgbAMPiNPLV4FVOLV+13e4slZ1rodTfS6j/DpsRt5Jw2XOvT6i8+9AEZS7+7bcjs/VUrebz8/5W+rg7n4JKg011FbIqDV9yZ/RtXPIcTcu8l57RQFo+nMprO42WfZUviLmrDE5hVeB2+LScmZGxwBrXRcVhiYgJckod+TiIiIiIiIiIiIiIiIi8jhzUQ0t4+8HC+trZ2n+tUV1cD0NnZeVD7zGQy/OQnP+Gf/umfuO2227j//vuZMmUKLS0ttLe3M3XqVD73uc8xb968lzx+GZnSNbsHQhKkdluWV4UQ2al6suXcjxU556NFVv3Z47YPpSj07ooPPfy1JOvu9jj1b4tMOTeirNFiVGBAjjF7BkEObVtDxo4hE45hTHg6J+TeW1rW72zn0bJP0uOupzFciG8r2eLfTWD6qI2Oo8/ZQpv/7CEfszqcTbe7DmuiIcu6vFUH3H574iG2Jx7a67I2fzFtewRZMtE4Ck4HkSlQFk2kOppFdTib6mjgX114QqlqioiIiIiIiIiIiIiIyMvdYQ2E5PN5ABKJxD7XSSaTg9Y9GKlUipNPPpmVK1eSzWZZsWJFaVltbS3mCD3VbWioOCL7lQPb/bWvaIDODQNfF7od0tbHmgCAINmu/ycZovGdMOds+PH10LJy1/zmZ13+9L40AFUT4LR3QqIcnvkF+Cm47HMw78qjMmSRo6qBCqby+4GJPbsv7fyk0McO8nSQo52n+S5ruRWLpZZZjGMRYziZ5/gJ23iCMSzgHD7NHO8Gnua73MH7h+U8su720tf97hb63S1sTdxbmmdwmMkrGMtCuliHR4Y65tDBarbxBJaYBOVM5gJO58N4pGhhKZ2sI6LAZM6jmuls4l66aWIal1DJRAB2sISn+BYxEcfzBqZxMRa715Y3IiKyb/psLyIyOun6LyIyeukeICIyeukeMDwOayDEdV3iON5vQMNaC4DjHNxfOa9cuZJ3vOMddHR0cNVVV/G+972vVCHkV7/6FT/84Q955zvfyVe+8hWuvvrqw3IeMrJkdis4ExUNZdlp9JWtBqCP7fvYSka7xjnwD4/DjW+BZbcMXd69Ff76hcHzfnAVLHob3PBNSO12D7IWVRORUa+cMZQzBoBJnINl4H5udmvktZD3EpLH262W0yL+nlpms43HSVNPH9tYxR/pYgMGQ5o6pnEp1Uyjk/X4pNnK42znycN+DpaYNfyJNfxpv+tt5iEe5ot7WWKoZBI9NJWmJ3A6ATlaWFJa6zl+jMHFEpGhgSlcgE8ZWVqpYSbzeA2WiG6aqGEG41hETEg3m2hjOVnaqGQitcymhhn00cwT/CcFeljIexnD/H2OPSCLR3rQ/4uIiIiIiIiIiIiIiIxOhzUQkslk6O7uplAo7HOdYrEI7KoUciD/8i//QkdHB+effz7/+Z//WZo/ceJEPvrRj1JXV8eXv/xlPve5z3HeeedRWVn50k5iN62tvYdtX3JwXkiC7f7aO5kUu//JurdjBkwfCIT02x20tHW/pDYK8vJ29fdh4rk+j387QdfGA79Pnvo/aHoqYu51Ievvdulc79DfZpiwKOa67+eonGCHYdQix7pg0FQFZzCHM0rTx/FPe91q93vAdv9hlqX+F9+WUReexA7/MXb4T9IYLOL0/s+Tc1po8Z7BI0PBdLIheRM97gbK4vF4toxud+0RaCtmdwuDDExv5bF9rDnQJidLKyv47aBlT/KNQdPGelgTHtQInrU/5OTshzku/27K4nEAxIQ0+4+yOPMfbPXvoyKewtl9/8Hk4mUHeV77F+88F1U7EZEjaW8/B4iIyMufrv8iIqOX7gEiIqOX7gEH53BVUDmsgZCamhq6u7vp6ura5zqdnZ0A1NXVHXB/ra2tPP300wC8//17Lzf/1re+le9973t0dXVx//33c8011xz6wGVES9UMfgDvt0+H6QNfxyagYDpJ2QO/n2R0Mg4seFvA/DcHrL/HZfOjLtuecdn8yL4vfy3LXFqWDX7wufVJl19cl+ENf8hSPVmhEJEjbVxwNuOCs0vTJ+TfM2h5RTyFxvDU0vT83AeH7CNv2uly19LlrabdW8La5O8oOB17PZ6xLi5JQpM9TGdw8A42DPLCuovLvsrisq+Sicbh4JF1monNrhBOr7uR26tezeTCldSFJ9DjrqfXbSIdN1IVzWBq4Roqo6k8m/lPetx11ETHAbDFv5vYFDk+97fMzb8dg8PizFdZnPkaMSHl8UTGF8/j1OxnyMRjS8cLydPtriXntFAbHTdo2YsRUaDVe5bKaBoZ2/iS9iUiIiIiIiIiIiIiMpod1kDI9OnT2bhxI1u2bNnnOlu3bgVg6tSpB9zftm3bBu17b1zXZdq0aSxevHi/x5VjV7p68MN3r33yoOms00wqUiBE9s9xYealETMvHfhL99aVDs/9wicKYe61Iatu9Xj6fxP73Ud3k8OPLyxj5hUhp7y9yIRTY/Jd0Pyci+NBqtpSPyfG0R/Ri4wIKVvH2LCOseHpwFs4ve+LNPuPEpoc1dFMiqaHbncdqbiWMeEZJGwFbd4Snij7Z7b491IRT2F88Vyqoln0uBtYl/wdgdNLTTiX8cF5bErcTp87UDEkGdcwN/82qsPZrE/eRMHpwLEJ2r3nCZyeI3J+WXf/bdOakrfRlLxtyPznMv+NY/1SiKSJOwYtf6jiIyzJfINed9Og+b3uRlalN7Ip+Rdm5l9Dj7ueLm81vc4mrIlL69UHJzO5eDmTi5fTEJ6CwcES0+YtYat/H5HJMyY4nfJ4Ej3uOiICEraChK2kz9nKo+WfpM9twrVpLuz5HtOLNww5h5AseaedwGTxbIpedzNbEncTkuWE3HuojId+bsybdlybwqeMFu9Jlqd/RCKupDFcyKTipSRtzX5fTxERERERERERERGRY81hDYTMnz+fe+65h2effXavy3fs2FEKeSxYsOCA+ysvLy993dLSMmh6d+3t7UPWl5ePPSuEuO0TBk1nnWZqo+OHc0jyMtAwN+biL+xqbzX5zIjqKTF3fya13+0KvYZlv/VZ9tudbYyMBWtKyzP1MTMvC5l1RcjU8yL8zBEZvoi8CB4pJgYXDprXGC4aNF0fzueq7puIiYa0SDmr78vknBbK48kYDGfzH8QEWGIc/FL7sjmFN5e2iQnpclfj4OHZMpoSt7PDf4JkXEV5PJk271m63NUkbBWZeCzV0Swy8Tj6nCbWJX9Pt7cWgKpwFtXRbDYl/3xYXovdK4rszZ5hkN3lnTaez3x3n8vb/Gdp85/lmbJ/Jx03UhseR7u3lLzTfkhjjEyOeyrfjdPjM7F4EU2JO2hK3sF2/xF63Y373G5N6le8ovsW0nEjO7zHafYfZUviHrq8VXi2jIpoEp3eykHbJOJqLun5CRODi/a5335nG5sTdxOYXmblX6fqZCIiIiIiIiIiIiIy4h3WQMgVV1zBf/7nf/LEE0+wfv36IVU9brzxRgBOO+00Jk6ceMD9TZ8+ncbGRlpaWvjNb37DJz7xiSHrPPHEEzQ1Dfx17hlnnHEYzkJGmj0rhJiOMYOms86O4RyOvIyd+rcBFeMsj34jQVmDZdHfFJl2YcSqP3nc8ncp4tAM3cgOnpdtc3juxgTP3ZjAS1umnR8y+6qQeTeEeEmwFrADrWxEZOTaMwwC4JGmIp6yx3r+AfbjUbuzJQvAcfl3cVz+XQc1hoXZT9LsP0bBdDKxeDEeKVq8p9iU+AtbE/eRc9qwRCTjGqqjWYwNzmR64ZU8m/kaK1P/R+D0HdRxDpax3iG1twHIOS1sTbS86GPGJuDOqjcOXGvNwbXrKjid/LH6AqyJhiwLTf+QMAhA0eni9qrXcFbfV5hcvJyi6aXPbaLP2UyHt4zt/qN0estL6z+b/k+Oz7+bLnctWWc7ns1QEU1hevF6xgZnYnAI6KfbW0MmHltqo2OxbEz8ic2JuxkfnMOMwqtKQSIRERERERERERERkcPNWGsP7rfrB+kf//EfufXWW5k5cybf+c53mDJl4MHJzTffzKc+9SnCMOTHP/4xZ5111qDtmpqaCIKAiooKGht39Yv/9a9/zWc/+1kcx+EjH/kIb3vb20gkBto6PP7443zkIx+hra2NV7ziFXz9618/nKdCa2vvYd2fHFhDQwUw+LVf+1eX371pV5mF+Z9bwuZ/Prk0fVrf5zk59+FhG6OMTtuecXjiOwnW3eURZPcSDDkAL2UpH2PpbTaU1Vsu+0q+1L5GRAbs7R4gL05MRI+7jh53A+XRRKqj2eScNtYlf8/S9Hfod7YyuXgZJ+U+QGD6iSgwNjyDZu9RFme+Tof3PNZEONZnYfaTnJz9CG3es9xd+Q563A2l4yTiKqqimdREc0nG1WxN3E+H9/xRPPOjLxFXURZPoNtdS2yKYA2TgosZVzyHDck/0eo/XVq3JpzL+OL5RKZIv7OVvNNGbAIsFs+mycRjmFS8lKpoJp3uShw86sKT6HRX0JS8k7xpIzQ5yuNJjA1OBwwF00F5PIn6cD414TxckkPG2OI9Sa/bRE14HNXR7FIAymLpcJexPfEgWENddCJ14YkkbOVwvXwvmsViOPT7s4wcugeIiIxOuv6LiIxeugeIiIxeugccnBdep5fqsAdCOjs7eetb38rq1atxXZfZs2fT09PD1q1bAfjwhz/M3/3d3w3Z7qKLLmLr1q3ccMMNfPnLXx607Ktf/So/+MEPAKioqGDKlCl0dnaW9nnGGWfw3e9+l0zm8PZm0Jtw+O3tArD1SYefvaKsNH3iB5vY+o1df6F9Qva9nNX/78M3SBnV8j3w10+kWPa7XVUBJp8TMmFhROtKl433u4T5Az+QMo7l4i8UWPjuAKPnVyKAPgQOF4slJItP2T7XiQnoczaTsFWDWqOE5NiWuB/PZqgO55C2jUMewvc5m2lK3ElT4na2Ju4nMnlScR2NwalMLF5Mytaw3X+E0GSpjKbh2wqKpofA9BKYLPXhSTQGp3Jb9SvJOUOri7g2SUN4CuXRRDxbTmRyGOthMKxK/2wfJz20wkgmGscp2Y/xTOYrZN3th/AKHjsc61MZTSc0OSwxFdFk+tzN9LmbS+t4toy68ER8W0aHu3yvr0VlOJ2x4RlMLlzBpOIl+Ay0acw6zbR6i+l3tjGxeDGV8VRiIrLONvKmk6LTRcF049k0Y4MzS++5mIi8aSdla/ZaZWdt8nesS/6OxuBUjs//LRsTt9LiP8nUwtVD2vpsTPyFp8u+RN60c3L2IxyX/5ujGgzJmhYSthKP/begk6F0DxARGZ10/RcRGb10DxARGb10Dzg4IzYQApDNZvnhD3/IbbfdxubNm/E8jxNOOIE3v/nNXH755XvdZn+BEBioBvKLX/yCZ555hs7OTsrKypgzZw7XX389119/Pa47tLT7S6U34fDb2wWgfa3hf88qL00f9+ZOmn9WW5qenn8ll/T+ZNjGKAKw4T6XLY+7TLswZOJpcWl+sR823Oex5naPdX91yXXsvxXAGR8ocP5nihR6IMgNPMAqa7A4h/+SJjLi6UPgy09EgaLpJWXrDvkhfcF0sjL1M3b4j9HrbKY6msXMwmuYULxwnw/bm73HuL/ivWSdFurD+YwJzmBseBqNwWkkbQ2bErexNvlryuNJnJz9CClbS6+ziduqXk2Xt2qfYzHWoS6cj2/L2J546JDO4+XGs2VMLbyCdm8pnd6KQcv8uBKDQ9HpGrKda9PUhHPJOTvIOjuwJsK1KWYUXsmM/KuojKbjU8aTmS+yKv3T3bZLEplCabo8msyY4NSBYIuzlX5365BjVYezSdpqzuj7EmPC0w/63AL69xuU2p+s2cGDFR9iU/LPpONGLu/+JY3hqS9qX6OV7gEiIqOTrv8iIqOX7gEiIqOX7gEHZ0QHQl4u9CYcfnu7AGTbDP993K5AyKxXFGi/ddeDoHHFs7mm+7bhG6TIQYpDWH+Py/3/mqR1+cEnPBLllgmnRky7IGT2lSHVU3WZltFBHwLlaIkJ2JK4m+3+I3S5q0nGtVTEk6mIJlEeT6E+nE/CDrw/e50mni77Vwqmi2mFa5hWuI7Q5GhK3MGG5J/odlfT524jEzfSGCxiW+IBck7roOMl41piAgJn8HvdtUlcO/AZJzD9WBMecOzGulijFmR7Y6zD7PybqA/n49ky0raBpsRtbE7cTdF0YYExwWlMK17L8+nv0eE9T0OwgAXZf2JK8SoMA6HOgH66vbVEFKiKZtLmLWZN6jcYHKYXrqfH2cgzZV8m77SXjp2K67iq+yYc69PmLabbXU/S1pCOG4gJ6HE3sC3xIHnTxuz8m5if++CQaikWy6rUT3ku/S08m2Z2/o3MLryp9F48XELyNPsP49sKxoSnHdZ9H8oYgoaV1HMc/a2JozIGERE5OvQzgIjI6KV7gIjI6KV7wMFRIGQY6E04/PZ2AYgC+OqEXW/4yWeHhA+MLf3SvSqcwes6Fw/vQEUOgY2h6VEX40Dt9Jg/vjPN1icPPiAy8bSQRe8JmHVFiDu0sr7Iy4Y+BMrLUUSRVu9petwN5Jw2xgVn0xguJKJAu7cUAM9mKIvHkbDVpUoqlpgW70maEncSmiw10TxCsrR7S0nYKqYWX8GY4AwAOrznaXefx7cZEraSLm81bd4S2rzn6HU2kbCVFJwuQtMPgGtTzMm/hbxpp9V/hl53IwDJuIb6cD6TipeRsJW0eUto37mfyOSH/8U7iqrCmUwILmS7/9CQSihHQlk0kYnFC8nEY5gYXExMwLOZr7M1cd+g9VybpDE4lbzTTpe7hop4EtMK1+HZNEXTS0O4gNrweJ7NfJ02bwkTihcwo/BqOrxlxBSZEFyAbytYk/wV/e4WQpNjc+KuUnum43Lv4uy+r5XCMBZLU+J21iX/QL+zldgETCxexPzsP+Ax0K6z19lEp7eSscEZ9DpNPF7+/8g5rczLvYN5+XeW9rUvOdPGrdVX0+ktJ0UNl3X+irHhmYf/RRYRkRFJPwOIiIxeugeIiIxeugccHAVChoHehMNvXxeAr08vp9g38ICk4biIqiXz6fCWAeDH5byjfdvwDlTkJQiy8Pu3pdl4v3dI26VqLLMuD5n/poAJp0WYQ+u+IDLi6UOgyJETkqUpcQf97jamFa6lPJ5UWlY03cSEJG3tXlv7hOTYlniAtcnfsj55E7EpAlAfLKAxXEinu4LtiYcHVraGicGFVEUzScbVJGw1He5ympK3UTBdZOKxlMXjSNpqtvkPE5ncXsfrxxV4pMk5LSTiagLTN7hiijVgLMm4hln515F3Olib+s3he8Fexg6mqkxteAK+zdDrNJF1m4csrwpncnbf1+jwlvF42WewJt7LXmBMcDrTCteSc1pp9h8hJqIqmkHOaaXbXUdZPI4d/uODtvHjcs7p+zrjgrNp9h+jYLpoDBdSEU1hefqHNPuP0O2uAwzzsx9kXv5dQ963loEfcXcFrCz9zjayzjYqommkbf2QsXY762j1F5Ow5VREU6mO5hxyq6uDFVEgNDmStvqI7F9E5FiinwFEREYv3QNEREYv3QMOjgIhw0BvwuG3rwvAdxaW0bN54K8LK8bHTN1wCVsT95aWv6N1Gz7liBwrwjz85UMplv/Bx7iW8QtiKifFhDnYvtilb8f+/5p2/MKIcz9WYNqFahMgLx/6ECgy8uVNOzv8J6gJ51AZTy/Nb3efp91byrjgbCriyXvd1mIHPWAvmE42JW6j211Lr7uZmCJgGBecxXH5d2Ox9LjrqYymYXDZnLiDFu8ZGsOFjC+ej4MHGBwGqm61uc9RdHpIxTX8tfLNdHvr9joOx/pURtPocTcQm6A031jvoNr07EtDsICJxUtYnPkPMPoR63Db1/9PQ3AKU4pXUh5NoiKeyobETaxN/Zai6SFpawBL0fSWqtwY6zAuOJdJxUtpDBcSE7A6dSNrUr/aY78LmVy8jKbEHYQmx7jgLMYGZ+HaJDmnlT53M1iHlK2lOprF2OBMHJug3XuOtcnf0uo/Q2OwkLn5dxCYXiJToC48kQ5vGfdUvIt+dxtV4UymFK9kdv6N1EbHH7bXyhIfsDLLSGSxrEneyDb/YWYXXs/44LyjPSQRGQb6GUBEZPTSPUBEZPTSPeDgKBAyDPQmHH77ugD8+OIMO5YO/LLfz1hOaX8Ta1K/LC1/XftiquIZwzdQkcOkb4fBL7Mkd8szWQttKx1W/8VjyS98erbs+xf6U84NaTgupnZ6zHGvCkhVDt6PqojIsUQfAkXkcCmablalfk5InvJ4AkXTQ5+7lYpoMtML15OydRRNL8tTP6DZf4RxwTnMy7+TjYlbeD79P7T5z5b2VR8soD48CZckXe5qXJtmeuFaDB4bkn8iYcuZm3sbY8IzMBg2Jm5lWfp/MThk4rHUhHOpDY8jMP3knTZcmyZhK2kMF9LsP8rjZZ8dCBfshR9XsDD7SRrCBaxO3siWxL30u1sw1qUymka3t3aYXlE5oJ1Va16smnAuns2QsFVUR7OwxPS4GyiaHmICMvE4GsNFuDZBYHqpjuYwoXghnd5yutw1pOJ6et1NrE7dSJe7isZwEY3BqWSdZkKTJRXXU3R6aHefI+e0Epoc6biBCcEFTCtcy5TiVfusiBITElMstQnqdtbR6a0gE4+lMppGTIRvy/Ape9HnD/B05ss8XfavwEBw58ruPzIxuPAl7VNERj79DCAiMnrpHiAiMnrpHnBwFAgZBnoTDr99XQB++ao0mx7c1V7jvPaP8XztV0vT13TdzrjgrOEZpMgwikNYfZvH8j94bLjXI8juO+HhJi11M2Py3Yb+VkNUhLEnxZz/6QLTLlAlERn59CFQREYCi6XDfZ6s00xtdAJl8bgjfryss53A9LHDe5KtiXuwWCYVL2Va4ZpBVfAslrxpx7cZPDL0OVtp8Z7CtxkiU2Bl6md0eM8zsXgR8/LvYFPiNvrczVSHc4hNkVWpX1AwXcwsvJpphevwbLrUxueBig+yOvULAFybJDIFAJJxNadkP8683DvZmriPB8o/SM7dsc/zqQnnMal4KatSP6fgdAxatr92NWNYQBfrKNDzUl9SeRHqg5OZXLyCTm8FRdNDRIGC00HOaSVvOsBYyqPJRCZHzmkdsr2xDpOKlzGz8BpiAgpOJwXTRcHpJG868G05VdFMqqKZVEczqYim4pIABiqarEr9jAcqPjBkv5d1/5K68ATK48m7tQCK6XWa6PJW0uWuJRnXMLPwaiwxzf4jlMUTqYnmkDNtbEncRSqupyFcQMrW7fP8s04zy1M/Iue0cmLufVRHsw7TK3v4hWRp95ZSGx5/SFUyi6YH16ZKr7vISKGfAURERi/dA0RERi/dAw6OAiHDQG/C4bevC8BN706x8ha/NH3xmm+yeOYHd033/IQZhVcOzyBFjpJCHzx3o8+T303Qs/XQyoAf98qA8Ysiupsc+poNjcfHnPzWIumaoetaC92bDTaCmmm6Rcjw0YdAEZEjb8/WPbvrcTYAUBFPIWTgwX95PKnUlgcgoJ91qT+wOvVzut11zMq/jlP7/5kedwP9zjbGB+fi4BET0OmupMNbjmsTjA3OJmmr6XOb8OMKUraO9ck/sM1/kNrweM6v+DB5uni073/pdFeSc1qpjeaRiKtZk/olPe5GphSuZEH2o9RE81iS/i8Wl32V0GT3ei4V0VSMdTC4+LaMsngC6biBLYm791qRxViHKcWrqAnnsir1c7Ju827L9h1kkRfHWIfyeDIJW0HOtA56vfcmHTdSE84j57TS424gMrlBy6vCmYQmS7+7DYAphVew3X+IotNdWqcmnMfE4kU4JIgoUBvNIxOPY6t/HyvSPyq9l/y4kld030RteBwWW6p8EhMAzqDvBxj4nup219LmPYtjfVK2nsZgER6p0jo500aXtwpjHXxbQVU0AwePHncjlpjKaNoBgxqWmDXJX/Fo+acoOB1URFO5tut2yuLx+3+xgSXp/+bpsn/DWMP83D9wcvajQ85D5GjRzwAiIqOX7gEiIqOX7gEHR4GQYaA34fDb1wXg9o8mefanu35BdtljN/HU6TeUps/s+zIn5t43PIMUOcrCPDz1/QSPfStBvuvF94RJlFsmnh5RMT5mzitC6ufE3PeFJOvu8ij0DOz3+FcHvOK/8zjeAXYmchjoQ6CIyOh1oHvA3oIsRdNLt7uGnNNKh7uMdm8pmXgsc/Nvoyaau8/9dLor2eE/Tpe7GoNDwlYxrXBNaZuQLKtTvyTntDK1cDUV8SS2+PeRdZqxJiIRV1IRT8bg0u9sp9l/lDZvMa5NUxaPY2xwBmODM1mb/D3t3hI8yuhyV9PpLQcgE43jFd03k4xrWZ36BatTN9LrNgGWyOQHjddYDwe3VLHlpfJs2UD4wCbo8FYMCVbIUOXRQHWSXqcJgyEdN2JwCU0O1yaw2CFVc9JxA8fl/oaacB7rk39kQ/LmQaGiF8JKsQl2Tnuk43rAkLK1VEUzqQtPoiE4mU3J29iUuJ2ss31IMGlMcAbXdP0ZB599eTrzbzxd9m97bHca0wrX76y+MpGKaBoOLk2JO9mYuJW68ARm599IZAr0uBvJxGPIOs0736sbqYimUB+ezKTipfi2gh3eYxgcxgZn4ZEuHafP2Uoyrj6odkJd7hq2+HeTdVpI2HKmFa+lKpp5wO0OJCRbanckI5N+BhARGb10DxARGb10Dzg4CoQMA70Jh9++LgD3fynBo99Ilqav+PPjPHHVGaXpk7Mf4bT+zw3LGEVGiiiA3q2Grk0OD3w5ybanB/7Kz0sPtI7ZsfTw/NXfia8PuOq/8phDK0oicsj0IVBEZPQaDfeAVu8ZutzVTCm+goTd+w/0edNBj7sOg0tlNI2krcFi6XHX0e4uw8HHYGhK3EmXu4qyeAJjgzPJOs2EJsu44GwmFy+n19lEv7udsmgsCVtF3mnHWJeKeGqpMkRInqWZb/FU5l+xJhwylkRcTTquJ20bKJjuUqDFtWkmFi8kHTeQc9owuGz3H6TgdL7k16gynM6swut5uuxfX/K+RpN03Mik4iVMKl5KU+IOsk4zFdEUOrxltPhPHXB7z5YRmv4XdWzH+qVgSyKuYmbhtUwvXM9z6W/RlLwN1yaZl3snBaeLLYm7CcljcEjaajJxI/XhyXS4y9nuPwxm16+njHVpDBdRGU3DEnFc7m+ojKazPvlHCk4HmXgcvh1omZOK66iKphOaHDmnjapoOhFF7q/4e7YnHmRMcBonZf+B7f6DdHgrmFK8gjn5t9DqPUO/s42UrSUVN5CJG0nHY3ZrZ2Rpd5fS4j9JwlYyqXgpSVtdWhZRwCOFxbI58Vd2eI8xpXgljeGpwECbniXpb9DnbmZ64QYmF68YFGzLmTa2Ju4jFdcwIbgIg8FiCckSmhwpW7fPik57Csnv/D7sojqcQ00095hpDzQarv8iIrJ3ugeIiIxeugccHAVChoHehMNvXxeAx7/tc+/nd5XcveJn63nizTNK07Pzb+SC3u8NzyBFRiBrYeuTDjY2jFsQ4SUHWszc9/kkS270iYMXX0kE4MQ3BFz25Tx+et/rxBHsWOpQPTkmXfuSDiejlD4EioiMXroHHD1t3rOsSP2EpK1hauFq6sITdgZPBqeB292ldHorGV88j4wdM2hZSJZNydvoc7aQsFUk4xqStppkXEPK1pI3HXR7a+l2d/3rcTcSE2Aw1IcLmFF4NTPyr8SnjK3+fWz1HyA0fXS762j2HyVw+vBsGeXRRGqiudSEc0jFDTyX+SZ9bhMAVeEsetx1WBMDA9VYxgZn0utupNVbPChwsKeyaHyp5YyMTI5NEJviET/G2OAMfFtOq7eYrLt90LKaaA6O9ehxN1BwukjEVYNaEwEcl/sbphau4uHyf6LbW1ua3xAspDY8HrD0uZvZ5j9UCmNNK1xPZTSVlamfUnA6AMhEY5mXfyczCq+iKpqBwSEmpMV7il53I+l4DIHpZX3yJpoStxM4faVj+XEl83MfYGxwFhsTt+KRYXrheurCkzAYCqaTTnclKVtPVTSTXmcj3e5aKuIpVEWz6He2EpocldHUIRVo8qaddcnfk3V2UBlNIxOPwZqYDncFHd4yyuOJzM29lfJ4Mr3ORgwDFXHavGfpdZuoimYwvnheqXLMoV7/c6Z1Z8WmmOpwNh5pYiI63GWEJotLkppw7qBqNfvS7a4lMP3UhScOueYdirxp5+myf6PDXcaswhuYk38LEQUiky+FiA5kfy3dDma5iMixSD8DiIiMXroHHBwFQoaB3oTDb18XgCU3etz2oV0/zF/+3908+YHq0vSE4gW8ovuWYRmjyLGm2Acty1w6NhjKGiyuB4t/6rP+bo8gO/QXSm7CEhWHzk/Xxky/KMJPW+IY6mbFzH9TQKoK2tc43PTuFK0rXJJVllf9NMfkM6Mh+xDZH30IFBEZvXQPkP2JiYjI77X1SEiWpsSdJG0N44Nz6XCXsy75B6qjWcwsvAaHgd6HOdNKh7cc16aITUCr9zR5p5268ATGB+eRicfS4j3J/RXvJ+tspzqajcGl010JmJ0P5A1ZZwdgcG2S2BSJKFAVzWRCcAHGuqxL/Y52b+mgMfpxBTMLryFpa8iaHXR5K4kIqIlmAw5d7iqKpgdrYrJO85DWQY71qQ/nk4nHMrl4BVv9e1mX+v0hvYZjgjOYXLicpZlvkXfaD2lbOfpcmyYZVxGYvkHBj0OVjhtIxFX0uOtLwak9Qy2ezRCaLABl0USmFa4hYavIOjsomm6akreXlr9YjvVJ2moScSVTvHOZwgV09LaTc3aQdVqJTYFEXEUmHkNNNI+KaDJZZwfPp79HU/L2QWOdl3sHm5J/ocfdUJrvx5XMLLyGaYWrqQvn0+9upWi6qYimUB5PwmJ5vOwzLM18G4CKaAqTi5djrEdVNIMZhVfxXOa/eT79PTyboTFYxNTi1czOvwmDocfZQM5po+B0kHWaeTrz5UHhod2NK57LnPwbmVy8gpStA6Bgumj2H8USUR5NZln6f9iQ/BO10TzO6vt3Iors8B+nLjyRZFzLo+WfpMV/irHB6UwrXIdnM7gkGRucSTKuYYf/OIHppTKaRlU0E5fkXsfyAkvMqtTP2JS4nXHBWRyf+1vA0O2uJe+00eWuodl/jILppCaaQ324gIZgAZXx9FIoJWKgldn+jlUwnbR4T2FNhGfLqA2Po2h6WZr+Np3eCsriCdSFJ1AbHk9VNAMHn2Rcc8Awz6GGYyIKrEr9nKyznYnFSxgTnn7YwzU508q65B/odTfh2zLK48lML1xLwlYdcNui6cGzZaUKXvsTkifrbMOamFRcf9CBowNR4EiOFv0MICIyeukecHAUCBkGehMOv31dAFb/xeMPb9/1A+FFn8+z4jPjKDhdAFSFM3ld5zPDNk6RlwNrodADS3/l8/DXk+Q7DeMXRlz1jTw102L++M40a+/w9rsPP2MZe3JE87PukHDJ2JMHwiOTz4o49e+KpA78exAZ5fQhUERk9NI9QF4uYiI2JG9ih/ckKVtDTTiPCcGF+2wTtCdLTI+7nm3+Q3S6yymLJzIr/3oytrG0TkA/91f8PVsT9+DYJDl3xz73VxlO57j8uzg+97e4JImJ6HJX0uI/Ta+zkRb/abYm7i2tXxfMJ23r6XRXlVq65J12IoqMD85leuF6+pzNNCX+yubEnYQmx7jgbApOFxsTt+61/dALPJuhKppBTEjBdA6Ea3ar2jI9/0omFS9lVepnNCceOajXS+RQuTZFKq6n393yorY31sWaF/HHD9ZQF56Eb8to9557ScGe3bk2SWQKu02nmFp4BRXxFNrd5wFLwlbR7a6j212Lb8v3GV45kERcRX14MoHppc17FoDqaA414TzK44kkbCWO9XHw6XU3sir180MODxnrMaV4FXPzb2VscDqBydLsP8rmxJ1s8x8k57QRmRxl0QQawoU0BgupjmbR6j1Dp7cS1yZx8OlzthKZHElbzTb/YSKTKx2jLBpPRTQVgH53G6m4lmmFa2kMTyU0WUJyFJxOdnhP0uktoy48mZNyf882/0G2+Q8QOL1YLNXhHNJxA83+I2xJ3Dvk+peIq5hWuI6yeCyeLScd1+PbchwSVEczScY1PFDxATYl/4IfVzIuOIvqaBaZeCwN4ULGBAPj6XO20Os2sTFxK+tSv9/1mlrDxOAi6sP55EwbADXRXAqmky5vDem4gamFq6mIJ1EwneSdTmICGsOFZOKxALR4T/NI+cfo8JYxsXgJC7IfoT5cQEyRFv9ptvkPsMN/HGMdJgQXUhseR9bZTr+7nazTvPP7qY4udw0d3vMEpg+Dw4TgAk7p/wQpW0dEDpf0oMBJp7uSLncNvi0jHTcOVF/arRpRTEDBdBGYPtJxAz4DLcJypo21qd+ww3uChvAU5uXfttfQTdZp5qnMl+hztzC98Epm599ATAAYPFJD1oeB4JBD4kUFY/KmncD0UR5PHrR9QB8O/gFDWqPZkfwZwBIDZq//pxZLn7OZdFyPR+awH1tERA5Mvwc6OAqEDAO9CYffvi4ATY+63Hjdrg9nZ324QMtXFtHhPQ8M/MD7zrYdSrOLvEhRAP0thsoJu24JYQHu/kySxf/30ntPJ6ssp76nyPw3BWTbDDued9jxvIuXgFlXhEw8feAXarlO2PKES6IMJp8dYQx0bzakaizJ8pc8DBnh9CFQRGT00j1A5MXLmTZWpH9Mj7OBScHFTCheQJ+7GccmqYnmHLAVRpe7hu3+Q9SH82kIT3nR48iaFtakfsnG5K0k42pO7f8sBpf1yZsojycyvXA9CVu5a32nmfWJm+nyVjO1cDUTgwuBgYc0mxK30eduoiqcxdbE/axI/ZDIFJlReBUTihdgiYlMHktMv7uVHmcjvi3Dt+VsTtxJt7eOuuAkzuz/N9Ymf8cO/1HGB+cxvngey9Lfp8tdw9jgTMYFZ1E0veScFrLODlr8J+lzNwMDVSYaw4VMKl5Kt7uOLYm7yJsOQpOjLB5PwlbQ4S0DBh5iN4aL2OE/NvhFsYYx4Wn0OZuHtCRKxrWl9jC7qwnn4dgE7f6Sg37t/biCKcUrqA1PZFvifrYk7j7obUVkZHrR4aMDcKzP+OBcknEt65N/HHIMz5ZhiYZUq3qxx4pNQDIeCEjWRPPoclezPfHgoPVcm6I+PIn6YAG97ia2Ju4tBZ2M9RgTnEZocrR7SwcFbxJxFXPzb2dG4QZ6nSb63C30O1tZlfr5kHZeA8dJMz/7QY7Lv4ut/v1s9x9mh/8Efc5mAqeXdNzAxOIl1IUn4toEWxP30emuIG0bqQ7nUhPNJh030u2upWC6Sdla2r3n2Zj4E9ZEVIdzBqoBxXVsTdzLVv8+fFvG8bm/Y3Lxcnrc9XS7a+lzt1AWj6M6nEOXu4as00x9dCL1wSk8l/km7d5z1IUnURseR5e7moLTSTKuoSwex5jgdOrDk/BtBTmnjR53PWDIxA1EpkjW2UHO2UHetJOwVZTFExhfPJe0bSAkT4+7nqpoxl5DKjER2/2HaErcQWyKjCueQ8JW0uI/jWNdaqMTyEQDYSKPNIm4ciAIhk+3u44Obxkd7jIC08uYcKCi0QuV2mAgoJEzbbgkStVtGhoqiInY2L4Sz2ZI2/qd61o63GW0eUtI2hoaw1NIx40YHCwWS4TB3euzgJiQ5akfsDjzNUKTZVrxWmbmX0tlNJXQ5GnzFrM0/R3a/efw4wpOzX6G43J/W6rSUzCdbE7cRZe7ilRcT2U8ncZgESl76P2xi6aXpsRtbPMfpjqaxez8G0rVokaCbmcdeaedhnDhQVUp2puAftq952kIF+Dy0n+HLHIsUYWtl0a/Bzo4CoQMA70Jh9++LgAtyx1+dMGu8sAL3l7Efu9ampJ3lOa9tW3DiPpAJfJyseJmj/u/mKRr04vvqXwglRNjEuWWtlUO2IEPUTMuDbExrL/bw0tZzv1YgdPeF2B2G0b7GoetTzlMPC2idoZuZ8c6fQgUERm9dA8Qkf05mPYUL7BYAtOLbysO+Re0Fku/s22glUc86YDbt7tLafWfYVxwNpXRDDYn7mBz4i4CsrgkmJd/B/XhfCyWgukotetJx42lv2pfnPkKS9L/TTpu4LT+f2Z68QZg4C/oNyZvpd1dSpe3hogcFkt9eDJjwlPJm3YiU2BscDYTixcN+qv3jYm/sCTzXwDMyL8KayI2Jf5Cr9tE3nRQHo9nbHAWPe4GutzVlMcTGROcToe3jF5nIynbQJ+zZa9VNBzrM61wHROCC4kpDLQ7wpK2jZRHE9mY/BNbEndjsTQEC/BIE5KjIp5KdTibZv/RgQoZpp8+d/MhP3T243ImFy+n122ixX+yNL8smsjU4lVknRY2Jf5CbIqHtN+XqiyayJTilSxP/QCMxY/LSdgq+t2twzoOEZE9uTbFjPyraUreRt5px4/LGRueQUCWotNdqjTU4254yS3BdufH5WTiMRSdHmJCAtNHbAJgoJVdTTiPjvTTtLOSiIFrdlk0kaStIus077XFnWMTA9VejMVYF99WUBaPpyKaRHk8EYtlq38fPd76QxprWTSe8ngiOdNGr7tpaBjLGmqieTvDp2XUhMdRGx1HKq4jNFl63PXknNaBoFBcS2O4kC3+faxL/X5QhSLXpphcvJwxwek7q8cZyqLxRCbP2uRvafUXk3Na8W2G6YUbWJD9KIm4CgcfB48W72nWJX9PYPqpjmYRmn663XXEBDgkcG0ChwS14XGMDc6kx11Pr7uJimgKns2wPvlH+t2tJOM6etz1pftodTibaYXr8G05Lgnqwvmk4joKTgfd7np63HVEFHFJ0BicypTilRgc1iZ/w0Pl/0jR6aYsGs+lPT8nHTeSd9qpDmfjU0ZMtLNOy9DfK+dNBzv8x1mZ+inN/sN4NsOk4uU0hCeTjKtJ2GqStnpnO7EUvc4mAtNPfXjyoIBOh7uCvNPO2OD0UrWhrNPMVv9+HDwmFi8+5BZbIVlCkydpazAYIooY3FJwJqJIwXRiTUwirihVMjoQi2V98o80e48ypXgFE4OL97luuztQeakmmnvI47dYQvoPelwvRd500OWuojFcNKja08vZquQveKT841gT7WwteBXzcu9SKOoQ6fdAB0eBkGGgN+Hw29cFoHe74dvzd9285l0fUHfje1me/mFp3is7H6Q+nD88AxUZZayFQi/kOwxRYOhYb3jsG0m2PjXwIdi4lqnnRhz/6oA7P56i2H9kkrH1cyKqplgytZZsh2HdnQN/aWAcy4K3Bcy7IaRifEz1ZN3ajkX6ECgiMnrpHiAio9nAg5yR9Qt0i6XZf5ROd8XOh21TSdhyUnE9HukD7+AghGTpa3iKLjYQ9qRJx42k4zF4NknB6aLH3Uinu5K8005Ijqp4OnNzbyNtGwDY6t/HhuTN1ITzmJN/aykUkzftbEncw3b/YfqczZTFE0jaGnrc9XS5a+h212IwLMh+jAXZj5J1Btpf9DpNLEt/n+2Jh/BshtP6Psfx+fewKvUzHi37FIHTA0BjcCrjgrNJxjWkbC2ZeBwTiufjkqTH2UCv20RjsAiPdOmv7bck7qHTXVl6yDiueC4V8WR6nU1URFOZm38bz6e/w4bkLVRHs5hWuJ5m/xHaveeZXLyM0/v/hW53PZ3uChw8etyNbPHvITB9jAlPozKaTpe7kk2J2wmcvdxLraE8nliqgGOsx3G5d7EheTNZtxnPljG5cBnV0SxScQON4ULKo0m0e0tp9RfT5i2m1VtcCrjUhHNxbYoOb1npAe+ejHWYUryahvBkcqaNHf5j9DlbmVK8gpOzHyYwWdq9pXR4z5N1dhATsCVxL8Wd7aH35No0NeFcHDw63VWl/499smZXWyprGBecg8HQ6a0g57QCkIxrKDid+9/PQUjHjUzP38C04nWEJsvy1A9oSt7+kve7pxceJns2TVPi9lIr7ZHAWA/PZg78/yIih8xYFwf/sFTuORwScRWZeCxd3qp9ruPaNLXhXDq8lcQUGbczwJqydbR5z7E58Vd63Y0v6viOTTC5eDll8Th2eE/Q5j8LDAR7GsJFFEwHzf4jWBPvHEuSqYVrWJj9BJl4HK3eMzh4pONGKqLJgGFD8ha2JO6iw11Ot7uudC/NRGNJ2wY63BVAzMTixaRsPeuTNw0K+6TiOqqj2dQHC8g5O2j3nsezA59t+twt5E07VdEM2vwlhKa/tJ1nyyiPJmBwGB+cx5zcW4lNkWcyX6UpeVtpvepwNnPyb2VS8RIiUyDntNDvbKdoeohNkbrwJCYWLyY0faxO3cjz6e/R426gMTiVE3LvoSacR6u/mPXJP1IwHUwr3MDc/FtJ2VpCcuzwHycw/SRsJa3eYlr8p0jEFUwILtz5mSbFmuRvaPeWkokbmVi8iIZwIWuTv+HR8k8RmyIV0VRO6/9nep0m8k4HDeECJhTP3+cfcedMKxuTf965z7HMyr+OHncDnd5yyqIJjAnOoMN7nm53LS5pkvFA1aOyeByOTeDbMjzSWCyd7kpiU6Q2PH5QZSIY+EybdZr3GtwJybMm9StavWeoC09kUvFiyuMpQyrm5E07q1K/IDZFknEND5V/ZFDrS4BxxXO4tOdnpfN9oXqIJWZD4mZa/KdKgap9hdx7nPV0eWsoml4MDom4kvJ4EtXRrCGhqr1VJ+l1NvFE2edp85YwO/9G5uc+tNfqPzERLd6TOLg0hAsPWFHySNHvgQ6OAiHDQG/C4bevC0CQg69N2fWmn3p+yJxbv8iT5Z8vzbus+1dMLV41PAMVEawdCGsV+ww1U2PcnQHY7s2GNbd7VIy3pKotj38rwfq7vf3v7DCbdGbIFf9RwPEsLctcWpY7FHoMqSpL9dSY2VeGJHZ+/uvZZlj1J4/ysZY5V4c4L65CohwG+hAoIjJ66R4gIjI6HY3r//7+Whmg39mGb8sHtTjKmVa2Ju6lOpxLfXTSizpuSJ5edyPpuGGfD0diwiEPMg7tGFma/UeJTUBdeBK+LadgOknFdfiU0+tsosV/moZgAZXxNCIKdLlrqIqm45E54P7zph2DQ9LWlMabdbbT52wlNFliExAz0NKjPjyZ8njCIY9/Y/LP7PCeoMNbhoNPQ7CQseEZjC+eWwojWWK63bW0eE/R426kMprKuOAcHDwi8qTjMUQmT1PiTvJOOzMKr6IsHlc6zkDVIYNLgk53FZsStxGaPjxbhmfTeGSoimaQiutZkvlPtvj3Uh/O58Tc39MYLCIyedq8JeSdDurCE6mOZg95KNPnbKHXbcISUTS95J02QrIUnR42J+6ixXuC6mgO5/V+k8bwVPqczWSdZjq9VWzx76bHXU/K1lEeTaQ8nkhFNJVJxUtLfxUf0M+WxN0UnC7qwhMwuHS5q/FshppwLi3+U2zzHwAgaWtIxjV0eatZk/xV6SGWsS4zC6/hlP6PsyF5C83+owPVEYhoDBcxrngu44KzCUwfmxN3EZp+MvE4ynb+CymQd1rJxGOoDU/EI0Wb+xwPV3yUHf5jpOMGqsPZ9LibBlUcSseNzMi/kpSto8tdQ6v3DN3e2tLyVFzP2OBMXJukxX+SXncTxjpUR3MYXzyPCcH5rE7dyKbEX0oPfPeUjhuYkX8N7d5z9Lmb6Xe277VykLEe1dEskraaVu/ZQQ94ATybOWDVDMcmqInm0O4t3e96IjLyGOvs8zpyrElHY8g77YNaa+2PsR7l8USyzo4h177DyY8rSdoqIpPHt+VUh7Ppc7fS4S4bEqo4JNZQE82laHpL9xg/rqQ+PAnPpnFJAYYW7wmybjPGutSHJ5OOG7BEWBPR7j1PzmkZtFtjXdJxA+l4DJl4DClby6bEbXttRbanimgqC/s/wcbkrWxK3E4mbiTntA4Kzybiaqqi6aTjMVRHM6mMpgOWjck/77Ptox9XMDY4g0nFS9mcvJPt/qN4Ns2Y4HSScQ3WRFhCNiVuI3D6StuNK57NydmP0OEtZ23yt+SdNqqjgXZlL7xmVeEM5uTfwuTiFXg2TZe3hm53Db1OEwlbQVk8EbAYPCYWL6A8nnRo/0/7od8DHRwFQoaB3oTDb38XgP+YXE6YH/jhasyJEec89CPurfzb0vKze/+D4/N/O2Q7ETn6Wlc4LP6JT9tqh8oJlsYTIsacELPlCZenvu+T63DwMwNhjZ6tDoXuI9t7L1llOen1AX0thrV3eATZgePNfkXAdf+TL4VbCr3w4L8nWX+PS+PxMSe9IWDKuRHuyPrjvZcNfQgUERm9dA8QERmddP2X0coSw85o0nDqdFex3X+IqmgGjcGiI9ZOICSLS7p0fkXTQ5e7CoulLjxpUJsrgILppN1bhmfT1Icnl/6i2WLJm3Y8m8anbNA2fc4WVqd+SZe7iopoMrXR8TvbWEVML9wwKHhlsaxL/p7Fma8ClonFi5lUvIyxwemlMFTIQNAn62yn6PRQHc6hMVxERI4ub/XOikUdVEbTSMW1dHgrcG2CicFFlMXj6XO20ONuGAhh2XrGBKfT5j3LhuQthKafqnAmVdEMyuPJ9Ljr6XHXUxZNJGVrWZ36Be3eUiYUL2J+9oP0upvIOi1URTOoiCaRdzrp8JbS7D9Gv7Od0GTxbIaqaCaO9cg5LbikyOystJSK6yiYLtalfs+G5E2l18FYF8+mSw8M/biS2BQAh8poCtXRHKYUriJpq9nq348lYkx4KuDQ4S0jJAtYAjMQbgpMD4HJUh5NoDY6ntrweEKT5fn092jzluDbMpJxDQ4JPJskE4+jw1teqgxhcGjgeMoK08mbDtq9pVhikraammg244vnU3A66XCXEZg+QpPHtcmB8JfJUzBd9LlbBoV2jHWoC+dzUu79TCpewobkn+hy15BzduBYn/J4Eg3hAiYWL2ZN8tesTP/fzpYvLSTjGiqiKYwJT2NC8UIC00ur9yzbEg/Q7a7DEh1SSzLXpphReCWTipex2b+LDcmb917FaSc/riBhK4et3Zgfl5Oy9S+6UsfhlIirdwYLh7flm4gcvGRcy6s7H6EsHn9Y9qefAw6OAiHDQG/C4be/C8B3F5XR3TTw1xtlDTGvWn0Ht1bvqggyP/thTu///JDtRGRke6EdTbICzM7fg2x62OWJ7yRIVljO+GCRTQ+6PPOjBF1NBhvt+mVJssIy6ayItXe6A+VgD4NMfcxxrwoJ+mH1bR659sF/NealLGNOiCkfF+OlINtmsDGMPSliwmkRU86O8Msg22pwfEu65rAMa1TQh0ARkdFL9wARkdFJ138RkSNrh/cE2xIPMDY4k3HB2cRE5E07SVuNS2LYx2OxdLjPE5g+ZtecSZKKl3QPsFgKpoM+dzMRRWrD415U0MkSH1TbhLxpp817jj53C0XThcGhMppOeTQR35bT6a2kzXuWdNwwJJgUE9HpLqfDWw5YIhPQ62wkMP2MDc4otZLoclezNP1dutzVuPgUTDe97kZScR0zCq9mXHAWWacFz6aojmbj2wpiisQmIGfa2Za4j053JeXxJGrCuXS5ayk4nTQGixgfnEurt5ii08WUwit2hn/upc/dQjpupGA6afOWEJuApK2mLBpPdTQH35bT6j3Dc5lvlgIkVeEsZhVez/TC9TxQ8X52eI9TF86nKpqxs4JRJ5XRNGLCUsuyFxjrMSY8lfrgZMaEpzOlcBWxCdjhPU7OaaXodJE3nRRNFwWna2fwaDyhybM++cdB1Rqqw4HXoNV/ujSvPJrE9MINFE0va1O/GdSmZX+MdamKZlC2s8JVq7eYoummJppLzmkl77QBAy1xJhYvwrMp8k473e56+tym0n5cmy4FiBzr77W92oGqk7xQ0cIS0+YvPqjx786PywdViziSjHVLbfFGCz8u57quu9iSuJvHyj7z0qqdHEMu7/41U4pXHpZ96eeAg6NAyDDQm3D47e8C8POr02x5YmfZTGN5z7al/Hbs/NLymfnXcFHvD4dlnCJydFgLhR7ItRuKWUPtjBg/DW2rHTbc69K22mH57/1SxY+jwU1YkpWWbNvAD5EN8yJmXhay4O0BlRMGbrlxONBuJ11jSZQPtMVqX7urMsrYkyOSR+aPdEY0fQgUERm9dA8QERmddP0XERm9dA849sREZJ1mPJsaUoUHKFUFslhiAlwSO0NAy+h1N1IwXSRtNeODc0nYqhc1hogive4mIvJ4pKmMZgDQ7a4h67RQE84hbRtK62fNDp4u+zfWpH5FMq5hUvEyUraGPmcrXe4qss4O6sITmJd/F5OKF+OS3OO8LAaHiAJNiTsJTB9TileU2qe9IGfa6PSWk7CV1IYnYHAomE4StgpLxPrkH2nzljC5eAUTgvPpd7bT5a6iLjyRwPSzMvV/dHlrcK1PeTyZubm3UhlPA6DNe5b1yZsomG5cmyBl6yiLxpO0tRRMJyvSP6Lde5503MDk4uXMzL+GhvAU1if/SKv3NNbE+LaCScVL8GyaFan/o817ll53A57NMCG4kOpwDkWni3TcwITiBWSdHWzzH6LX3UjeaacmnMuMwivpdZto9h+j011BzmllRuFVzMm/hacyX6LbW8OE4gXUhyez3X+ITm8lfc5mQpPDtUmyTguB0wPWUBeexITgfCYUz2dz4m62JO6mMprG5OJldHjL6HbXUxFNZUxwGgB5p40+Z8vOFiwh/c5m2rylQMy44FyStoZt/v0UnM5B/y+uTTM2OIOss51Ob+WgZcZ6jA/OYULxQmJTpM17jpyzg6yzg6zTTGQKpXUnFS+hIprG+uQfcW2SC3q/y4TgAgCaEnfycPlH91ltpyyawJjgNGIT0uo9PaSNzAsScRXz8u+gPJqEJabgdNDhLafZf3RIaxvXJgeN7wXji+eRtLWDqkPt2n81RacLgHHFcwlNdlCQ6kDKoom8uvMRkrb6oLfZH90DDo4CIcNAb8Lht78LwE1/k2Llzbv6NLznmQ5+u2DXh46xxbO4tvv2Iz9IERnRurcY7vtCktYVDrXTYxqPH/hXMT6mb4dhyS981t4+/D1fjGupnxNjDHSscwjzBuNYaqZZupoMcbBb5ZNKy6K/KbLw3QGZutFzm9aHQBGR0Uv3ABGR0UnXfxGR0Uv3ABlOFjvsbbpkMIslZ1rwyJCwL/0hd0QRg4ODV5oXExJRIDIFYlMkGdeWqiGFZIlNhGNdDC4Gr9SibG9jLZpuss4OUnHtoJDR3t5LEQVWpH7MpuRfqAmP4+Tch0jGNfQ5W6iIpw46jsWSc3bQ5a6hz9lCwekgGdcwpXjlkKDRwDlFNPuPsNW/j8poGjMKr8IlSZ+zGUu081wcPFtGytYC0OEuoynxV4pON65NMqVwFXXRieRMKy6JUqij013F5sRfafYfxbEeVdFMqqNZVEbTCUwv/U4zDh5JW8X44nmlNmuHg+4BB0eBkGGgN+Hw298F4O7PJnnye7vK2L3lz/3cc8X0UjKuIprCGzqWDs9AReSY1tts2LHUoWOtQ1mDZe51IY9+I8ET30lQ7Bv8Ya5uVsRF/1Ig32lYd5fHtmdcujYeuITk4eD4linnRLhJS6bWctIbQiaeHmFj2PaMw6YHPRwXZlwa0jBv3yUGjxX6ECgiMnrpHiAiMjrp+i8iMnrpHiAiMnrpHnBwDlcgxDvwKiIjQ8W4wQ87e7c7lEcTSoGQPmcrMdE+E30iIi+oGGupGBvBpbt6G57z0SJnfKBIf4sh225wvIFKHVWTLGZnRuT4V4cAREXIdRrCHKTrLWHWsOUJlw33uWx8wCPMQcO8mFynoXnJi78mxYFhw727btXP/dJn5uUhzYtd+nbsCqXc98Uk9XMjJp8Vkaqy9LUY+nc49LcZjAEvZamdETN+UUTVJEumzlI3K8b1Id8FuQ5D9bRd5ykiIiIiIiIiIiIiIiLHPgVC5JhRMW5wMZve7YayeBKtLAbAmpCc00JZPO5oDE9EXga8JFRNGgiB7I+bgPIxu9ZJllvmXB0y5+oQGNy7r2uj4YnvJVj+R59CN3gpqJwQUzcrpnOjQ8c6h8rxlolnRJQ3xmx7xmXTg/u4PVuzz3Y3bStd2lbuO3yy+VFY8vNd037GUjkxpn2NA9Yw6cyQq/4rT77bEOYNY+dH+GmwlhcVFLEWtjzu0rHOYdblIZl6FSQTEREREREREREREREZTgqEyDGjfOyegZCBCiG763e2KBAiIiNK9VTLZV8ucOm/DQRFDiZcsWOpw5Jf+Cz/o0++88iU7QiyhvbVuwIkmx/1+J/Ty0vTXtpSPSWma6ODl4Ip54Q0nhBjDNRMj5l1eYiXgiAL6+/12Pa0w9iTYmZfFbLiZo/Hv52gdfnA/u+rjXnTLTnqZx/7bW1ERERERERERERERESOFQqEyDGjYvzgB4l9zYb6eOLgec5WGjl1OIclInJQDqXKxpgTYy77coFLvlSgv9UQFeGBf02y/A8D1UEc3zL13IhZV4YUe2HFzT47ljrYePBB3KTFOBDmDj1UEuZMqeJImIdVt/qsunXXcj9jqZwQ073FGbR/P2MJsoOPl+tw+ME5ZZz+90UyDTETT40Yd0qMjaBttcP2xS5hAcbPgp7tsOHJJJXjLae+p0iifNCuCHLQ8rxD7YyYdO0hn5aIiIiIiIiIiIiIiMiooUCIHDMq9qgQ0rPNUBYPrhDS524ZziGJiBxRjrvr2nft9/Kc9t4i/a2GCadFpCp3rXf6+wMKvdD8nAt2oJ1NWWNMsnIgiBJkB5a1LHPIthm6Njo0PerSt2MguGGjQw+MBFlD+5qhLWr2DIPs7vFvJ0pfG8cOCbDsMrDeE99NcOLrA6IiZOot2VbDqj97ZNscjGuZdGaEnwEsTDojYsalIV2bDPlOQ9VkS+2MmLJG+6Ja3oiIiIiIiIiIiIiIiBzrFAiRY4abgEx9TLbNAaBvry1jth6NoYmIDIux8/fdciVZAVPOjva6zM8MBCYmnbH35W2rHO76dJKuTQ7jFkS4Cdhwr0uhz1AzNaZnm0Oh+/CmKvYdBtml0GN46vuJvS6zkaHpoV0fY9b91eO+LySHrJesHAiG1M6IaTw+YvrFEfVzYqICGHcgdNOxzmHHUoc4GnitJiyKKB9jh+xrf+IQ1t/rYiOYen6Enz6kzUVERERERERERP4/e/cdLtdV3/v/vfbeU0/vko56lyxZcm/YxgbbGEyvIRBCQhKSG3JDkhtyudzU+0tCcnNJAoQUpwChBQwYsMEGY9y7LVmS1Xs5vZdpe6/1+2PLRz5WO7KORpbm83oePz4zs2fPmjlba82c/ZnvV0REZNopECLnlOoZjvHe+OeRTkM2elnLGF+BEBGRU9W8zPK+b+WOe7sNoWuDR37QUBg1bPhGgt33x9VB6udZZl5kCVKOQ8/49Gz2aVoScc3vFVnx1pDnvx7ww4+nwZ2dMh2FYUPHcz4dz/ls+laC+/9kcnUSL3DYcPLYvIRjzpUR2SZH7zaP0S5DVDDMuTKibXXE1h8EjPV4tK2OQzZREbZ8L8HA7jiwmKp1zL48IipBVUv8MyZudVY3z7LklpAgBeN9huo2h38485Lrh56tPvkBQ/OyiIaFcXWTUg423xlQHDWsfHtItunUwioiIiIiIiIiIiIiIlKZFAiRc0rNLEf3xvjnqGDw+mZimnycib/1PuapZYyIyHTzAph50ZHqJMvfHGIjwMW3vVR+EFJ1TLRpWfP+kNYV43Q855NpchSGDLt/5jPS4ZHIOqpaHDMviqhqcTCWIVkNBzYVePpfkoS544dIEll3wvY0J/LS6iQvD4MA2JJh70NHv0Xa+ZOAnT85cv3eh4JjblcYNpO22/StxKTbXxpICTKOluWW4YOGsW5v0nbpBkftLEvfdo+oGG//yN9YbvyjAstuC0lWx9vlh2HPAwEv3BGw95GAlmURN/5pgVkXH7+izPE4x6QWOzaCvQ/6WAtNSyx1c9SCR0RERERERERERETkXKFAiJxTamZMPrk11hGQXTyTMT8OgoyqZYyISFl4/rGvT9cffd3Mi+ykQMnaXygd874tLfH/e3qKrP1Aiee/niBV45h1iaV/h0fn8x6zLo644N0hxsQVNvyUY/iAxwt3BAzu82iYb6me4Rjc69G/06N/h8fgXjOlFjXl8tKxhLm4gsmx5AcM+YHJt+X6PO76rQx3f9xR2x5XChnabyZVYDnwZMCX3+iz8HUR2WbHeK8h129IVsUBnEU3xVVGHvjzFP07PVpWRqRq4MCTPli46MNFrvytIrZo+PaH0xx44sjbxYaFllv+Ks/86yKcg7FuQ+8WjwNP+gwd8GhcaFnw2pC21RYzOd8yJTaCjd8M2PnjgOallit/qzjl9jv5ISiOGWpnnbyCig3hwFM+ySpH8zJLcHS3IRERERERERERERGRc55xzqnu+HH09Iyc7SFUnJaWGuD4r/0jf5PkoU8fOWvz7q+Os/Fdr6Mr8Xh8hTN8pLcXj8Qx7y8iIq9eJ1sDXqmoCH07PHb/zGfvgwGFYUOiymEjKI0Zqtossy+1ZBodB5/22PCNBC6KAxZ+0lE721EYhvHeIwmHTJMl1zc58dC0NKJ5qWXHjwOiwqsngPJKeAmHLR3nORhH60rL4F6P4uixt8k0WuZdG7Hg+oiGRZau5z0OPeNz6FkfG8KMNXFYZXCPh3OQbXQ4Bz2bPfp3HAnBLLwx5J1fyk201Rk+aNj3iE/XRp+RQ4bCiKGq1THWZdjzoI+zhgvfX+SWvy7gJyAqwb6HffY/7tO53ifb7Fj9vhI//eMUXc/7E8912W0hN/9FnkzjiV+XsW5DKQf18/T2WWS6nak1QEREXt00/4uIVC6tASIilUtrwNS8+DqdLgVCTkAHYfmdbAJY/9WAH/72ka8Kv+Fv8vT9xgfYmb5j4rqf69tAjZ13ZgcqIiLT7tXyJnBgt2HvwwENCyztl0YE6ThYsOunPoN7POZfH9G8zDKwyzCwxyPMGapaHLMujfD8eNvSeNxOp3eLR+d6Hz8F2SbL7gcCDj7lk6x2ZBocnc/7DO/3qGq1NC+ztKywpOsdnc977HsooDgWBy5q51gy9Y6uDccpzXIeq51jWfamkN0/8+ndMsXnbxyr3xey88f+pCDPiTQvi3jXl3PUz4/fGhdGYOSQR/cLHvse8dn3SED/znhfS99Y4rX/u8DunwWMdBiCNNTNtiy+OZwUKomK4CUmt+E5+JTHngcD5l4TMefKaGrPR6QCvFrWABERKS/N/yIilUtrgIhI5dIaMDXTFQhRyxg5p9TOnJxfGukwVNnZk64b9Q8qECIiIq9YwwJHw4LJbW38BCy5JQKOnMBvXORoXHT0CX0/AX5d/POsSyyzLjnSLmfJG47e3oZxeOTlwjzsf8ynMGpY9LoQPwnbfhiw90Gfg8/45PoNNoTqNsfsKyPmXxcyc63lob9K8sIdCUrjR1IIxncTVU9eiaalEaWcYXj/K+gDc5qG93s89Y/JU7uTM2z42qlVC+vd6vOPl1dTO9uSHzTHrX4CsO3uBNvuPnr/XuCYeZGleoal5wWf/p0emSbLBe8Mab8sYsudAVt/cOR+864NWXB9RCkHfds9Rjo8bAgYSNc6qmc65lwZUjvbMXzA4Plx257erT57H/IZ7zNERaiZ6WhbHWFDw3ifobrN0rzc0rzUUtvucBacBT8ZB5Y23REwvN+jebllxpqIujnx+6vhg4Y9DwQceMIHE4dkWpZbWlZaqtvcpGDLmTbSaab0mDaKw1q5fsOyN4Ukq8szPhEREREREREREZFzgSqEnIBSSeV3skRYzxaPf72uauLymg8UmfMPf8ejNb8/cd2Nw7ezuPCeMztQERGZdkoFTx9nIddvyA0YMo2OTKMjzMPunwU8+28JBnZ7rHhrias/XqSUi9ug1M1xdK7zePILSbo3eQwf9Mg2O27+dJ5Fr4vID8OP/2eaTd8K8Hyon29pWOhoXBCHChoXWw4947PngbiaRmHk6DP5iazDT0J+8Phn+ZuXR4x0eBSGznz6INtsp1xB5Fw10f7HOGpmOEY6jn6+mUaL58NYz/Ffi9p2y6KbQpa9OWTeNRG5fsPeh30OPOkzfMCw6KaI1e8tceApn+6NHqWcoTBkGO0yBBnHkltC2lZbOtd7DB3wGO811LY7lr+lRLruyOMMHzJ871fTHHgyoLbdcuXHiuy8L6DjOY+lt4bc8McFiqPmcIDG8My/JulcF1eOqZ9vueXTecIipGpgzlVRWUMsex7w2fK9gMbFlot/qUSQOvl95AitASIilUnzv4hI5dIaICJSubQGTI1axpSBDsLyO9kEkB+Cv11y5OBfcEPIFd/5JvfWvX/iustH/4S1uY+f2YGKiMi005vAc0NxDILUsauavMiGcOhZj70PBYz3GVqWW2ZdEtG83GIM9O/0CPPxCfwgBeP9Bs+DZI0jkYkDLQ99OsmT/5gkzB05oz/zooiFN4bMuTIOoKTqHKOdBhcZGhdZ7v2DFOu+dKSaiPEdC18XsfLtJRoWWJ65PcmW7wfUzHC84W/yzL8uYsM3Au7+7fRJK6h4gaNlpaVrgweujCmDV6HqNstYj8HZ038dElnHoptCatsdxRHY+K3EpN/56Vj6phK3fTY/qWrI4F7D3kd8wpzBT0L7ZREN8y1P/XOSA0/4zHtNyEUfKpHIHn+/NoQDT/oYL77/WI/hZ3+aYtO3jlR/WfqmEm+7PY/nQ24ABnZ7VLU6qtscQ/sNfds8+rb5mMCx6PURzUvtMR+rb7vHc19MkMg61ry/NNHSaLrZMK62crZCLM5BbaaGZBX09WkNEBGpJPoMICJSubQGiIhULq0BU6NASBnoICy/k00AzsFnFlVPlHFvWhrxtscf4TsN109sszL3K7xm9G/O/GBFRGRa6U2gvFxUhP5dHmNdhsbFcfuTE3EO9j3iM3LI0LDA0rTMkq49ep9eAOYlxTA613s896UE3Zt8hvcbMk2OmpmOmlnxY7ZfGtF+WUSyKm5P8pNPpcgPxi1Klr05JCrCrp8GbP9RwPCBeMfGdzQvs/Ru9abcrsd4Dj91+OR8qbJDJ9OpqsUSZCBZ5ejZ7J98+1bLZb9WYv51Ibt+GnDgSZ/uTR5hwdC8LGJwt8dolzex79ygOe7vq3FxRP+OEz+m8R0XvDMOOmWbLXOvibAleOqfkjz+ueTEvo3vWHxTyKxLLfkBw+BeQ+1sx5JbQ/zAkR82tF5gyTQ4Nn0rQddGj3mviVh8U0j/Lo+oBC3L4+DJ3od8RjoNWMOun/ps/1FAut5x86cLLH9zOGl8vds8tt0VMNoZt6ma/9qIZbeFE9VXcgMwuMej9QJLcQye/uckYz2GtR8sMWPNsYMuLxUW4Pu/kWbr9xPMWgtv/qdRGhboI6qISKXQZwARkcqlNUBEpHJpDZgaBULKQAdh+U1lAvjX67MTf8wPMo7f2Lub/2xZNHH7vMIbuWX462d2oCIiMu30JlDOB1ERRrsNmXpHshpGuwx7HvQZ7fAICzD/uojZV0Tk+uHgMz42NAQpR8MCS91ch+fHwZbSOHRt8Nn3qE9pPG7pExZgYJdHut6x8IaIGWsijA+9Wz36tnkkqhyZBsfQ/vhy71aP4UMeiaxj5KDH4N44xFA9w3LVfy8y3mfofN6n63kPZ6FtlWXmRRELXhuRyDp6tnj0bPboWOdz4PF4rHLm+cn441lUnJ7X2/huIpSUabIESY7ZOuhFmSZL81JLIgNjPYbuTd5R1WDmXB1y0/9XoHerxw9/J01p/OixGt9x2a+VWPDakOGDHgee8LEh1M21FEYMIwfjyj5dG31233+k5FB1m+W2z+eZfXlExzqfwjC0XWhJ1zk2fD3BgSd8BvZ4GANX/Lciy24Lj3rsYymOwViXoXpmXIno5UY6Dd0b439fDfMd2eYz9zHZOcBNDqaJiFQqfQYQEalcWgNERCqX1oCpUSCkDHQQlt9UJoBvfTDDjnuO/NH4Yy+M8PXlLUSmAEBTaQ3vHHzozA5URESmnd4Eipw5zkHX8x4jnYZ5r4mrnZyK/DBsuytg3ZeS9Gz2yDY7lt4asuCGkNEuw1P/mKRni8fsyyNWvSektt2SrIqrbXSs89n6g4DiiKHtwojWlRbnYNdPArb8ICAqTA4T1M213Pr/8nRt9NjzQMDsKyIaF1ru+98pRrs8atsty24LSVQ5qlocy24LKY3Bg59Okes3cYhgSOGV84nxHek6R67/6ATFvGtDFlwfUT3D0rTEsu2HAS98O0FhxFDdZjEeFEYMwwcMOEMiGx8z814T0rDAEYWw+dsB67+SmBR8mXtNSMtKy1iXoZQztF8a0bTUUhqL91ccNfgpR7bR0bjI0rba4iehf5dhw9cSHHzaZ+Zay5oPFimOGcIctKy0DOz0+N5HM/TvMrSssMy9OuKCd5aYebGdqLpyptkIiqOQyICfPPn25eIsrPtSgv2P+6x6T4mFN0Zne0giUgb6DCAiUrm0BoiIVC6tAVOjQEgZ6CAsv6lMAPf+zxTP/uuRv1x+6J4xHrjxQoaDXQCkbRO/0Lf7zA5URESmnd4EipzbohL4iVO8TzGuzDDaaTAeJKugeZk9ZuWEqAgjHXGbFO8knV9e+E7Ag3+RojgKqZr45Pd4n6F+vmPl20u0rIjb+Tz77wnGe+OQyfK3hHSs89j/WHDU/jKNlkSWuCWQcSy4PsI52POgT5CG1e8pcdlHi9S2O77zkQw77433EaQdrRdY2lZFjPWYONAyO67A0bTEsuv+gA1fO/GLNuuSiNp2y+77AwojCrq8Wvkph/EgzL2y31HNLIsXQLrO0bY6wk/AwG6P/LAhKkDNTMesSyJStY6oYGhYaGlbHXHwaZ++7R7ZJsd4r2HL9wOG9ns0LbbMujiiOGYo5eJ/B+N9hq4NHmPd3sSYZ18eseTWkIt+oXRUOKSUg/yQIT9giEJoWmwJ0tDxnEfvNo/aWY7GxRZbgmQ1ZJtO708L9/9pkic+l4ovGMc7/j3P0jdOrQKMiJy79BlARKRyaQ0QEalcWgOmRoGQMtBBWH5TmQCe+IcE9/9xeuLy227PseODb+BQ8sGJ636pp4uAY9SCFhGRVy29CRSRM8k5jqrAEBVhcK9Hw4L4ZDxA1waP9f+ZYGC3x9zXRCy7rUTDAocxcaUUP4BENt62MBqHYILU5McZ2m8IUlDV6k5a9WFgt6FzvU8pD4ee8tn9QIBzsOh1IWt+vsSMNRaIqzr0bY9bAaVqHPXzLfsfDTj4jEeyGgyw5fsBI4c82i6MWP2+ErvvDxjaH7dmiUqGXff5uMgw56qQRa+LMH5cZWXmxRHf+aUMvVuOkbQxjrUfLLH2gyV2/TTg8c8mKY4e/0nVzIxbD227+xTTQYDnx89Tyi9V61j0+pC+7R5j3Yb8kCHMm6O2aVhg6Vx/7ETW7CtD5l4dERUMpXHAQGHYMNZjSFY5mpbEQaimpZamxZZkdXy/wgg8/c9JHvp06qh9XvfJAguuD2laaicqG4V5GNrv0b3Jo2ujR7oW1v5CkeK4YdvdAbWzHEveEFcv2veIj/GgcZGlZYU9bkWUns0eT/9LgrFew5W/WWT25faVvpRnXN8Ow75HAuZfG9KwcOp/zhntMgRpR7ruDA5O5BXQZwARkcqlNUBEpHJpDZgaBULKQAdh+U1lAtjy/YDv/vKRsMcNf5Rn/A9+ie3pr01c957+Z6iPlpy5gYqIyLTTm0ARkdPjHBSGIF1/7Nvzw1AaM9TMPPojYCkHW78fEKRh7tUR+WHo2ezTsiKi8SUnncd6DOu+nGDdlxKMHPJY+qYSb/zbPN2bfEYOGRbfEpKqge4XPDqe8+NwTBrmXhWRbbYM7ffiViUpx3P/kWDPQwGtF0R84IsJwgL87HMFujZ4jHZ5h0MDjk3fSlAYNrRfFnHt7xeYeUnEQ3+Z4pnbJ7d5eal0g8OFgAeJjKO23VHVYtnzYEBp/Bj3MY55r4loWGDZfGdCbYfKoGamBQPjvYaoeJLX2ziqmh1h0Rzzd5NucIQ5JkIsLSsiBnZ7k0ItyWrH/OtDEpm4olHjIku2ybHvUZ9tdwfg4m39pOPNX8hTN9sSlaBpicVZw+Aeg5eA+rkW48dhlyANQcpx8Gmf7o0eGEjXwfzrQ+rmxP9unIMDT/j0bfMI0o5Mo6NxocVPQ982D2OgZYUl3eAoDMVtjV4Mnb0oNwAjHR4bvpbg6dsTuCgO2fz898ZpW33i8IoN4ad/nOKZf02Ag4s+VOK1/7swEcgROdv0GUBEpHJpDRARqVxaA6ZGgZAy0EFYflOZADrWeXzx5iON7y/+pSINf/+/eK7q/05c96bB79Feeu0ZG6eIiEw/vQkUETl3OAelcSaqNpyuE60BURHyg4aq1skfXce6DT2bPUq5OLzStcGjqsVx4QdKzDjOSfLiKBx8yqdvp8fwAQ9n46DA0lvDiRPr+SF4/qsJxnoMC2+MqJll2f9YQH4obr2SqnEkqx2lnGG0y9DxrE/XRg8/EbdNab8sYuGNIVt/kKBro0ey2jGwy6Nnc1xZo3qG5T1fz1Ez07LpjgSbvpVgeL/BC2C02+CiIyGGIB23SCqOTT2gUjPLMnLoGH2XgExTXJ0jVQM9W+LXQKafl4grrjQudGy/x6d/x0n6XL1M7RzLjAsj2i+J2PbDBAefOvb9GxZY3vHFHEHSUTfXTVQ6erEi0vAhwz2/n55oY/WiqhbLnKsjmpZYamY4Zl0a0bTY8vjnkmz/YUDzcsuq95QY6zIM7Y9bXUVFw8ZvBgzt86ibY2lZaZl/XdzGaP+jPl4Clt4a0rzs8L+jQdh6d0BVi2PhjRHGg8HdhsKYwYUQpOOqMzWzHM7CC3cE7L4/YKTLkMjCireWuOBd4UmrLJ1IfhgOPe0zY42daGl0rGpRZ5IN4wBSQgVMj0ufAUREKpfWABGRyqU1YGoUCCkDHYTlN5UJYLzP8PcrjnydadFNIRfc8XkervntieuuH/4Cywo/f8bGKSIi009vAkVEKlclrAGdz3v07/RYdFNI6jjVGUo56N0aV42oX2BJ1x5uQ7TP0LUhDgU4Bwef9BnY41E/zzL3mojRDkNxzDD7ipD2yyzjvYbRTkO6wZHIQHEE/BRUzzjSxsg52PCNgHs/kSbMHTlDXjfXkml0pOscmYb4/2O9hh33BNjQkG22LL45JFEF4z1x1Yz9j/nTEi6ZdUnEZR8tctdvTR6TnFzdXMvCG0J23hcw0mGobnWMdpnjVtE5U+ZcHZKuc+z+WTDxO2xYYImKMHzw6GPETzmiwvHHmG2OAyYXf7jEzIsjXrgjQa7fkG12JLLxn7MyjY7aWfHzHT5kaFzgMJ7j0b9NURg2eAlH3WxHkHYM7vVYeGPIkltD9j/uM9rpUTvLUj3TEaQc2eY41JWpd/Ru89l9v8+hZ33S9Y7lbwlpvyzCWeje5DFyyKN5maV+vuXBP09x4AmfxTeHXPuJuALL7vt9fvg7aUY6DUtvDXntpwoTbX5sBLvu89n2w4B0HVz5sSLpekfHsx692z3GezyalloW3xROBH2OZ7zPsPPHPrvuDyiOGNpWR8y9JmLea+Igjg3B+OUJwtiQk4735c6X+T/Mw/BBQ8PCk7eLExGR2PmyBoiIyKnTGjA1CoSUgQ7C8pvKBOAc/L8F1ROlnpuXR9z8+Hf4Ud27J7a5dOx/cfH4J87sYEVEZFrpTaCISOXSGnD29O80PP/VBJnG+IT3i61OXm68zzCwy9C6yh5V7cBGsP9xn7EuQ82sOETiLCSyjqpWR27A0LfNo2+7R9+2+IT30F4PLxFXXJl5ccSKt4TMfU2E58PQAcOBx31GOjwG9xi6X/DJ9RuCtCNV56if62hYaPEC2HZXQMdzcVhm8S0hB570yQ/EnxXnXhMy+4qIgV0eu+4LKIwc+yxtssox56qI3Q/42JLO5MrpqZ5hqZnl6Hj26MouiawjyDiivJlS5Z+aWZYVbwuZuTYikXWM9Xjse9hncK9HutFRcnpX+wABAABJREFUGI4DYscK/qTrHXVzLf074nZFK98Rsuj1IYks9Lzg0bvNo6rVMWNNRMdzPgM7PermWlpXWQ494zN8MA6yrHhbiPHiv8WMdhpeuCNg47cSjHYZ0nWQaXCkah1D+w39O3yq2yzL3xpSP98y2mHAAA4OPBmPu3GRZfEtIbXtjlSNY/XNWTJ1R+Z/Z+M5xU8c/Xrk+mHb3QkG9xpy/YbWCyyr3lti/+M+W38QMN7r4aI4XLbmAyWq247/J8/BfYZH/iZFcRSWvjFk2ZtC/NSR4ExhFHbcE+AnoXGhpXmZPWbYxTlY/58J7v/jFIURw6xLIm76izw9WzxKY4Y5V0a0rLQnDIkM7DLsvC+uzjPvNdFR2+59OA4mzb48YvYVR9/+SkSl+DhoWnr0nA5xi7j8oKFuriVInf7jvdRIZ7wmVLc5GhZY/OT07v9Y8sNQHInXqDMV2CmMwGhXXNEsXXtmHkPkfKPPACIilUtrwNQoEFIGOgjLb6oTwO3XZendEv9xJVnl+NDeJ7mj6cqJ25flfoHrRz935gYqIiLTTm8CRUQql9YAeaWcg57NHuk6R227IzcAu38W0LjIMuPCI62DoiIM7Tf4yfiEc/cLPvlBaFlhab3A4idgYLfhvj9KMdbl0bIyIpGB/h0exoOGhRZbgqH9HsaPgyylcSiMGhrmW+ZeHeEnYetdAZu/M/lMdtvqiNU/VyJIwUhHfCI0KkHTYouzceujsADpOsd4r6Fni09h+MgZy9o5lvnXhaRr4wqZu+4LeOLzp3AG1Tiu/niRuVdFPPb3SQ494098weJUpWrccYM1cn5qXBxXGhna752Ryj1+AtovhtFey1i3oTAKuLiyS7bJxa2NZjlKY7DzJwFhfvIYktWO4ujR4/ICx9xrImZcGNG73SPX69GwyNJ6QcTIQY/1/5k4Ziinfp6laZnl4FNHwmUAte2WG/64QGkc9j4cEOYgzBsG9xn6tp24LVTTkog5V0a0XxbRdqFl548Dtv8oICrGc1Pfdm8i1FM31zL7ioixLkMi6+ja5DO8/0h1ndp2S5Bx+AmYfUVEut6x+/6A/FA8F7VeYFn+1hJNiy2De+P5K9Po6Nvh0b/do3pmPN/96HfTDB/0SNc7LnhXiXS9o2ujR3HUMLTPY2ifN/E6Ni2xE3Nl6wURuQHDwad9jAezLo5ovcBS3Ra3I3uxGs3wIcPT/5Q8HLSL20Q1LLCMdptJISYv4WhbZWlaGs+xL1bjmXNVNKmiVdcGj933B4T5+O+A1TPcxO8qXRsHMXq3eXg+GC8OMboIZl0asf2HAT/7sxSlcUPLiohV7y7RuMRiQ8PQfkOm3rH4lpBMw5HfWSkH3Rs9Rjo8mpdbmpdaCiPQ+byPLcbPs262JVEVt7be9K0E2+6KK2kBVLXGrbdWvTskUeUIUpCociSz8fhetOunPpu+lSDb7Jh1SUTDQkvtzLha0csdesZj/VcSDOz2SFZD7SzLsjeHtK2O6N3qHT5+HfkhQ/9Oj6oWy8yLLN4JDk8bwoZvxC3uFt4Qsuj10aTxDR0w7HvYxzmYc2VE/TxHrt8w1mMY7zOkauLWX92bPDqe88kPGZyFedce3tcZWC76dhgOPBEwY01E26pjtwi0IWz8VsDwAY+lbwxpXXns7aaLc4Cb/LuVqTmTnwGiUvz/YwUMnYOhvYZsi5u2FpgiInJq9HegqVEgpAx0EJbfVCeAb74/w86fHPlqxq9v7eAbS2dNXG4v3sCbhu48M4MUEZEzQm8CRUQql9YAOZ/0bfc49KxHtsnRvMwet+rK8dgIOp7z6NnsUzPLsuD6aFJlAhvCg3+ZZM8DAek6x+A+j8E98Vko4zlaVljG+wxBGpbeGrL2F4o0LnKT9j+wy2P4oKFjnc+z/5pgtMsjVeO49n8WMAa6X4i/vd+60jK4z1AaM8y+MmLeNRGFkbhN0J4HA8ICzL4sYviQx/ovJ47ZFubFcc29JqJlRXxyMsxD10afg08d/qJHtePGPy6w8p0l1n05wcOfTk2pgsapMr7DRQq0iJwxJg6r2JDTalnVsMAyY22ELcUhjBcDKsdS1WIZ6zm9M/F+ytG4yFIYjiujvDxkNOfqkK71/rTMS4lsHGbp2+5NBEherm6uZfblEaVxGO3yGOkwjBw69eeYabI0LYlDQMUxQ5iH9ksj1n6oxHiP4Wf/J0X3xiOJkaalEbMusbjD61Df9slpklOZQxsXR8xcaymNMxHgqZ/vGNhl2PD1BEP7PWpmWRoWxpViGuZbGhZY+nZ47HkgYLQrXnuq2uLXAuDgUz77HjmyIM6/LuSyXy+y4LURwwcMw4c8so2O+/53it0/CyaNJUhDzSzHJb9UZMENEQO7DLvuD9j/qM94nyHMG2rbLbOvjJhxYdw+b8+DPj2b4302LLQ0LnSk6x09mz3yg4bqGZaOdT7PfzVBYdiw5A0hy98Skqxx7HvYZ8eP43X60l8pMfvyiL4dHiOHDGO9hmxT3M5scF8csGlbZWlbFbH+K0l6t8avTf1cx1iPoTBsMJ4jXQ9zrgqZudZi/DgI1b8zDn7Vzoyvyw8ZRg4ZRjriwFf94cpPfiIOSAztM9TNdccNSRx6xmPnTwJsCPNeE7/u+x7xwcQBsPr5Dj/h8ALwk3GIK0jFodrO9T6d632Ko9B+WcQF74rbnjkLxXEojRlKY5CshqrW+H1JS0sNYRF2PjuKn4Tadofx4rHuf8ync13ctm3mxRHVrQ4v4SiNG8ICZBsdqbqjW6KVcvDkF5I8+Q9JSjlY9LqQJW8MqW51EwHHZ25P0LPZJ5F1XPM7RS779SJeALk+Q98Oj47nvLgaV72jZqZj9hURzctOXO3pWK9n7xaPHT8O6N0c/05Xvy+kacnxA0o2isPDiTTHDIZNp6gIW74XMNZrWPG2kJoZr+zxXgxpzb8+pG72K9tHYRQ61/sks462C08cZBN5NbAR7Lg3wFmYuTY6oxXIzmf6O9DUKBBSBjoIy2+qE8C9n0jx7L8f+UbUL/54jHtunE3BGwSgLlzEeweeO2PjFBGR6ac3gSIilUtrgMgr5xwcfMpjYLfH/Osiamae2p95oiL0bvNomG9JVp/GOGx8sm7X/fHJm7UfLJHIxAGZbHNc7eHl+rZ79GzxmHtVNOnER3EUcoOG2nbH3gd9Hv9skqgEq94dsuTWkMIwREWDjWCsyzDcEbcvqZlh2X5PwKFnfOZcGXHlx4ocfCYOnsy+PKJ1VcSmbybo3xU/ZvtlEaNdhlxffHKpd6vPwad9bAj18y2tKy3zrw/pecFj54/jtkM2gro5juo2y6Y7EnRt8JhzVcSqd4U8/H+Tk07azrw44ua/zLP5O/E38PODhqgIGKidFVcB2PK9I1UvamZaLv+NIs7BtrsDDjx+jB4lL1M/z7LkDfHrUj/PsuOegIf+KkmuPx6Hn4xbOB3vxPMrVTc3PtGbHzTY0OCn4pOCL61u83KpWnfC20VEXolklTulsE51m2W061VYzsM4cNM/R1a1Wla/r8TGb8QB0HSDY8FrQwpDhtyAwU84woJhcG+8Tp0qL3DHXGOM78g2umMGtmZeHNGyPKJ7fZLurXGIDOLfZbImrnQzlapUfjJuT1jd5qhqsdjIcOhZj1zfqf9+vYQ7YevAbHPc4spPxKG1+nlxlSAXQX7YkB+M18B0ffy8O5+PAzIv17w8Dsj6ifg9XO2sOMyz+2cBXRu8iTHMvjJk7QdKZJocng9NSy0HHvfZ/N24KlSmMW7LOLTXw1nwU5DIOIIMtF4QMfeqiK4NPv27DXVzHJl6x44fB4x2xPcd2HOk8leQjkMvyao47NO8zJKqjfc/uMdjYI+HMXEAaMaauC1bssrx2N+meOqfEjgbvw+49n8UqWqzjPfEbd2allm6N3rYyDDnysmVmMa6DXsf8dl1X8DWHwQT1euyzZaZay1VrXGrNuPFobLqGZZcr0dYjCsWzb06rmg0uMew/Z74vdTca0JmXRJXHNt1fxwgC5KORTeHLLwhOmmLstI4DO6NQ3gvzin1c20cwN4fV6GadXFEWIirAo51x4Hq6rY4BF4/P/69Fkahf3vcHjNdH1dWKowYWldEOGf46R+nOPCEz6LXh7zm9wqM98XVApuXHmmjNtpl2PiNBMMdhtqZjtZVEfOvj6YUlnEODjzh07XBY/71Ec1Lj4SQxnsN+SFoWHjiAENxFLwER7WNi4pM/F4gDnk/+x8Jujb4rHpXiQU3RCcf4DkuKsI3fz7DngeOvE9vXRXxxs/kmbHm+IEv544OsJ0JL1ZNm3PFyY/5s01/B5oaBULKQAdh+U11Anjicwnu/9P0xOW3/1uOjR+4kr7E8wD4LsUv9XZheBW+uRcRkWPSm0ARkcqlNUBEzlVR6Ug5ehtB71aP/JDBmPib+N5JMh1D+w3rv5KgutWx6r2lSaXrx3sNHes8erd54OJvY7ettsy6OKI4ClHJUNV69B/0cwOw4esJMLDy7SHGxN9iHOmMKx/UzLTMWGPp3+nRv8Ojdnb8DfyujR5D+z0aF1pGuwzrvpSM21NUOermWurmOpoWW1a+o0Tb6vgP3s5BcSw+YeAn4hZOu38Wn61oXWUJkvG3tRsXW2rbHd0bPTrX+5Ry0LfNY+ePkwwf4vC3sC2puvib66XxuJ3HWPeRv+sEacfKd5ZY+faQkUOGp/4pSfcmn2S146rfLnLhz5XI9Rue+2LcvmOk48R/E6qZaVn7oRL92z1GOuMToz0vHDnTkm5wLL4p5NCzHv07jn0GJkjHlQ9Wv6/EgtdG3PeHKQb3xu036uY6tt4V0LneO+3KNFWtlpqZjoHdHn7STQRxXpSqcxSGTu0x/KQjKh59n2SVo3V1RO0sR992j96t3jG3m6pUXdxi68WTnH7KMe81Eem6uE3NyVrunCtSNY4FN4aM9xr2PeqfkRP7IiLnKi9wtF8ekal39O/06N16enN/dZsFA6Odk9f6IOOOGSbKNFnWfqBEw0JL10YfHBNhDgy8cEfAgadOPnenauKg7TFDaMaRaYiDNK9kDahqtSx7U0hYhM3fSRzV4rFurmXR60KKY4bCcNy6sjBkKOXiNpgr3hYy2hVXYep63p8Y08IbIlpWRnSu99n7cPwcW1ZEXPj+Ek1LLAO7PfY94lPKGTINjq4N8e/HTznmXhUxY02E8eOKMv07fDKNlnnXxr/L7fcEk38HxtF6gaU4aphxYRy+zjbF1XmKo4YgE79vyvUbOp6LKw72bPaomelY+sZSHBbf7FN9+L3qwG6PwT0vBpfi0EzzMksi60jVOFousAzt89j+w7it3OwrI2ZdHJHIQiLtwIOO53w613ukauNqHrWzLYksFIYM+x/zee6LCfp3etTPsyy4IaJhgY2DZq2OqlZLpsHxwncSPHN7gqgYh+eOJUg7bv1MnpXvCBnYZdj/hE8iE7ex2/ydBJ3Pe7RfFrHqPSVq2x3JKkf9XEemKX4v37vN49l/jVvDGQ8wcfCmqtXSfqll7tUhDQtcXJXnyXjfs6+ISNU4wnx8HKz7UoIn/yGJDQ11cy3v+I8cbasspRzsui9gvM/QvCz+DLDlzgAvAavfV2LpreGkzyzOQWEEklWc0Yo9+jvQ1CgQUgY6CMtvqhPAlu8FfPcjmYnLN/5JnoFPvIu9qbsmrvtA73ayru3MDFRERKad3gSKiFQurQEiIq9OpVz8B/gz9Y3GF9sFDAwde/7PDRypOlLd5giOfDcI52Bwt6F6piORmXw/5+IKNMMHDA0L4jBF7zaP/p0eIwcNXgJWvac06dvKAPse89n87YCqVselv1YkXRu/Bo9+JsnWHwQ0L7Vc9KE4EOMlHKnak782xTHoWBdXquneFLefWP2+Es3LbNw2o92SqoaOdR5bvx/QsMCx4IaQ7k0efTu8+ETOisnfOC2MwL5HfcKcof3yOLwx2mXY+oOAHffGJ0UaF8XtDcb7DFUtjpaVcXuLznUesy6NuP6TRXb+2GfvwwGtF0QsvTWkdrY7KsQUlaB/p0fPC3FFHy+AOVdEh9tL+AwfNIx2e4S5eFtbMjgHsy6JuOxXitTPd4SF+IRMcQzaVtn4BNxh+aE4/OQlYPf9QXwia6NHVDh8AijjmHFhxPK3hrQstxRGDMMH4lYZXc/7DO4z1M+NTzQGaYcNIdMYB2S23xOQ6zdc/OESV/5WMX4Omz2GDx1us9Fu2f+Yz/YfBpRyhnSdI1XnDrfacOQHYc+D8TfXje9Y/uaQlpWWMBd/i7wwbGhaapl1ScTCG8OJQNfAbsPGbyYYPhB/cz8qxifCimMw1h0fhy9a+qYSK98RxsfmofhbvQef8icCNMaPT0rVznYse3OJ1e8p4axhx098tt2VoDACLcstXgIG9xoSmfh3373RY98jcWUjOH4Vh1StY+ZFEWM9kwNRftLRttqy4IYQPwF7H/YpjRuqWizZlrjqU64/bmVT1WaZ95qIujmW3fcHPPWPyZNW7Kiba8n1meNuF2QciYybqHb0ouoZlsU3h2y7O2C89+RfBJxqWOp4r89U+Sk3ccyKiEjlalkR0bvVm3LrvFRtHLzueeHk92lYaBnYdWpfgp99eUjvdp/8wIkrILVf9mLVnbgdaXEsrvzTMD+u1uMl4IJ3lrjs10qn9Pgnor8DTY0CIWWgg7D8pjoBHHrW40tvOPK1mUs+UqTq7/47G7P/OHHd2wbuozW87MwMVEREpp3eBIqIVC6tASIilUnzvxyLDWFwn8Hz4xZN5iwWAM71w6FnfVpWWmpnTc+f0ft2GA48HtC0NGL25UeXly+OwfBBj0y9m2gX8Uo5F7+eXhC3aHj23xN0bfSpbrXMuTJi8RtC0rWHH3c0flwbxlV9Xt4qYKryw9D1vE+yylE90zHaEX+jemBPHJBZeGPIzLUW52C8xzCw22Ngt2Fgj0eq2jH/tRGtF8SBppFOQ9fzHkEa6uZY6ufFx0OYh60/CHj+a4e/2T3f0rbaMrDTozgOl36kxLLbwolvOfuJONj11D8lsWH8reoFr41YcENI68r4d9Cz2ePQMz79OzxGuw2NiywLbwwpjcUBpP7dHvmB+PqqFkfPVg8/4VhyS0TrBRE77g0Y3BeHo7ItjsWvD9n3mM+2uxOEeSZaSlQ1O4YPxWGamhmWZA288O2A7k0+868LWfXe+GTbi2GuTIPDRYberR57H/EZ7TI4C+m6w8EvzzHa4YF3pOJB9UxLfsCw+c4EHc8e/wDyk3GIyvhQN9vRuMiy+JaQZJVjz4MBxnPMvy4iSEPHc3EIKgrjtm1REXJ9hvyQobY9/jb/jDURxVHD459NxgG4prgaQKoWElVxeO/gU/5EKMoLYOZqqF9YopSLvyFvQ0Mi42haaln4upD8oKF7k3+40hAksnHFrPF+w1iXYazbMNZzpGpSusHRfknEJR8pMu81Efse8Rnc6zHeH9+eyDpaV8QVDrbdHbDpWwHj/YaoGL/etbMtbRdYWlZGFMcMXRs8dt0XTHxrvzBiTnhi9aXq5lou/sUic6+J2HlfwPYfBfRuOX7VpaalcQu9znX+UdUhjqeq1ZLIQFiAsGAojpy4TdxLQ1JNSyOal1m2fj9xwscwflzBYLrbz70oyDiWvTGkMGrY97B/Si2gRM62TKM9Krx4Pvvg3WO0X3r81jinQp8DpkaBkDLQQVh+U50AxnoMn73gSHPjRa8PWfadv+bx6k9OXPe64f9gUeEdZ2agIiIy7fQmUESkcmkNEBGpTJr/RaScwnxcgt9Pnu2RlIezsPWugH2P+sy5MmL5m0NK4zB8yKOqxZKuj6scOTf9laCc5ZhhLufilmHFUVhxXRXJ7OmvAc5Crt9gI47ZSm26jffFQZRSLn7d4upCcSCnb0dcUamq1TH3muioQFdUgpGDBjxwEQzt88gPG2aujaibE5+qK4zAhm8kGNgVh2qKY9Dzgk+q1rHi7SELbwjJDxkSVY5U9eT9F0dh988C+nfGLeHaL4sY2OWR6zfMWBPRsNDRvzNuNTJjrcXz4ypNwwcN1TMcpbE4mBOV4vYh1TPiIJYXxFW5nvtiMm5XkXU0LbEsf0vIrIsjHvu7JAef8mlbbWlaGrH/sRfbU0SUxg3b7goY7YoPCOM5ZqyxzHtNyNxrImZfEU1UWHIOCsPx+R9n4+DR8EHDeI9H5nBlog3fCOjb5pGshurWuF1Xut6x9fvxY9bPdTQvsyy6KSTXb3jm3xLse/gkPfyA+nmWOVdH1M+1JGscthRXSyiMGGpnx1W9tv8owJZg2ZtDFr0+xPPjilG9Wz2GD8at7jKNjrZVcbWF3KAhXesY7Tbs+mlAVIgrLV3+34rsui+ge5PHjDWWqMRRY1x0c8jq95QojcOz/56k47mjw12JbHzMvDxAVD/fMu/akBfumNx6pmaWJVXr6N1ygqShccy40DLWbY5qv9eyImJov0dxdHLbutlXROy898j4g3TcxmQqatstwwdf0iLwZW1/ktUOP+UoDJuJ6lnTrWaWZaTj1Fr9LHxdyLu+nGP4gOGOD2Xo2Xzi9KaXcGds/OX0/jvHmXtVNC370ueAqVEgpAx0EJbfVCcA5+Bvl1RPlA1tWGC56bmv8pO6D05sc8Xon7Em99/P3GBFRGRa6U2giEjl0hogIlKZNP+LiFQurQGVw7k4kFUcNSSrHIlseR9/z4M+m76VIF3vWPDakKoWR37YMHIwrnLTusoy54ropBWp4tZovKLx5/qhd5vPjDXRUa3uIG43NrDbI5GNKyLVth85descDO01FEYMqVpHqsaRrImrH4V52H5PQNcGj6oWx5wrI9pWW4wXB4Ti9iWQqIpbjGGgc51H92aPkUMeibRj3rUR9fMt431xECjTcLg13544FJIfNDQvj2hc6IhKR9ryFUYMC66PyDQ59j4UV1iae01E4yJL53qPwX0ehaG4lVyyKn7NB/d6JKscbavilms1Mx39uwzdG30yTY45V0SMdhsGdnnUzbbUzYtDXlHx8OMeNIQFw8ghQ9cGHy9wLLopblu254GAsW5DmI8r5oQFqJ3lmHN1SGHI0P1C/FyKY3FQJ9sSV0GafUVErj++fbzHMNp9uPpQt8dolyFd71j1nrjd384fBwRpWP3e0kSbvVIO1n0pweN/n2Ssx8NPOhbfEtK40FLKGWZeFLHstpADT/h0rvcJ83GVo8E9HgO7PYb2GRJVcUuWq367SLrOgYMohN7NPnsf9tnyvYCezT6pOseSN4Rk6h39uzwwEKQcQQrS9Y5lt4XUzLD85FNpdt3v4yITv0avD5l/fdzOxk/BkltCcgOGDV9LcOBJf+Jcq59y1M+Nj7/RbsPQPg9jIFnjWP3eEtf+QXHaQndaA6ZGgZAy0EFYfqcyAXzxluxEKtL4jg8dfIjvtV0/cfvK3K/wmtG/OTMDFRGRaac3gSIilUtrgIhIZdL8LyJSubQGiMj5JMxD53qfhoVxa7GperF118mCFmM9hnSdm3KVK+cgzMXtuU50H2dhaJ/BS0DNzPK16tMaMDXTFQipnMZGct5pXHSkT5WLDNGuhZNuH/X2l3tIIiIiIiIiIiIiIiIiIlJBgjTMviI6pTAIxIGNqVTdqGqZehgE4n0msidvk2Y8qJ/vqG0vXxhEyk+/WjlnvTQQAjC2oxHfHam1NeorECIiIiIiIiIiIiIiIiIiIpVJgRA5Z708EDKw06M6mj1xeUQVQkREREREREREREREREREpEIpECLnrJcHQvp3edTYOROXS94wBTNY5lGJiIiIiIiIiIiIiIiIiIicfQqEyDmrYcHLAiE7PKqjuZOuG1WVEBERERERERERERERERERqUAKhMg5K1kNNTOPhEL6d3pUv6RCCMCof6DcwxIRERERERERERERERERETnrFAiRc1rj4iOBkLFuj/TAwkm3D/u7yz0kERERERERERERERERERGRs06BEDmnNS6c3DaGbasmXRzwt5ZxNCIiIiIiIiIiIiIiIiIiIq8OCoTIOa1h0eRASLht0aTLA8Hmcg5HRERERERERERERERERETkVUGBEDmnNS2eHAgZ2pGlOpo7cXnA34LDlXtYIiIiIiIiIiIiIiIiIiIiZ5UCIXJOa3xZhZC+7R4N4fKJy0VvkHGvs9zDEhEREREREREREREREREROasUCJFzWt1cR5A+UgGkd5tHQ7Ri0jYDvtrGiIiIiIiIiIiIiIiIiIhIZVEgRM5png+NL2kb07/Toz63ctI2A4ECISIiIiIiIiIiIiIiIiIiUlkUCJFzXsuyI4EQWzL42y+adPuAv6XcQxIRERERERERERERERERETmrFAiRc17TSwIhAMUXloAzE5dVIURERERERERERERERERERCqNAiFyzmt+WSBkYGuGGjtv4nK/vwWHK/ewREREREREREREREREREREzhoFQuSc17wsmnS5d6tHQ7hi4nLJG2bMO1TuYYmIiIiIiIiIiIiIiIiIiJw1CoTIOa9+nsNPHakA0rfNozFaMWmb/mBDuYclIiIiIiIiIiIiIiIiIiJy1igQIuc8z4emxUfaxvTt8GjIrZm0zc7Ud8o9LBERERERERERERERERERkbNGgRA5LzQvOxIIsSVDzbY3ELjsxHW7U9+jxOjZGJqIiIiIiIiIiIiIiIiIiEjZKRAi54WXBkIABjfXsKDwlonLoRljd+rOcg9LRERERERERERERERERETkrFAgRM4LLw+E9G71WJp//6TrtqW/Vs4hiYiIiIiIiIiIiIiIiIiInDUKhMh5oWlpNOly/w6PWaXrqIpmT1x3KPkgo96Bcg9NRERERERERERERERERESk7BQIkfNC/TyHF7iJy/07PQweSwrvnbRdZ+Kxcg9NRERERERERERERERERESk7BQIkfOCn4D6+UfaxvTv8HAOZhavnrRdb/B8uYcmIiIiIiIiIiIiIiIiIiJSdgqEyHmjafGRQEhxzDDaaWgKL5y0TZ8CISIiIiIiIiIiIiIiIiIiUgEUCJHzRuNLAiEAfds9sq6NTNR25LpgAw738ruKiIiIiIiIiIiIiIiIiIicVxQIkfNG05KjAyEAzdHqievyXi/jXkdZxyUiIiIiIiIiIiIiIiIiIlJuCoTIeaNx0eRASP/O+PBW2xgREREREREREREREREREak0CoTIeaPpGC1j4OhASG+woWxjEhERERERERERERERERERORsUCJHzRqYRMk1HQiFHKoSsnrSdKoSIiIiIiIiIiIiIiIiIiMj5ToEQOa+8tErI8AGP4hjURYsIXNXE9QqEiIiIiIiIiIiIiIiIiIjI+U6BEDmvNL6sbczALg+DR1O4auK6YX83RTNU7qGJiIiIiIiIiIiIiIiIiIiUjQIhcl5pelkgpG/7sdvGHEo8VLYxiYiIiIiIiIiIiIiIiIiIlJsCIXJeaVo6ORDStTE+xGcXXzfp+l2p75ZrSCIiIiIiIiIiIiIiIiIiImWnQIicV2ZcODkQ0rHOB2B28UYStnri+r3JHxKSL+vYREREREREREREREREREREykWBEDmvVLc5atuPhEK61vs4CwEZ5hbfMHF9yRvhYPL+szFEERERERERERERERERERGRM06BEDnvzFgbTfxcGDH074wP84WFt03ablfqO+UcloiIiIiIiIiIiIiIiIiISNkoECLnnZkXvaxtzHPxYT6neBOBq5q4fk/ybiKKZR2biIiIiIiIiIiIiIiIiIhIOSgQIuedmS+pEALQsc4HDreNKdwycX3JG6Yz8WhZxyYiIiIiIiIiIiIiIiIiIlIOwZnYaS6X4/bbb+euu+7iwIEDVFVVsWrVKn7hF36B66+//hXt01rLHXfcwXe/+122b9/O+Pg47e3t3HjjjXz0ox+lrq5ump+FnKtmrHlZIOQ5f+LnecVb2ZX+9sTlfcl7aC+9tlxDExERERERERERERERERERKYtprxAyPj7Ohz70IT73uc9x4MABlixZQjab5eGHH+ZXf/VX+dznPveK9vnhD3+YT33qUzz99NM0NDTQ3t7Ovn37+Ld/+zfe/va309nZOd1PRc5R6TpoXHSkbUzXRo/ocGeYOcXXY9yRw35f8p5yD09EREREREREREREREREROSMm/ZAyJ/+6Z+yfv16VqxYwY9//GO+853vcP/99/PpT3+aIAj47Gc/y6OPnlqbjj/+4z/m8ccfp7W1lW9+85vcc8893HPPPXz3u99l/vz5HDx4kD/8wz+c7qci57CZFx2pEhIVDD1b4kM97ZpoDS+buG0o2MGQt7Ps4xMRERERERERERERERERETmTpjUQsm/fPr73ve/heR7/9//+X2bOnDlx29ve9jY+8pGPAPDZz352yvt8/vnnufPOO/F9n9tvv50LL7xw4rZly5bxJ3/yJwA8+OCDdHV1TdMzkXPdSwMhAHseONIdaW7xlkm37UupSoiIiIiIiIiIiIiIiIiIiJxfpjUQcueddxJFEWvXrmXx4sVH3f7+978fgGeffZZDhw5NaZ/f+c53gDhQsmzZsqNuv+KKK/jt3/5tPvWpT+F5017wRM5R86+fHAjZdvdLAiGFlwVC1DZGRERERERERERERERERETOM9OaoFi3bh0Al1xyyTFvb2tro729HYAnn3xySvt8sb3MzTfffMzbjTH8+q//Oh/4wAdoaWk5xRHL+ap5qaVx8ZFQyKFnfEY6DQCN0SqqovaJ2zoSDzPmTS2gJCIiIiIiIiIiIiIiIiIici6Y1kDI3r17AZg7d+5xt3kxELJnz56T7i+Xy7Fv3z4AFi9ezOjoKF/96lf5rd/6LX7xF3+RT37ykzz00EOnP3A5Ly19Yzjp8vYfxlVCDIb5xTdNXG9NiQ2Zfyjr2ERERERERERERERERERERM6k4OSbTF1fXx8AjY2Nx92mvr4egIGBgZPur6OjA2stAJ2dnXzwgx88qtXMHXfcwRvf+EY+/elPk0wmX+HI5Xy09I0hj/99auLytrsDLv5wCYBVuY+yKf0vYBwAm9P/xkXjv0vKNZyVsYqIiIiIiIiIiIiIiIiIiEynaQ2E5PN5gBMGM1Kp1KRtT2RsbGzi54997GNkMhk+//nPc80115DP57n77rv5q7/6K+6++25qa2v5kz/5k9N8BpO1tNRM6/5k6qbjtW+6CeraYehgfHnfIwFVQQ3ZBmjhIlbwLjbzTQBK3ih7m/+Ta/jkaT+uiIicHq2/IiKVS2uAiEhl0vwvIlK5tAaIiFQurQHlMa0tY3zfB8AYc9xtnIsrMnjeyR+6UChM/FwsFvnyl7/M61//ejKZDA0NDfz8z/88f/iHfwjAf/3Xf7Fr167TGb6cZzwPVr3tyGUbwu6Hj1y+ik9M2v4x/ophDpZncCIiIiIiIiIiIiIiIiIiImfQtFYIyWazDA0NTQpyvFyxWASOVAo5kXQ6PfHzO97xDtrb24/a5h3veAef//znOXjwIPfffz8LFy58BSM/tp6ekWnbl0zNi0mw6XrtWy4OgMzE5W0PF2i9Mj4GA5bSXncDB5P3A1BgiDsLv8wtw/+F4fihJhEROTOmew0QEZFzh9YAEZHKpPlfRKRyaQ0QEalcWgOmZroqqExrhZCGhgYABgcHj7vNwMAAAE1NTSfdX21t7cTPK1asOOY2xhgWL14MwP79+6c6VKkQbRdEky53bZh8yF81+hd47kiLo32pe9ie+npZxiYiIiIiIiIiIiIiIiIiInKmTGsg5MXqHAcOHDjuNgcPxi055s+ff9L9tbe3T1QJebGyyLG82KommUwedxupTPXzHclqN3G5e5M/6fbGaCWXjP/BpOueqvozLJODJCIiIiIiIiIiIiIiIiIiIueSaQ2ErFmzBoB169Yd8/auri4OHToEwEUXXXTS/fm+z6pVqwBYv379cbfbvXs3AHPnzj2V4UoFMB60rjoS7hja55EfnLzNmvHfpqm0ZuLymH+AA8mflGmEIiIiIiIiIiIiIiIiIiIi029aAyFveMMbAHjyySfZtWvXUbd/9atfBeDyyy9n9uzZU9rnm9/8ZgB+9KMf0dHRcdTtDzzwALt378bzPG666aZXOnQ5j7WtspMud22cXCXEI2BN7mOTrtuc/vczPi4REREREREREREREREREZEzZVoDIfPnz+e2224jiiI+9rGPsXfv3onb7rzzTm6//XYAfv3Xf/2o++7bt4+dO3fS3d096fp3vOMdLFmyhPHxcX7lV36FHTt2TNy2ceNG/vAP/xCA97znPbS1tU3n05HzRNvqye1fujYefdjPL7yFlG2cuLwv+SPGvENnfGwiIiIiIiIiIiIiIiIiIiJnQjDdO/zUpz7Ftm3b2LZtG7feeitLly5leHiYgwcPAvDxj3+cq6+++qj7/eIv/iIHDx7k7W9/O3/5l385cX0ymeQLX/gCv/zLv8z27du57bbbWLhwIcaYiXDIVVddxSc+8YnpfipynjiqQsgGHyhNui4gzdL8+9mQ/RwAzlh+XPtB1o7/LvOKt2Iw5RquiIiIiIiIiIiIiIiIiIjIaZvWCiEADQ0NfOMb3+A3f/M3mT9/Pjt37mRgYIDLL7+cv//7v+ejH/3oKe9zzpw53HnnnXz84x9n+fLldHR00NXVxZo1a/ijP/ojbr/9drLZ7HQ/FTlPNC+zeAk3cbn7GBVCAFbkPzzpcnfiKe6tex+bMv94RscnIiIiIiIiIiIiIiIiIiIy3Yxzzp18s8rU0zNytodQcVpaaoDpf+3/7cYs3Rt9AIzv+N3dowTpo7f7Ue172Zf64aTrqqJ2fq5/Ix7+tI5JREQmO1NrgIiIvPppDRARqUya/0VEKpfWABGRyqU1YGpefJ1O17RXCBF5NWpbfaRtjIsMB585drjjdcP/ysVjf0DaNk9cN+Yf5GDip2d8jCIiIiIiIiIiIiIiIiIiItNFgRCpCHOvCidd3nZXcMztElRz6fgnuWr0LyZdvyXz5TM2NhERERERERERERERERERkemmQIhUhMW3hHjBke5I2+4KcPb42y8ovIWkrZu4vDd5FznTeyaHKCIiIiIiIiIiIiIiIiIiMm0UCJGKkGmAuddEE5dHOjwOPXv8wz8gw+LCuycuW1NiXfZvzugYRUREREREREREREREREREposCIVIxlt02uW3M1h8kTrj98vwvTLq8Ift5ns3+NQBjXgcvpP+NAX/L9A5SRERERERERERERERERERkGigQIhVjyRtCMEfaxmz9QYBzx9++OVzLvMKbJl33dNWf8VD1x/lG48U8XPPbfLf+RoVCRERERERERERERERERETkVUeBEKkY1W2O2VccaRsztM+je9OJ/wncOHw7baUrJ123OfOvhGYMgJI3yvrs303/YEVERERERERERERERERERE6DAiFSUZa9aXLbmB33BifcPkEVtw59i5bSJcfdZmfq2xTM4HQMT0REREREREREREREREREZFooECIVZfHNLwuE3HPiQAhA0tXyxqFv01S68Ji3RybH9tTXp2V8IiIiIiIiIiIiIiIiIiIi00GBEKkoDQsczcuPtI3peM5npNOc9H4p18Cbhu6kpXTRMW9/tOb36fc34XDTNlYREREREREREREREREREZFXSoEQqThLbplcJWTnSdrGvCjtmnjr4E95x8CD/GLvfhrDVZNu/1bjVfyg7o30+5snXV80wzyd/f94tOoPGPH2nt7gRUREREREREREREREREREpmBqZ8JFziNL3hDy2N+lJi5vvydg7S+UpnRfD5/mcC0AK3O/xMM1vzPp9o7kI9zRcA1LCu9hUf6dNEQruLf2/fQm1gGwMfsPLMq/E0MAONpKlzOneDO1dv40PDMREREREREREREREREREZGYAiFScWZeZKlqsYz1xAVy9jzgM9JhqJl5au1eluV/gX3JH7Mv9cNJ1zsTsi39Vbalv3rM++1M3zHx8470f2FcwBVjf0p9tJRdqW/TEK7kgtyvEpA+xWcmIiIiIiIiIiIiIiIiIiISUyBEKo7xYOmbQp77jyQAUdHw+GeT3PTnhVPaj0+SNwx/g2FvF7tS32Vd9jMUvaFTHo8zIY9Xf3LSdXtS3+Omoa+QdW2nvD8RERERERERERERERERERHvbA9A5Gy4/KNFjH+kIsi6LycY6TCvaF+1diFrc7/De/qfZlnugwQue9rj60o8yR0Nr2FT+p8Z9nYx5h0ib/oIyZ32vkVERERERERERERERERE5PynCiFSkRoWOla9O2TD1xMARAXDo59JcstfnVqVkJfKujauH/08V49+mr2pH3Eo8SBdiSdxhFw8/gfMLt7IC5l/ITR5ZhdfR8JlebT6E3Qlnjjm/nJ+F4/U/N5R19dE82kpXUxLeBEt4UU0h2tIurpXPG4RERERERERERERERERETn/GOecO/lmlamnZ+RsD6HitLTUAOV57Qd2Gf75mipcdKQyyNv/Lcey28Iz/tgvChnn/tqPsjv1XTyXJOlqyXu9p7yf+nApc4tvYH7hNlrCi/FJTtzmsIDB8MoqoIiIlEs51wAREXl10RogIlKZNP+LiFQurQEiIpVLa8DUvPg6nS5VCJGK1bDQsfp9JZ7/ypHwxA9+M03DwnFaV9qyjCEgy03DX2LY20PWthKQpTN4nMerP0l34ukp72cw2MZgsI3ns3+P79K0lC6mNbyMYX83B5I/IWUbWJn7CCvzv0zK1Z+5JyQiIiIiIiIiIiIiIiIiIq8KqhByAkollV+5E2H5YfjyrVn6tvsT17VfGvHBu8fL8vjH43AcTPyUfckfUzIjRCZPaArkvG76gucJzSsbX+CqWFB4C2PeIXqC50i7RmaUrmRF7sPMCK+a5mchInJqlAoWEalcWgNERCqT5n8RkcqlNUBEpHJpDZgaVQgRmQbpWnjnl3J88ZYqCsNxS5WDT/t0bfRoW1WeKiHHYjDMLr2O2aXXHXWbJWLQ30Zv8Cw9wTp6Es/QEzyLMycfb2jG2J7+2sTlEsOM+HvYnv46rxn5DCvzvzytz0NERERERERERERERERERM4OBUKk4jUuclz134v87M9SE9c9/9UEN/154SyO6vg8fBqjFTRGK1ha+HkAxk03e1N305F4hK7EE4z4eya2D1wVoRk76X4frvk4GzP/SGN4AUVvEM8lmFt8A8vyH8AnRYkxdqe+S9GMsrjwbtKucUrjtUQM+Tuojebjkzr5HURERERERERERERERERE5LSpZcwJqExN+Z2tEkFj3YbPr63ChnGVkHS94zefHyVIl3UY02bcdNGbWEfCVdNWupze4Hk2Zv6R3ak7iUwegEzURtEbmrh8PNloBnXRInqDDZS8YQBStoF5xTeSsa0kXJb24g20hJfg4U+6777kvTxS/XuM+HtI2UYuGv9dVuY+QkDmzDxxETmnqUyciEjl0hogIlKZNP+LiFQurQEiIpVLa8DUTFfLGAVCTkAHYfmdzQng27+YZtvdiYnLqRrHzZ/Os+LtIZ5/gjueQwpmgL5gA0lXS1O4hrzp4676N9MfbDrtfSdsDQ3RMgwe4143Oa+b0IwftV3S1rOw8HZW5X6NxmjlaT+uiJw/9CZQRKRyaQ0QEalMmv9FRCqX1gARkcqlNWBqpisQopYxIoet+UBpUiCkMGL4/m9kGDlU4MrfKp7FkU2flGtgVum6icsZ18ybBr/PvXXvoyvx5Gntu+SN0O09fdLtit4gWzL/ztb0l1hUeBczSleRcvXMLt5AyjWc1hhERERERERERERERERERCSmQIjIYQtuiKidYxne7026/ql/TnD5bxTxztN/LRnXzFsGf0zO9GBNCYNHytbRkXiU9dm/pSvxBJEpAHEVkJJ36mm9pK2j6A1Nus6ZiB3pb7Aj/Q0gbkNz6dj/YkX+l/A0NYmIiIiIiIiIiIiIiIiInBaddRU5zPPhLV/Ice8n0nRvOtIjZqzbY8+DPgtvjM7i6M4sgyHrWuElDaTmlF7PnKHX47CMe10ELk3KNRBRYGfqWxS9YerDpYz4+9ifvI9+fyPD/m4MHmnbTNa1krGtzC7eyMrcr9CZeIzNmX9jb/KHWHN0xZWCN8AjNb/HntRd3DL0NQKyZXwFRERERERERERERERERETOL8Y5506+WWVS36Lye7X0jFr/lQQ//Hh64vIF7yzx5i/kz+KIzg2WEgYfg3fcbXKmh8er/xfb018/7jbZaAYX5n6LhnAZs0uvO+H+ROT88WpZA0REpPy0BoiIVCbN/yIilUtrgIhI5dIaMDUvvk6nS2dZRY5h5dtLJKuOZKW23h1QGD2LAzpHeCROGt7IuBZuGPln3tn/KK8Z+QxXjP4fWkuXTtpm3O/k8epP8sP6d3J/za/isGdy2CIiIiIiIiIiIiIiIiIi5x0FQkSOIZGFZW8OJy6HOcO6/0hMXC6Nw6FnPUrjZ2N054emaBUr87/Mmtxv8dbB+1gz/vFjbrcj/V88VfVnZR6diIiIiIiIiIiIiIiIiMi5TYEQkeO44N2lSZfv/9M0j34mSX4QvnRrli+9oYp/urKKsR5zdgZ4HjEYLh/7Yy4e+/1j3r4u+zc8WvUJima4zCMTERERERERERERERERETk3KRAichzzromYsSaadN2Df5Hib5fW0LPZB2C00+OZf00c6+5yigyGS8c/xTv6H+aG4X8hbZsn3b4x+wX+q+FSDiTuP+q+DsuYdwhL6ajbREREREREREREREREREQqkQIhIsdhPHjnl3K0rIxOuN3GbyRwrkyDqgDN0YUsKbyXn+vbQHPpokm3jfud3F33Np7N/hURBQBKjHJ33dv5StNyvt1wPXnTdzaGLSIiIiIiIiIiIiIiIiLyqqJAiMgJ1Mx0/Pz3xpl9RXjcbYYPehx80i/jqCpDgireNHQnS3MfmHyDcTxd9X/4WuOFbMh8nseqP8nBZFw1pD/YyLPZT5+F0YqIiIiIiIiIiIiIiIiIvLooECJyEulaeOcXc9TPt8fdZtO3ghPuw1kI89M9svNfytXz2tF/4C0D91IdzZ1027jfwWPV/5Mtmf+YdP0LmX9TlRARERERERERERERERERqXgKhIhMQaYR3vWfOVJ1x+4N89wXk9z7P1Mcevbof1K7f+bz9yur+Ntl1Tz4l0mKY7DnIZ/d9/vkh870yM8PM8IrecfAA8wp3HTSba0psjHzhTKMSkRERERERERERERERETk1cs45459hlvo6Rk520OoOC0tNcCr97Uf6TRs/X7AzIsinvyHJFt/kDhqmzlXh6x+X4lFN0aMdBq+8pYspXFz7B0ax5yrIm75dIHmZcevQCIxh2N/8l7WZT5DZ/LR426XtHW8ZvRvWFB4Kz6pMo5QRE7Hq30NEBGRM0drgIhIZdL8LyJSubQGiIhULq0BU/Pi63S6FAg5AR2E5XcuTQA77/P55s9lp21/tbMtYR6allpu/osCLSsUEDkeh2Nn6g4erf598l4vgcviuxQFb2DSdsYF1EdLydo2MraVlvAiZpaupilcg8FM7OvFn4/3WEP+DqqimSSoPqPPS6TSnUtrgIiITC+tASIilUnzv4hI5dIaICJSubQGTI0CIWWgg7D8zrUJ4MkvJHj235MM7pne7kt+ynHN7xZZ/b4SNTP0T/R4imaYjsQjNIYryXt93Fl/M9YUT3q/ltIltJWuYHfqThyWi8d/nxX5XzoqGFI0w9xb+34OJR+kKprFG4e+S0O0/Ew9HZGKd66tASIiMn20BoiIVCbN/yIilUtrgIhI5dIaMDUKhJSBDsLyO1cngPFew4avBzz1z0lGO48Oh3gJx9JbQ/Y8GFDKwZJbQqraHM9/JXH8djKHZZstqVpI1zvStY50vSPT4GheZmlZaamZaame4QjUGYUDift5tPr3GQy2nvJ9M7aVxvACfJci5RqosjNZl/1/k7ZpCFfy9oH7CchM15ABGPUOsif5A1rCtbSFV0zrvkXOJefqGiAiIqdPa4CISGXS/C8iUrm0BoiIVC6tAVOjQEgZ6CAsv3N9ArAhHHjKZ+e9Ad2bPEa7DH4Srv2DAoteFxEVwfjg+fH2nes9/uvnMoz3nl6FES/hWHxTyAXvCqmba6mbbck0TsMTOgdZIvYkv8/e1A/pDzYy5O8kNOPTtv/W0mVcO/K3+KSwRNRG808rIJI3fdzR8BrG/IPgDLcNfZ9Zpeumbbwi55JzfQ0QEZFXTmuAiEhl0vwvIlK5tAaIiFQurQFTo0BIGeggLL9KnADGug0b/ysg0+yYdZHlJ/8rxZ4Hg9PbqXHMviKiqsXR84KPl3A0LbbMvjLiwp8rkaoB5wAX///FgMr5KmScQX8Hh5IPsiHzOcb8Q9O2b+M86qIlNIWrqY3mU/AGcTjqo8XURgsJXIbqaA51dtEx7/+z6t9gW+Y/Jy63F1/Lm4a+N23jEzmXVOIaICIiMa0BIiKVSfO/iEjl0hogIlK5tAZMjQIhZaCDsPw0AcQBja6NHlu+G7D/8YDcAOQHDfkhgy2duL3MVBjPUT3DMdZtsKHBeI6Fr4t4yz/mSE3PvPKqFpJnV+oOCt4g8wpvYmfqDjZkP0fe6zujj9tefC0Xjf8PZpauweDhcGxPfZWf1f76Udu+p/8Z6qMlZ3Q8Iq9GWgNERCqX1gARkcqk+V9EpHJpDRARqVxaA6ZGgZAy0EFYfpoAjs85CHOQHzYMHzB0b/QZ2O0x0mnY9dOAwtDphUXmXRvynq/l8BKQ64v3lW2ePD1EJSjlIF17Wg/1qmOJKJlhfJcmMgVyXg+9wXoG/S00hhcwp3gTP6i/jZ7Es6f9WGnbTMa2Murto+SNHnOb1eP/javG/uK0H0vkXKM1QESkcmkNEBGpTJr/RUQql9YAEZHKpTVgaqYrEHKafSlEpFyMgUQWEllHzQxH+6V24rZSDnb+OKBvu8dol2H3/QGDez0g3j4Ok5w4MLL3oYC/nl2Dn3JEhXjbGWsirvndAvOujXjmX5M89rdJoiJc8sslXvu/C3hTmEF6t3l0POeRaXTMuNBS3fbqy6B5+KRcAwCBy5CK6o+q0PHWwfvoDp6kK/EUg/5WDAHWFOn3X2AgeIHIFKb0WHmvl7zXe8Jttqa/Qk00n9bwUprC1fgkX9kTExEREREREREREREREZGKpQohJ6BUUvkpETY9nIPB3QYbGRoWWoyBg097PP65JHt+FoCBpsWWns0eNnzllUVmXhyRyDraLrC0XRjRtsoy1m3ofN5naL+hc71Px3P+pPvMvy7klr/O07Dg/Jl6LCGD/jZyXg8p24AzIQP+Zsa9HgpeP1vTXz5xSxpnMBicsUfd5LsUDeEKGqLlVNl2qqJZzCxdTUO0EsPptxASeTXRGiAiUrm0BoiIVCbN/yIilUtrgIhI5dIaMDVqGVMGOgjLTxPAmecOZw6MFwdH7vkfKdZ9qbwVKBJZx9W/U2TtB4rkhwxhwdC8LA6unI9KjLE9/Q32Je+hI/EIzkTURHOoixbTHK5lbvFm8maAu+vfOuV9pm0T1dEcqu1cZhdfy6zSdWRsKyUzxqh3gGo7i2o75ww+K5HppzVARKRyaQ0QEalMmv9FRCqX1gARkcqlNWBqFAgpAx2E5acJoPxsBA/8nxTbfhhgPEeyGlI1jp7NHuO9XtnGMf/6kLf9S450fdke8lXnqeyfsiH7D4RmfFr2N6f4epbmP0DaNhKaHCUzQpVtpym8kKSbnkVEZDppDRARqVxaA0REKpPmfxGRyqU1QESkcmkNmBoFQspAB2H5aQJ49YiKsPm7AXsfCejZ7OEiuORXiox2eDz818m41Yxx4E5c1qNxkWXlO0sURwzPfTFBafz42zctiXj3V3LUz6/caclSoj/YRFfwFN2Jp+gJnmPI34Ez0fQ9iDPUR0toDi+iJVxLc7iWmmguGduKT2r6HkfkFGkNEBGpXFoDREQqk+Z/EZHKpTVARKRyaQ2YGgVCykAHYflpAjg3jPcaCiNQ2+4ojBi6Nnh0Pe/Tu80jVeuYeVFE83JL7SxHtunIFDOw2/Cj30uz96HguPuubrO8744czUttOZ7KOSEkz6i/n3Gvi75gPQcSP2Ug2ErO6yIyhWl9rJStJ+nqKZlRHBFp20jaNZO2zaRtExnXTHU0l5mla6iPlmKIAz4FM8iu1HfpSDxMY7iSFflfIuXqp3Vscv7TGiAiUrm0BoiIVCbN/yIilUtrgIhI5dIaMDUKhJSBDsLy0wRQGTrWeTz3xQR923z6tnvkB4+uGnLBu0vMvzZk5TtD/MRZGOQ5wGHpCZ7jQPInDPt7yJs+PJKAoyPxEAVv8Iw+ftLW0xAtpWhGGfS34Uw4cVvC1nJB7le5IP8RquwsimaIzsQTjHsdzC6+nmrbfkbHJucmrQEiIpVLa4CISGXS/C8iUrm0BoiIVC6tAVOjQEgZ6CAsP00AlWnL9wK+/xtpouLRwZD2yyLe+41xktVnYWDnsBKj7Evdy5h3iKIZJnAZApdhMNhKT7CO/mDjtFcXORbjfAKXpWRG4xZDgO9SzCy9hrRtomRGydoZVNlZVEXtVNlZVNt2krYeD5+Ua8DgAeBwWEo4QgKyZ3zsUn5aA0REKpfWABGRyqT5X0SkcmkNEBGpXFoDpma6AiHH79sgIlImy98SkqzO8e1fzBDmJ4dCDj7l860PZnj3V3IklAGYsgTVLCq847i3W0r0+5vpTTzHgL+Fca+Lca+LnNdF0YyQcNUYPPJeHwXTjzOvrIWPMxElM3lBj0yBA8n7pnT/lG1gSf595Lxu9iXvoeSNAlAXLmJF/peZUbqKhKumLlqIh0rJiIiIiIiIiIiIiIiIiLxIFUJOQKmk8lMirLLte8zn3k+k6N3iH3Vb4+KI2z6bZ9YlryyYIK+cw1IwA+S9Psa9bnqCZ+hIPMpA8AIj/l58l6I+Wkpr6XLai69le/rr7E3dVdYxJmwtM0vX4AgJTZ7GaAWeS7An9QNyXi9N4SpmFa9lVuk62kqXE5BlyN/BocTD+C7BnOLNZFzLpH2WGKUr8RQlM4LBoyW8hCo784TjsIQcSN5HSI65xVsIyJzJp33e0RogIlK5tAaIiFQmzf8iIpVLa4CISOXSGjA1ahlTBjoIy08TgDgHuX7DA3+eZP2Xk5NuM57jdX9W4NJfKZ2l0cnLRRQwBHhMDvEM+tt5IX07HYlHAUeCKmqi+Yz4e+hMPHZ2BvsSSVtH0RuauGxcHPhoCJeTdk3kTR+7Ut+ZqEgSb+Mzu3QDo94hcl4XjeEFtBdvoC28nPpwKcP+Hp6o/l90JZ4EoD5cyo0jt9Mcri330ztnaQ0QEalcWgNERCqT5n8RkcqlNUBEpHJpDZgaBULKQAdh+WkCkBc5Bw/+eZLH/i511G2X/lqRaz9RIFUdX956V8CmbwXMvy7i4g8rLPJqN+LtY9DfSpVtJ2vbGPc6GfUPMOYdYsw7yKh3iNCMUfAGOZR4EGciABK2htbwEvJmgL7E+rP8LKYmDppcTFO4Bt+l8EnhuyR10SLaizeScFkK3iBZ2zbR8sbhyHldlMwoNdF8vCl2d9uT/AHPZP+SjGvhmtG/pi5afCaf2hmhNUBEpHJpDRARqUya/0VEKpfWABGRyqU1YGoUCCkDHYTlpwlAXm77j3x+9Htpxrq9SdcnqxwX/nyJluWWH/5OeuL6N302x+r3huUeppwho94B9qS+T9o2M6/wRhJUAdAdPMX+5H0UzTBj3kEOJO+n6A0edX/jPKrsbEb9fWUe+akJXBVtpcsomEGG/B0TlUkytoUFhbdQHy6nys4ka2dQZWeSti0cSP6UbemvwuEAyYuVSQBStoErxv6MWcVrqbZzj6rg8mqlNUBEpHJpDRARqUya/0VEKpfWABGRyqU1YGoUCCkDHYTlpwlAjmW81/CtD2Y49MzJT2rXzLT86mNjJLJlGJi8alhCxr1Oki6eQ7qDZyiZMdrCy8jaGYx5hziUeIiOxCMM+TvJeV2kXD1zijdTMIPsTd3FsLcHzJEl0XNJFhTeQmt4KcPebralv0rJG8E4n7RrIud1H3MsgasicGnyXl85nvpJZWwLS/MfoCm8gIgS1badrJ1Jf7CJnOmmMVpJS+liSmaMUX8/Q/5ODD6ziteRda1EFAhNDs8lCMhiMGdsrFoDREQql9YAEZHKpPlfRKRyaQ0QEalcWgOmRoGQMtBBWH6aAOR4SuPwg4+l2fr9xEm3ve6TBa7+7WIZRiXnk5Bxhv09hGYMi6UhWkrKNbzk9hyj/gGydgYJV82wv5Ou4Am6E88y7nXhuwQ1dh7Lcx8icFU8n/179iV/xGCw7Sw+q1fOOI9qO4dRbz/OWCAOlyzKv5OUa6Av2ETSVdMQrqQuWkx1NJsaO4ekqydnesh7vdRG8wmYejpLa4CISOXSGiAiUpk0/4uIVC6tASIilUtrwNQoEFIGOgjLTxOAnIhzcOAJn2f/PcHm7xw/GJLIOl73ZwVWvr1EsrqMAxQ5hnHTTd7rw5oCEQUK3hAdiUfoDp7GJ0ngMnQlniDn9WBcQG20gLpoMY6QA8n7cWbqLZAaw1XkvC5yXs8ZfEYn5rkE1pQO/5ykJbyYjG0h6WqpD5dQZdspmEHyXh95rw/jPGrsPJKujnRNiMORG4GaaA4t4cXsS97LoeQDeC5JfbSUjG3GdynGvW6K3iBZO4v6cDG10SIyruW0Kpg4HAXTT8o1YPBOfgcREZk2+hwgIlKZNP+LiFQurQEiIpVLa8DUKBBSBjoIy08TgEzV9nt87vpYhvygwUs4bOnYJ4GT1Y7WCyJmrLXMuDBi1iURjQunPu11bfTY8I0Esy6OWPn2qZ+YFzkVLwYRkq4Oj2Di+oIZoC/YyLjXwbjXxZjXcfjnThKulsWFd5K2LRxI3kfS1bEq91EsRbanv86It5eBYAsHkz87e0+szBK2lvpoETXRfJKuloSrPvxfDUlXQ8o2kLUzMUDe68d3STK2DYelP9jI89nP0R9spDZawHUjn2VW6bpTHkOfv5Ed6W8SuDRL8j9HrZ0/7c9TROR8pM8BIiKVSfO/iEjl0hogIlK5tAZMjQIhZaCDsPw0AcipGO8z7LrPp221xXjwpTdkKY6dvDrAjLURMy6M2P+YDwYu+9USF76/hHPg+WAOFwbY86DPN9+fISrG+7zlr/Nc9KHSGXkuuX5I1x95bJHpMuBv4WDiAQA8Egz7uxj3OqmLFlFlZ9EZPMGov5+UayBjW6iLFjLqHWRP6gcUzWBcecO2UjKjdCYeO6WKJec0Z6iLFhOZApHJEVEicBl8UljillQp10jaNpK2TYBhxN9NT+K5iV0Y59EaXk7WzqAuWkRL6WLG/AP0+1tIuhqq7EwKZoiiGaY+WkxDtIK86aPgDZGy9aRdExnbTMJV43BEJkfBDOKRoCaaR8rVY4nYlv5PtqW+RpWdxUXjv0tjdMFxn5alxNb0f9IfbGJ+4TbaS689+UuBZcDfQk/wHLV2PjNL15zuqysichR9DhARqUya/0VEKpfWABGRyqU1YGoUCCkDHYTlpwlATkfnBo97/keajmf9V7yPRNbRtirCWcPBpyfvJ1nl+MjDY9S2T9+06Sx8/zfSvPDtBA0LLB/4/jhVrZqW5dUpZ3o4mHwAg6GldDElM8ZA8AIj3n5G/f2M+vvImV6ydgZJV0dX4glG/D1ne9jnrZStJ3BVjPkHJ64zzmNW6XqydgY5r5txr4usbaMuXIzBZ2/q7km/k7pwEdV2LuBoCFdg8OhJPIsjojFcRd7rpyPxEHmvb+I+i/Pv5fKxPyIyeYpmmJIZJ2XryLgW0rZ5UpWbFxXNMEUzRNbOwuCRN704LBnXOqnNT4kxDD4B6Sm9BhEF+oLnKZoRmsM1pF3Tqb+QL+Nwp9V6SEReGX0OEBGpTJr/RUQql9YAEZHKpTVgahQIKQMdhOWnCUBOl7Ow5fsBu+4LGO009O/yGNo3vWU3rv2DAn4AXtLRtNjSttpS3fbKptL1X0nww48fOfF58YeL3PzpwnQNVeSsC8kRmnHGvW4Ggs0UzAAp1zBRWSMyBYb9PUQUaKptwuDRN9xDd+Jp+oJNVEUzWVh4BzV2DsP+bopmmNDkyNhmkq6OUW8/g/4Ohv2dDPk7GPb3Vk4Vk1eplG0ka1tJ2xYytoWc1324ukyE7zIELkXBGwQgYWuojxZTGy1k0N9OX+L5iX2AJTQ5qqO5tIaXkrEthCZHb7COIX8nACUzijVxxRbjPFrCS5hbvJlZxeupsfNI2XrAMejvoDexntCMElHkYPJ+OhOPUx8u5bKxP2RO6fUMeTt5tPoTHEo+xMzS1Vw++ic0RxfGLYX8TeS8HhqjlWTtjGl9vYpmhBFvD1V21pQDLQ7LoL+djG2elhDMK2EJGfZ3UxstOGYISORU6XOAiEhl0vwvIlK5tAaIiFQurQFTo0BIGeggLD9NAHIm5Aagc73PoWd9Nv5XgoFd09+XparF0rTU4hzYoiEKIZFxtK6yzL48YskbQoLU5Pvkh+AfL68mPzD5m+i/t2+EYGpfjhc5r0zHGmApkfN6KJlRimaEkhk9/PMwea+Pca8T4HAYJU/O68a4gJSrozW8jLbS5TxV9X/Ym7yLyBQIXBbfpfFIEJpxLEU8koAjbwYoecOTHr82WsDS/AcommF2pP6Lcb/jFT8XKZ9M1EbO75p8pTPU2HmUzMikCik10Xwaw5UAdAfPTNyWcNVU29lUR3OotrPxXILQxIGoyOQJGafojTDq7aPgDdFcWkNdtJgd6f8iMvl4H7YWZ0KSto7FhfewuPBOBv1tDPhbGPJ3EZkCHgGdwePk/C6MC1hSeA8rcr9EU7ganxQReUpmnMjkcFhyXjcdiUcpmTFmlK6kvXQDAEUzSMEMYvCptrMxTH1tHPb2cE/dexkINlMfLuPm4a9SHy05nV+BiD4HiIhUKM3/IiKVS2uAiEjl0howNQqElIEOwvLTBCBnmrPQud6jMGyYsSZi988C1n05wWiXIdvkGDnkMbj3yEmxbLNl1qURO36UOK3HrZllufzXiyy4ISLb5Bg+YHj0M0m23X30ft/6zzlWvE0VDqTynItrQETxcCDAkbEteCSOuj3nddETPEd/8AJp20RLeDGRKZAzXSRdHT5p+vznGfX3k7GtpG0TBW+QvNdH3vRSMmMYDJ5LkXL1hCbHiL+HEW8PY34HddEiLsh9lD7/ebanv07RG5p4fN+liMzkqkOZqI0lhfewNf2fFLyBYz8xZ8DEbxGTto4ZpatJuXq2p782ra+fTKOX/M5OxHcpDAGhGZu4LnBZGsLlNIQrSbn6uMWPN0TRDBG4KqrsDCwRRTNMwRvgYPL+o/bbGK4i4arJ2jaSto6814s1Raqi2dTYudREc6mO5lFj55C0dTgTYjn8nylhKZLzesl5PUQmz5jXwd7kXQwEW6iKZjOjdCUzwqtoK11BlZ2JwSNk/PC/Dw/fpQioYsw7SGfiMUKTpyqaQY2df8wqJiXG6A82ApBwtdRHi4/69yvldS6uASIicvo0/4uIVC6tASIilUtrwNQoEFIGOgjLTxOAvBoURiE/aIgKUNvuMD5876Nptn6/PCeK5l8f8r5v5sryWCKvJloDTp/DUTRD5L1e0raJlGsgb/oOt9sZwSdJS+kSAtI4LGNeBylbT2QK9AebsIQ0hxfiuxQDwRZ8l6IhWomHD8ChxENsT32dojdM0taSdLUELkPBG4xP5ptucl43Oa+X4uG2MAA10TyawtUM+tuJTIH6cCkGnyF/O8P+bpyJAGgKV5OyDYx5HXgEeAQM+Fsn2sIc2d98ApfBcwmaolUkbA0Hkj9lKNhettdazh7jPDxSRCb3suuDY7aM8lySKjuLlG0g5eoAQ2fisYnKLAAp28D8wm14JCmZETwSExVWHBF10WJCk+NQ4iEMhjnFm6iNFtAdPEPJjGDwMfh4BGRsCzXRfHyXAhxZ20bWzmTc62DUP0hocoClOppDbbSQ2mghGEtn8Dhj/qGJ8cwqvYbQ5OlIPDIxhvpoCSnbyJ7U9zmQ/CkAGdvKzNLVzCnedMxKLyVG2Ze6l1HvAFnbRk00j8ZoBUlXd8LXOWfi9l0JV01L6WISVAFgiSbaf3n4WEoUzTBJV4fBI2d68UmScvVH7TNknP3Jn0zMRQVvgEF/Gw3RchY3XgRoDRARqTT6DCAiUrm0BoiIVC6tAVOjQEgZ6CAsP00A8mo23mcYPmQY7zE4B4VhQ/dGj87nfbo2euT64pMwXuDwEhDmzEn2eHztl0UseG1Iw0JL40JLw0JL+sTnbUTOeVoDzi8RBXJeL2CpsrMxHHtOjCgy6u0n6erIuOZj7mfI3zlxEr0uWkzKNRxzX8Pebg4kf8qgv40x/xAlMwpwuDLLRVRF7YCj1i6gPlzGxswX2Jm+g3GvC0uJ9uJrWZP7bXamvs2B5H3kTR/OhLSULqEuWkxP8Cx9wQZKXnyMpmwDddFiDIa8GWDU3z8pZHAs2SiubjHmHwTAOJ/mcC0JV03O6yE044z4e064D88laStdRnfi6aMqwEgFOE41mKStI+UaGPc644opLkvCZRnzOwjN+FHbZ6I20q6RtG0k5RoIXObwLYaCGeRA8qcTARvjfKrtHFK2gcFgG6EZw3dpsraNUe/gpO1eDHjNKF3F3MKtAHgEjHtdx2+l5QwXmPcxn9cxNDJKbTSf5nANvksTmQJ5r4+Sif/d5U0/A8FmLBEzSlfSEC1jzOvAmiJJW4vBJzJF0raJtGuMd48jZIyCN0TGNuOTOny9nQix7El9n0F/Gz4p0raZucWbqY+WAlA0I3QkHsYSMrN0NWnXREieZ7N/xZbMf1AVzWJF/sM0h2vJe33Uh0uptfNf2e9XRKTC6DOAiEjl0hogIlK5tAZMjQIhZaCDsPw0Aci5yjmwJfACMIe/nJsfhH2PBjz+2SSHnvGPeT/jOVa+IyRd73jm9uQJHyPTZGlaZJlzVcScqyJq2x01syyp6VkPRM46rQFyLnA4xrxDOCKq7ZxJQReHI2/6GPMPAOC7DIHLELhs/H8yGDwcju7gKQaCrcwqXnvUieM+fyPPZ/+evOmnNbyE1tKl1EVLSLpaIpMnZRsJSDNuutmdupPeYD1DwXaM8wjIELgqfJeKq6y4JPXRUopmiL3JHzIU7KAqaqcuWkTaNVIyYwz4mxnyd+CMnfLrkLC1XDb2KdZlP3PsE/zTJG2byZu+KbXDEXm5OIDlk/d6J8JaxvlU2XZKZpiCN4jv0liKxzz+68NlpFwDvcH6iYo0xnnUR0sZCLac8LFnFq9hVul6MraZQX8bo/4hUraelKvDUsIjQUO4gtCMsz/5Y0pmnJbwYprD1QQue/jfcIIhfyf9wSZ8l6Y2mk+NnU9NNJeIIgVvgIIZmPh/aMZJuUYiCnQkHqHg9dMQrqAhWgFA0tXQGK6iLloUV3cyg4z6B/Bdkmo7B98liUyBhKvGI0FInnGvE0dEX7CBTZl/oi/YyKzS9azK/Rr14TKSrhqfzHFDfyIiJ6PPACIilUtrgIhI5dIaMDUKhJSBDsLy0wQg5yPnoOM5j/2P+hx6zseWINPoaF5uWfHWkJqZjoHdhtuvqyIqnOIf041jzpURi28KaV1lGe81HHzGJ9PguPRXimSO/SV6kVclrQEiZ09IniF/Rxw4cfUkbR0JV0PJjDDmdeCTIulqGPe6yHk9NJfWknWtlBhjKNhOxraRsS3kvG6KZoSMbcbgM+odYMTfx6i3jxE//i8ih0cCQ4Dngon2QCnbQMa2kXBV+KRpDi+kKbyQkhmmM/EEnYnHGPC3kPf6CMmRdo0kXS0OR2TGyZsBEq6KGaWrqLazGfMOMehvYyDYSt70UvAGJ0IBadvE3MKtZFwLQ/529iXvPao10bH4Lo0jwprSmf6ViJxVCVszUQ3ppJwh7RqpixZRFc3GwydwWdK2BWdCxr1Oxr0u8l4vKdtAY7iSgCpCcox7neS9XjK2lWo7h4g8414Xg8FWcl4PKVtPxraQds0Y5zPud2CcT2t4GRnbwqC/nYLXT8g4addEc7iGmmg+CVdFyYyR93rJmz6K3shE5RnjfPJeP0P+dgreILXRQurDJSRcDY6IMb8DS4mMbSHp4vcmI/4++v0XiEwhrsBDGt+l4uAfaerCxdTahTgi8l4vSVt/uD2boy94nl3JOxn3O2gIl9NWupLmcA27Ut9hffYzWEJW5D/MytxHCMgc+yXGsjP1bfYnf0xtNJ9ZpetpLV2Kz4kD5SLnAn0GEBGpXFoDREQql9aAqVEgpAx0EJafJgCpZPse9XnqHxPseyygMHT637LMNlvWfrBE3TzL0D6P4QMete2WeddFzLo4InHsvzeLnDVaA0TkTAvJE5oxUq4BgzdxfcEMMuhvI+GqSLpaLHELlIxtwZmIfn8LYGkO1xKacQ4k7yM0BVpLF1MTzcMZiyMiosSYf5BRbz+OCGcso95Bcl4XGdtKTTQ33r+JGPH2MOTvYtjfSWSKcXWI0oWAx0DwAp2JxzF4zCpeS9o1M+hvZ8jfwah3gFo7n2X5n6cuWkxX8CQbM/9Eb7COhKuiNlqIMxElM0ZoxjAuYHbpBmYXX0fe9DMYbGPAf4Ex/xB5M0DRGzzma5W09SwuvBOHozd4jhF/P3nTR42dR324hFH/ADmvm+poLlk7g5IZJjIFMraN/mAjI/7eY+63NlxIc7iWgjdA4LKMex30ButOqUKNyKtRyjZQNCMTLZQStprQ5CbaKL3US9srHbl/PS3hxaRsXL2pZEYJzRiByzLobyfnd03aPnBZ2kpXUB3NxpqQvuB5CmaQhmgZTeGFZO0MAEa8vYeDefsxeFTbOXguoOANHJ4DhsjaVuqiJQz4Wxjyd1Bl25lZuor6cDlZOwNrirjDbdtqormMe10M+dvpDTYw6u8l5/XiuxSt4WXUh0sxGJKujrpo4eFxGLqDp+hJPEfK1h+uWuPIeb2MeHsZ9ncz4u+hZEZpCi9kVulamsILyXnd7Ep9l1FvHx5JsnYG8wtvpCW8dFJVmogCo/5+SoxjTYmsbaPKzsISB5IG/a2Me11kXDPZaBYJsoereKUJXAaf9MSaUDQjDPpbCFw1DdHySY+TN314JI6EEckfvu/kz24508Pe1I8Y9naTcFVU2Zn8/+z9eZQk6V3f+3+eJyKXytr33veeRbNpZsRoQxqEAElGLDIGcw2XI37m55/Bko85x77YWNfn2vI1cI/PwYCvAVsCyxeDgcsiQJZASAgJLTOaGc2qWXp6767u6trX3CKe5/fHE5mVtXVX99R0V1e9X336ZGVmZGRkZGREZsQnvt8Dtfeo6PubwyRaVJKFMFu3RzeiYiZUtuPqSg81W2KtJ1Q6u6ii6183gLRRU9HLeq7tP0qyuq/8U+pN77yh8fAbAAB2LrYBALBzsQ3YGAIhNwEL4c3HCgCQXCpNnbKaPGU0ddpq6pTNrodQx2YwkdfAHU7D9zsN3xvCITbvNXS308BdTklF8qlU6Jbs2t1ugE3HNgAAblyqWlZ55fpCpU6JqmY6O+DrJYWfhyW3S1bxsmG9/IbG7+U0Fj+peTuiWIXsOWbU5Q5rV/0tqw5+TkUvaarvbyRJtbm8JqPnNRO/KkkyPqei61fBd0syinxRvemdSk1VF3NfUM1Oqz3dp9gXVbNzkryMjzQXndV09IqMYrW5ARV9v3K+U/P2nOajCyq4HhXdgFJTkZfXrvpbdaj2t2R9TpdyX9Gp4h9qOnpVqSoq+G4drH238q5bF/Of17y9oLqdV29ylx5e+FkZWZ3Pf1aSVd53hAPY0fnreh9uG97QwgnXba0AzGtRSnepJ71TXolmozNasCOrlkvrc9dVzcn6vGLfppqZbY6rMz2kXfW3yqmmK7knNRedkRTCOKmq8iZVe7pPw/VHVLZXNBudUWrKqpjJVdMT+aIOVt+ngu/VZPy8rsRPyBsn42MVXb/a/GCohuP65eVUN3OKVJBRpPH4Gc3Zs+pKD2s4ebPqZk5VM612t1tlO66LuS9Ixsv6nDrTA2qsi3rTuxX7Ni3Yy5KknC/pcu4xzUfnlHMdOlr9O+pN71TVTKlm5pWYRSVmQU6J2t0eFXyfZqITqplZ9Sf3qTe9O2tPVdZk/LxOFv5Aqalmr6+g+xc/rIHkjTKKVDOzmo8uZIGks1qwF+VNKuNjDST3a2/9XUpVVc3MaqBjt9o1rPpUh+ajizqb/7SqdkKd6UH1JHdpKHlIs9Fpnc9/Tl6pBpI3qi+5W21uSDnfISOr2eiM5u0FtflBdaWHNBW9rLnorNrcgEput2JflCTVzbwW7RXNRWey0OWjGkoeVtXMhIpCdkLGWxV8n4quTwXfK8koVUWpqSg1VaWqaCG6pAu5z2kmOqUOt08DyQMaSN6o7vToVQM+4b0Nga+6mc+qKfVrPH5ao7nHVLbjSk1Z/cn9OlL9QLNKUas5e1Ynir+nRXtJh6vfpz31d8qprsu5r+pi7q9lFenuyo+r3e2Rl9fl3Fd1Of6qcr5dh2rfrQ63f43pCi0Rp6IXtRBdkuS0p/ZOdbnDy6a9ZmZVMzPK+Y5mwMkp1Ujur3U59zXlfacOVb9nVTvEhvB9YEoF3ycrdjRsJTUzp5eK/1Wz0WndUfkRDSUP3+pJwg7BfiAA2LnYBmwMgZCbgIXw5mMFAFzd/KjRmS9GmjhhtXDF6NyXY02f3ZyQyJqMV1uvV6nfq63fq33Qq3O3V88hp95DTj0HvXLtXovjRi/+caxzX4k1eFeqN3+opv5jbF5wfdgGAMDOtVW3AT4LyKwMwjglqwIzS49xmole1Wx0SmUzrk53SN3pUdXNnGpmVtbnVbMzmoifk1eq3fVvVUe6T6O5x7RoR8NBR1WVmIra3IAGkgfk5TQbndFcdEbz9qJi36aC71XR96rgelXwvYp9mypmQqmpajB5SB3pfo3Hz2rRXpaR1YId0UT8bHbAsaKcb1dHuk+pqWneXpCMC9NmZlWxkyq6PrW7vaE1ivLaU3tUe+uP6mThDzUWP6manVNi5lUz81q0lzVnz141LBL5NqWmvKH5bn1e7W6PamZGVTvVMnMJpADYuoyPJXlZxepI9yn27VqIRlQ3c/LyG2pP1xD7dg3W36iiH8xCQFOq2MlmMGnpOSNJplmdqDEd/cm9qtrpVcPnXY+sIpmWtn2JWVTZji2fAG+0u/52WeW0aC9rNjrdbL0nSd3JURV9v6ajV1W1k8se2pkeVJsbVN0sqGZmFPt2RSpoJjqh1FRlfKwOt08d6X4VfZ+qZlqJWVTk80pNNWynVF1zvhR8r0puWIv2kmajMyq6fnWlRyQ5paaqdrdXJTesshlT2V5R0hLksYrVmR5SmxtSaqqajU5qIn5BOV/Svtq71ZfcK2eqsj6nSAUt2Muat+ezykAdSlVTzc5pwV5UxY7LeCujWEaRYt/WfE2hIlynqnZaVTOlqp1WznVpILk/C1R9U3NZmK3gu9WVHM0qyCWair6p+eiCSm5X9rq8ynZM4/EzqpkZDSQPaKj+JrW7PYp8MXyfiF7QeO5pxb6o/bXvklEUQmdKlPddsspJMmpP96rTHdBsdEpz9pyKvl/tbreqZloniv9DZXslW36sjlV/KFSXsxOaiU7KeKv+9H5ZH2smelWJWZSRVZsbUnd6VHPRWU1FLyvvu9WVHpRTGt5T5WV9XolZbAmb9WjOnlNiFpT3PSq6XuV9l0byf6NT+T+SVU7Hqj+ogeSNmrcXNB+d04IdUZsb1q76m9WRHlDNTuuV4m9rInpBJTekoeRNGqq/SQPJgyr4bhlFSlRWYspZ2C38tz6nou/Xxdxf6Xz+c7KKtb/2XRqqPyyrnGpmRhU7paLrV4fbp6qZDG0v7TlV7ZS60kPqTo/LK5GXV1d6WDm1S8qqVWWhYetj5X2P2t0eLdiLGs09rsgXNJDcr3a3T0ZGTnWV7Vho1al2Vc2UynYsa0lXlDOJKnZME9ELSk1FQ8mb1J/cJyOrRXtZl+PH5ExVXelhldxu5XyHcr5DkjQWP6nx+BnlfIe60iPqSo+ozQ8q0aJqdlZF179u2zkvr4qZkJFV0fetOcyNqpk5Xch9XolZ0O7629XpDq76DVA1U4p8cVkVKy8vryRbll8fZTOuS7m/0Wx0WnUzr931t2lv/duvO2y/HdQ1rzOFP1Ps23Ww9t7Xdb4D2Nm26n6grYZAyE3AQnjzsQIAro/30vhLVpeftZp81SrKS1FBeuH3Y42/fOvOuDHW6873J7r/79XVd8SpMmN0/muR6mWjfW9Otf8tqczO+02Fa2AbAAA7F9uA21+iimp2OjsDfl5lOybjI5XccDiTX+2hbVF0QpJXpLza3JCKrl8LdkSL0WXFvqSC61WH29cM3DjVVTGTzVYgFTOl0dxjSk1VPekxtaf7FKmgWXtaE/FzqtrJ7Mz7drW5fhV9vyJf1ET8vKajE4qUU853qis9rILr1Ux0QvPR+dDeRV7tbresz6lsx5WYRUleBden/uReFX2/UlWWHWCs2ildiZ/QVPxy8wBSzcyoaqYVq00F16d99W/TcP3NGo+f1WjuaxqNH5eX073ln1Rveqeea/tVXcl9XRU7vmyeRr7QrMDQkR7Qg4v/VNbHupj/gkZyX9RidLk5rPGx8r5r1YHZpXEV5eWWHZCOfUk539E8CNh4ngV7gTZOAABcizcq+n451ZdVmGpYKwxrfU4F36uKmWhWz4p8cVngaj2RL2ZBoOl1hzHerrkNtz6/9B3AG7X5AVmfk2Rlmv/NspaSXckR5XynZuITMt6q0x1U7NuUqha+B5mqnGpyJlHedavNDSpWm6zPZWGcBdXNgpxqilTUTHRq2fzoTo6qEHcoUk5p3WguOquyvSLjrbrSo7LKqWomVbGTcqamzvSQ9tTeqdi3Ndtj1psVrsrN1yEZ1c2C5u0FeaXqcPuU8+2qmwUVXZ8Gk4fklGgmelXepEpU0WjusVXVzIbqb9L+2ncq8m2qmznVzbxqZk51m4W8Fak7OSabBd68UrW7vWEemZoKrkfd6TF5OZXtlWYLPq/w/nS4/Sq6fo3FT2kqfkllOyanugaTB9WVHtFk/EIIGZlZRb5Ng8lD6koPqZ5V9EpMWW1uUAPJAyrbMU1FLyr27epwe1Xwvcr5TuVdpySj6fiVZlvTxJS1aEflTE29yRtUdH2ail+WU111M69zhc80vxv2J/fpTQsfUZsbytrs5VUxE1q0V5SaqqpmSpdyX9ZsdEolt0s96R0quWHlfbe8UklGed+lgutW3of/BdejupnLgoY1Rb6oou9TR7pPUqjmFfk25Xy7Fu3l5m+EULmrTxPxc3q27Zc1HZ3Qwdp7dV/5HzUros1FZ5SqopLbpVhtqptF5V2XetLjzbZ6FTOh0dzjOp//rMbipzSQvFH3ln8yBNyjM+pKj2pX/c2SjBJTzqqRmWy5ryj2paueGFA106HqmJlU0fepMz20bgBrLXUtaC46m7VrPKJYJaWqySuVVX5TKmw51XUm/ynV7Kz21d6tNjegifh5OdVUcrvCSQEt0+yUaN6eV953q+j7NG/Pa8GOqCs9qjY/cNXn8nKrqqeFFr3mul6Ll1OqmmIVr+u13mqXcl/WxdwXtKv+li0TMnNKm+v8W439QBtDIOQmYCG8+VgBAJvDpdKlp6xmL1rVFoxKA049B7wuP2t1/muRLj8TafwlK5fcmg1/W59Trl2qzhgVur3aeryKPaEaSbHbq9gbbus77nTkXalcIl14PNL8ZaPagtHQPU773kyoZLthGwAAOxfbAOx0Xl5lMyZn6sr7DsW+PZy5r4rqZl5F379sp13jDN6qnZJTXV3pkeyM9hHNRqezHfk+O1v9oNr8kMKZ5lfk5VVwvc0dqjUzpzl7ViW3S21+QDUzp8noec1F51S2Y4p9UampaTL6pip2QiU3rA63T33JPepN7srO1r+sy7nHslCQ16Id1Zw9o4qdVN3MqsMd1N7atykxi5qNTinyRRV8tzrSfepKj6jTHZL1sUbyX9J4/LSmopfk5bSv/i4dqL1HknQx9wWdLvyJJuMXmtUMSukudbpD6kwPquh6JVnNRxe0YM8r9h1qcwPqckfUke5XxY6HigGqKDXlLNhTVqJq83rOl9Sb3KPp6GVdzn+lOb+tz2u4/i2KVFTZjClWUUaRxuKnmqGdUrpbed+lnO/Q7vrbta/2LqWmpjP5P9Orxd9fUV3huLrTo6rYCZXtmCpmXHU7v+ayEfk2dadHNBW9vKwKREMp3a1d9bdmB5OuZAf0Jtds0xMqR9ynifjZTWnjs7v2rYp9m84XPnvV4QouVDGq2hklZuE1Py8AAMBGGR+r6Pvk5VYFsNcS+YJS1Zot+SLfprqdbd5vfa4ZDGlUlJS8amZu1Xc1460iFWV8JKNIRjYEIXy4NNl/r1RVM626bfk97I0iFZcFqYy3ssqHEJYfkPV5NeroeJOobK+oaqbV4faFVnY+p9SUNWfPKTVVldwuzUYnVW0JlrWG0BvP0ekOqjs5rjY3pPOFv2iGhNrTvVqILjbnw3D9zYpVVNmMyypWzneoMz2sxMzrQv7zzWBMmxtSmxtU2V5ptnbtTo/JyKpiJhSrqILrUc3Mq2anVUp3q9MdUMVMaiEa0YIdkTM1taf71On2h1CdnHrTu9SVHlXOl0JrOjslI6uC65UzoVWdU5L9jgqhsaLrU5sfUMVMaS46q8n4eS3ay+pID6gnPZ5VUYtU9AMy3mouOquKnZBTXUZWOd/ZrMqU913KuQ7lfadiX9JCNKJ5e16RL2os96Su5J5oztfh+iPaXf/WLKifaj66oNH4a6rZWfUl96g7ParElJe1F6ybBUW+0Jx/jQBOosUseBda3BZdv6zPN19npJza0z0q+gElZlEL9pKmohd1JfeExuOnJRntrX2bDtTeo970TtXMnKai8FuiZmdCkMfHyvsO5X2PCr5HbW5IB2rfqbzvvuZnaKPYD7QxBEJuAhbCm48VAHDzJFVp7EWrqdNW3knlSaNLT0Wau2yU7whVPsqTRosTRosTVpWpW5O+KA041ReN6ovLn7//jlS7HnCykbTrgVRH3pWoOm9UmTHqP+6Ua/M688VYC1eMeg87Dd3j1DG8fJNXnZNsLOXatKbKjBTl179/LZOnjOoLRkP3OgIr14ltAADsXGwDAFyPmpmT9ZFilV6356iYSdXMTLMtQmv5+oZEZc1F51Rywyr4nnXHlaisheiinFIVfLdKbtcawyyqYidkfV45367UVJWYskpuWFY51TWvueiciq5fBd+jBXtRziTZWdTLz7BMVdVM9KqcSdSe7pGRVc3MqOgHlfedmrPnNJL7koyM2t0e5X23cr5dsS9JMpq351Sxk+pKDyv2JV3JPa5Fe0VF16+871RqaupKD2kgeaMko8noBU3FL2rBXpIk5XxH1vLioDpcaJ8Rpqumy7mvaCp+sdnuKt9d1oJGNbZ4XmHn9Ds1mDyk2ei0xuOnNRY/rUgFHay+T21uSOO5p7VgLzbbgYSzWXerKz2k+eiC5u15daQH1ZferaqZVtmOKjV1SV4536mC61GnO6BFe1nn8n+hiplQ0fer6PpVdAOScaqYSVXshKpmUpJVrIIi39Zs5ZBTSf3J/dpVf4vm7FmNx882p6vRqmIuOqfUVNTudqvgesMBIJ9vHkDI+Q7VzZzK9opKbpf21t+l3uQu1c2CThR/Rxdyf6VyNNp8T63PhwMYbkgHat+lktulV4r/XXPReXWm+9WT3qnO9JDG4id1Pv+XkvGKfFEDyQM6Vvkhle2oLua/kJ2tn8grlTNJdga31JMe10DyoNrTPZqJXtWrxd9vHjSLfbu60sPqSPcp77s1F53VWPyUvBKV3C71JffqSPUDmovO6nz+LzSftVTJ+Q4VfI9qZk6JWVBXelid6WGV7RXNZ4GzBuNjeZPI+EhFN6CcSqFNWAtvnMp2TIlZUOSL6koPZ6GqRquTaFXQqfGeRSo0Kwq06k6OatGOrhvIWovxVkU3GNp+mEReiWpmfs3A1lWt0wZt5YE5Scq5sMy0VoZaPk3x9T//Ct1JOBC1EI28pvHcVLSSAwDgpmlzQ/rBycc3raUY+4E2hkDITcBCePOxAgC2rrQuLY4bzZw3mj5jNXXGauaclUtCm5rBu1IdfEeq5343p2f/e061+VuchjB+1Q6k3sNOux9MlSt5jTwVaeybkUzkdfjRVMe+K9HAXU75Tq8rL1g98et5XXkhkoxX936vw48muvtvJ1ocN1ocNxq402nPw2kzLOIS6bP/oqBvfCKU1Dv6XYm+99fKKnTc7Bd++2IbAAA7F9sAANiZWP+vL1VVVTOtvO9UpLYNl/ZOtKiqnVHJDa8q075RXl5VM6mc72iW2W/llEry65bN34i6whmwBd+jSIXmWdZXe51ePmsLVmoGoVJVZZWTJC3YEZXtmNrckEpu17KwVKMSU83OKPKFEEjynUpU0ZXc15WYhaytRl2Jyir6fnWlh7JWaAuKFAI9RTew6nU7pVq0lzVvz2s+OqfElLPQU4/yvlsLdkST8QuKfEF9yT1ZW4XdqmZnJ6emIi+nrvSQ2t0+VcyEFqKLsj6nnG9Xh9uvRlhrJnpVZTum1NSVd10quV0aSB7Qor2kkfyXZH2k4fpb1OaHVDdz8kqVqqbZ6JTmowtqd3vVk9yhip1QxY4p77pDqModkpfXWPykrsRPhvOLfYe606NKTU0T8bOSpJ7kDhV8n7wSzUZnNRudVpsb0EDyxqxNx3nFvqhYbUpVV2qqyvl2SdKcPae6nVN7uldF36uqmVbVTqliJlXwvTpUfb8SU9bZ/P9UairqTA+owx1Qye3SdPSKxuOnVTXTcqaugeSNOlr926qYSV3JPaEr8ROajU5mbUxSxb5NsUrh0peyNh6hVUbRDehI9QPySnU+/5eq2WmlqoV2Fr5Hi/ayFuyICq5Xne6gOtMDyvtuTUcva8GOhDP2VdV0/JIqZlKRitlZ8QdUdH1yJs0qdp1U3ndrb+3bJHlNZGehV+ykiq5PHW6/amZWFTOhkh9SKd0tZ+pKVZNVpNi3qze9U0ZWI7kvad5eyCqJdWlX/W1qcwOajU6raqayM8oXlJhFdaT7taf+DnmlmolOaTY6rUV7KVvmu7RoR1UxY3JZgGqpjYlT5IvqTo+qbud1JX4ya1txNLx/0Tl5JSFopYIiXwgtLHykip1a1bYuhOBKssorVVWRL2p/7TvV4fbobP4zmo/Oy9tETolSX1fR96ovuVc1M6Op+GVZH6vge1V0vTKKNB4/I2fqa64bIl/MPuVOPmtN2JHuk5HVXHReTjXFvn3ddjvt6V4drn6fhpI3aTJ+Xi8Wf2NZBYWbwhsZmVUtf9YKib3eupPj4bO5gWoa0vqtigBsP++Z+V0drL1vU8bF74CNIRByE7AQ3nysAIDtob4ovfI/Y114LFJSMzLWq++I18IVo5f/Z6zZ81bFXq/OXU7VWaPy1OoKILcDm/Pa82CqoXucTvx5rLmR5TvbBu5K9cCP1tV7KPwoyrVLpX6vngNOuauczFgvS0lZKvZqw1VGnvx4Tn/9bwtqH/b6vl8va9cDt98PMbYBALBzsQ0AgJ2J9T8AbE2NoMhGg19OqZxqcqbebKdxLdezDahrPguKhKBUqLbUrlilDYffKmZCE/FzinxBPekdyvkOpaamnO9YFkZLVNZ4/LTqZl7OJMr5TuV9p/KuUznfpbzvVN3Mazo6IWfq6k6PKfL5rK1FXdbntBBd0qw9rVjFUGnMt2XTaeVU12x0SmU7pp70Dg3Xv0Ult0eJWdDl3FdVMZPqTe9Ub3K3YrWpYiZ0Jfek6mZOsS8p59sV+YJmolc1Gb+ovO/SQPJAFk4bUc3MqmbmVbezShWqenWnxxX7oqxyKrldcko0ET+jmplXT3qHYl/UdPyKcr5Dh6rfo8Qs6FThk5q355WYcrPNXsH3qN3tVuzbZJVTX/IGDSRvVNmOazY6rYqZUN3MyiiWlwvTYmdUMzOqmhlV7bQin1NXekR5363ELKpsR7VgL8koUs63KzFl1cys2tyA2t0+pSqrYieboaO9tW/Xnto79XLbf9NU9FKoSOX61OUOKefbtWAvyyksh4vRiCajF7OAnFO726OB5I3aVX+LetO79GLxN3Ux/wX1JHdqKHmTRnOPazp6RQXfrZzrUMVOKDHlUL3Kl5SYihKzqMQsNqtsKVt2cr4jtPVwgyr4Xi3YS5qLTitVLXyeTBoqdClceuPCpVJJNmv9MaDO9IAko+n4ZSWqqOB7ZBXLqZ6Fxiqq2mmVzURWHcrI+Kwdiu9TznVpNjq9rM1NwfVJcs2gU2d6SD3pMS2aMaWmosHkIbW5IS3aEc1EpzQTvaqanQmvzlvtqT+qmpnWTHRaXekh9aR36mz+U80KW5Fvy6qP1ZrPmXMd6k3vVtVMatGOqW5nZX1evcnd8ibVTHRSNmvnk6iiqp1qtmEJlfBCAKzNDakj3avIt2kqflFVO6WC65FTurzFzg0yPlbJ7dKCvbjpVaeMj3S8+nd1Iff5Nat8WZ9Xwfc0q50tf6xVnH0eXmsVsFbdyXHV7Myaz3ktbemwfmDqb1Tyw5syLfwO2BgCITcBC+HNxwoA2Bnq5dVtWJKqVJkOLV8qU0azF41e+H9zOvOlSHFeOvbeRPvfnGph3OiF389p8uSNnem0FdjYa/ANLrTqmTDq2uc1fG+q6pzR5Emr0eesXGLUe8TpgR+pa+ieVO2DXu1DXqUBL2OlC49Feua3wllQ9UXp5T/LNcff1u/0v35qUX1Hbq9NPNsAANi52AYAwM7E+h8Adi62AcDrI1TUCp8r63OK1SYvl4Uwcup0B64apPLyqphxzUcX1JHuV5sfWDVMokXNRmfV5gZV9P0yMqqZOc1Er8orVX9y37IqY6mqMoo2FBZzSlSx4yq43mXj8FklIKtIXk5z9pwW7ahSU5FRpILrkZdT1U6Fpn+uT5FyzSpkTnWV7ZjKdkxF16uS26Pu9IgiFZRoMQsmGaWmprIdl1NdnekBtbvdsirIK1XdzKlu5lVrXNpZ1c286mZeRdevzvSgKnZcC/aihpM3qzs9JqdE09Erqpk5paYso1g5356FvopasCOqmMmWsFuHIhVD1SB51cx0Nt3jMj7KqmC1KfJ5Ve20KmayGTIKc6+s+eiiamZasW9XwfeqJ71DvcldKvo+OaUai5/UZPxNzUanFfuiepM3qNMdUMF1Z4GuRDU7F6p5mWlJTrvqb1fJD21sIdwAtgEbQyDkJmAhvPlYAQDYCO+lmfNGPpEqM0Yn/jzW+MtWpX6vfId05Xmr6pzR7odSDd/rNPmq1cUnrEaeiuTqS4n/oXtSLYwZLVxZ+wtwodur56DT+EtWaW1rVDAx1qvY41WevHYgpjTgNHyv013fX1fnLq/ylFHXXq/O3U4jT0WaHzUausdp/1tS2ViaOm104fEwjw6+PVHvES/vpdqCVJ40ah/0q4I8m4ltAADsXGwDAGBnYv0PADsX2wAA2LnYBmzMZgVCbrzRIwAAt4gxUs+BRp7Ra/eDtasO35BUpIVxo/qCUVtvqLjhUunSN6zGXow0dcoqqUk2kvY8nOrO705kY6k8JX3zD3Mae9Gqc7dXx7DXxSetzn811tSppWBGodPrzR+qqeeg0+f+94IWxja/iol3RuXJjYVTFsetTn/B6vQXrr65j/JeLgnjbtU+6FSZNUqr4Xab8zr+nkR9x5zmL1vlSl69R5zyWfub7v1O/Xc4zY+GaRy826lzN7lTAAAAAAAAAACAW4EKIVdBKunmIxEG4HYzP2o0P2pU7PJqH16qoOESaexFq0vfiFSdk+Sl6pzR3CWrS09bjb9sFRekYo/X/OWW4IjxGrjTKcpJo89F1zUtnbtDGGNlsONWGrw7VWnAK1eSeg85lQa85kaMylNGaWgFqVybFLd5dfXllS9JNVdVx26nviNeZ78U6cxfR0rrRnHRKy4oC+kY1RelgTud9j6SqtDhlWsPQZ6uPTf21WbuktHFr0cavi9V72G+HgHAzcTvAADYmVj/A8DOxTYAAHYutgEbQ4UQAAC2gI7hUDFkJRtLw/c5Dd/n1nycSyQThWon5Slp6rRVW69X526vuBiGufJCCJTMXzFauBKCJwtXrBauGBV7vO75wbryHV4nPp1TocPr0f+9qpEnI33tl/OqzRuVp6XyxOZXKbkeYy9eX6glKFx7kJbxv/jHuWW3de13yrd72SjMYxsr+9s3/7aRZGLJRmG4yozRuS9H8s7IRF4PfrCue3+wLlcP831xIoRsjA3vWVz06jnk1LU3jDOpSHOXrF7841gv/UmsuE267+/Wdcf7EuVK4X3NZZVU6otSVAjTUJ6U5ketug845dvD/d6H59iotC6NPBmpY8ip98jVgyzeSyc+E+vKC1Z3vC/R0D1rL58rHzN1ymjkyUg9B732vTnd+MQBAAAAAAAAAIBbhgohV0Eq6eYjEQYAm8cl0ok/j3X+a5FyRa/SgNfUKav5UaP+O5y693ud+WKkqdNWuaJXsTdU2PAuhAYWJ4xKfV6lfq+4zevU52Ml5a1TfeR2YiKvwbudypNGcyNWJvIqdEmVqawdT+zVd8xpYSy02yn1e7UPhvcs3+GVlI3qZam+aBQXpO4DTt0HnAqd0jc+kdPMuRD8OfyuRHe+P1Gh02t2xGj+slXnHqfBO53SuvTEf87rzBdDHthYrzu/J9G+R1LlSlLPQae5ERPe56o0eJfT/BWj01+INXt+KVj04Adr+o5/W1WUD2ER70K45XosThilNaljl7+u8MtKaU069+VI86NGR96dqn3wtX2tdYk0ccKq/7iTJTYN3FT8DgCAnYn1PwDsXGwDAGDnYhuwMZtVIYRAyFWwEN58rAAAYOsqT0pnvhTL1aXBNzglFWn6rJVLpLRmNPaS1fRZo44hr0KndPqvI419025KC5u46JVUlsZjIq8op2W34ebJd3h5FyqjeBfa+RS7vQrdXsXu0Aqpcb02bzTyRKTKjNRz0Ks6J028EhIkhS6vwbtSDdzptDhhdPm5SElZinKhgkuUk/qOOu17JFW+I4xr6rTR7AWrpCKNvxKpMh2WgbjN6/7/pa4j355o4E6nYreXzUmuLk2etLryQqSkJlkrnf2bSBcei9R3zOnt/7SmQ+9IdeaLkT71j4uaG7Hq2OX0Xb9Q1R3vS+RSaeqU1eKEUcdwqAoT5cN8WBgL1WOivFehQyoNeJmrFOXxTqrOS2nVqDQQvoKf+lyk0ecjDd2Tau/DqeLiUgWZ9Yw+b/XSn8TqGPa67+/Wle+49nuW1sP83Cxzl4zOfinSgben6trLzwm8dvwOAICdifU/AOxcbAMAYOdiG7AxBEJuAhbCm48VAABsL95LPpXKk0YTr1pV56TO3V7tQ15x3ss7o3olBAs6ih2qL0pXLixq4lWrqVNWnbu97vreunoPe3kfKkKkdSlfCgfXR5+zmjhhlSbSzDmrs1+MNX3WyKWSS4xcEoZzSbh+NbmSV33xtQdMbOyv+Vy4Pdi40RLILLtt4C4nn65uiWQir669XgN3OsUFr8qMCf+njaqzRtVZNQNSha4QjpFfvazkSl53f6CuO9+faPTZEF4ZeSqSTyWb8ypPLqVOSgNOD/14XfvfmspYaXHcaGEsPGeu3Wtx3OjFP85p9oLRkW9Pde/frSvKSzPnjRbHjIyVBu92Gr4/Ve+hEGjxXqrOSeWJ0J6qrTc8l0tD26Tnfy/WZ/5pUWk1hGG+499WdeidSagq1B+q35SnjNK61LXPqdh1Y/N/+qzR+MtWHbtChZ3NDLRg6+F3AADsTKz/AWDnYhsAADsX24CNIRByE7AQ3nysAABg57oZ24ClcIjknOQTyaVG3kml/nAw/OyXI73657HSupRrC+GV9kEvG4XHeydVZoymTlstjht5L9lYKvV59Rxyuut7E1WmjV78ZKzFMaPagtHlZ63GX4pU6PYavDuVq4fWMJ17nTp3eV1+xmrmvG0+V3nKaOGKUW2+JSxgvHJtS1U5ls27u1PNXzEqT1ylPEVjNJHX4W9Lderz0ZphhJWivNferELHq5/hiPx2lu8IlV0Wxo3S6tKy0bHLySXS4vi1l6+1FLq9uvc7de936jnglWv3qi2EwJYxkkz4XE2ftpo6bRUXw8+T1sBNXPQavjcEV0oDoUJQecooqUhxUbKRV1IzmjplNfqslUul3kNePYedeg87de72auv1ioteaU0aeTLS5WcjVWaMfCrt/ZZU9/8vdclIi2NGuZKUJtLkCat6Rdp1v1N1TnrpT3JyifTA36tr+L5U5x+LlValtj4vGSkpS+1DIRRkI6k6F0I1Ngqf3dmR8Ln2qdR72KnYszSfvJdq8yGIE7dJHcMhCDd7wUhG6tq71GJp8mRo52SM1HvEac/DqQrX+H2Y1rIWT7mNtXlyiTT2klXXHqe2vut7zzfKJdLEq1bd+5z2HuZ3AADsROwHAoCdi20AAOxcbAM2hkDITcBCePOxAgCAnWu7bwMa1RXMdRQPqS9K9bJRruQVF8Nj03o4QDxzzmr2klHPAa/9b0mVVKXzX400f8WoNmfUPujVsctr+mwIr6Q1KVeS7nx/osG7nKbPGV1+JhwVrkyFYWzO69A7U3Xvdxp7ySpXlPa8KVW+PRysfv73Yj3z33MqTxrFRSkuhMBIbT5UpAgVMbQqaBLlQ7BmbiTcvvtBp7Z+r/GXrGbOLYUMCl1e7UOuWd2lOmdUnVl7hhnrle+UDn5rorggvfjJWD597ZVZjPWb0uYIaLA5r/YBr/lRs2rZ6tjlNHiXU1T0GnkiWha66drv5OrS/OVwW74jBE3aer1OfT5aNq5cyev4exO1D3otjBnZnBQXvOK2EHA595WoOR6bCxVXBu9yah9yKnSGUJvNhaCNjaXKtNE3/ltOs+etTOR18O2pdr0xlY2k81+LNHXaqmuvV8cup/GXreYvW+XavPIdYTo7hr2OvDvR3kdSVWeNoji0dfrmH+b0yv+Mldak/uMuhHKmjAqdXu//v4ze+EPSxVPzah8KAbiGpCpVZ4yiopdPpflRK2OkvmMheOO9JK+rtmy6Gu+kC49Hmjpt1Lk7zOfO3Us/k9O6dPZLkbyXDr49tHbyXnriP+f05Mfy6trv9K3/tKb+404z54x6DvpmW6jtxiVhGQGAzbLdfwMAANbHNgAAdi62ARtDIOQmYCG8+VgBAMDOxTZge/BOqs6r2SbFe2ngDqe4ECoUSFKUXxq+Oi9NnbTKlaS+o27ZAV3vpclXra58Mxz8jYteXfu8eg855UrLn3d+1OjCY5EuP2M1d8mqOi/5xMhEoa3KrvucSoNe9UWpe7/X8H2pvvFf83r5U7Eq00bGeh15d6q3frim5/5HTic/H4XpT6X+O5y69nrNXzYae8lq/GUr76RdDzjtui+VS8PrnR8NLYyqs0sH6uO2UHWj8b/QI9UXwkF1nxp1DDsdfleqXJvXzPkQ3Dn/tUhpbXlwoH3IKd8uTZ8z6hj2uveH6rrweKTzX7l1R2bjNq+kTHgGrwPj1bnLKyqEyirzo2bNikaFzhBKmT5nmy2MQljNq33Yq++Ik/ehldLiuFF5KoTleg46VedCpaZ8uzR32Wj2wvI0Sf/xVAfelkqSTvx53AzUFLq8Dr4j0bmvxKpMrb3829jrzvcnuut7E/Ucchp70Wr6rFWumL2mspGJvIbvc6otGL3655HqZaMDb0114O2pCl3hJ3p9QZo8bTV50irf4TV4VwiqNFqcNYJ45akQxkvK4fXV5qWTfxmrOmu091tSHXl3kk1XqDxTGliqejV32SjOS239ft3AovfSic/E+vK/z+vKN62OvDvVt/8fFfUfY1cCgNeO3wAAsHOxDQCAnYttwMYQCLkJWAhvPlYAALBzsQ3A7SKphAOkrRUMGryXFq4YGRNapcSFtcdRnpKmz1gNvsGtGmbuktHzv5/TwpjR8D2p9r05Vc+hcLDWu+VVEKZOGV14PNKVFyLFbeFAb/uAV7EnHDB2qbT3TaHlzzP/PafJV60693j1HXXqGPaqLUijz0UafdbqyguR0rrUPhhaFxV7veYuGU2esIqLjdYxRvVF6eA7Un3r/1bV1381r7N/E6lzd6hIszhhVJsPLVSMlWbPh3ZIMxeWt6FZT6HbyydSbcFo4K5UR78j0fxlq0vfiDR5cmPlH9r6nHLtWauVa7RFsrlQcYKqMNhJjA2f8eqcUX0xLPuFrtBSKaka5Uvh85+UQ9AktFZa/RmxufA5L3R6tfWFyjO9h0OwL1cKrdhq89LUmRCUS2tSW49X7xGnYo+XqxuNPmc1ecqqY9ir/w6ntCotToSATmXaqDQYQjCdu71cKo29GAJ5ex5O1b0/rKPmRqwWRo0KXV7DD6ShNVXJSz5U2Zo+azV/2aj7wFJ7pfKU0cUnrBauWPUeDhVzSgNexnotjFm5bF2YK3nVy0Yz54zGT1jVF0KbuWK3V+der1wxtIvqO7LUAsolkomWKoLNjxqd/Gys+StGfUeddj+Qqvug14XHIj3xn3NKE+m+H0p0/H3Juu2cvJde/rNYJz4Tq2uP0+6HnA5+a3LNVlHA7YDfAACwc7ENAICdi23AxhAIuQlYCG8+VgAAsHOxDQC2L++k+StGM2et0kQqdHjZeKnVhyS1D4XWQsaEFksrD4xWpqXJU1a1OaO0Hg7I5kqhnYhPpaggtfWGKjLGhNtnzod2SIsTRpUpo7QWDub2Hnba95ZUnbu85i4bPfP/5DT6nFVbv1fXXh9CP6lR7xEnG4d2Ll7S4UdTLY6Hdiry0sF3puo56FSeMDI2VL+ZOmM0+apVlJdy7V7zl8LB6M69IYjT1uflEmniFauxF22zTUyxx2vXA2lor3TJaOSJSMZK+x5JFbd5jb0YafpsCLlEBa97fqCu4fucLn49umbLpFzJa/j+VLliODgdqtxcOwSz642pFq6EA+6tSgNOixNhWuKiV+9hp6QawkC1+aWQwTUZf83QDnBbMCEQU5kymrsUWi3ls0pW1bm1q9usvL1jl9OuN6ZqH/BKa0bl6bDeynd4jb9sV30O46LXsfckKg14VaZMqFA1ZzR4V6qhe53yHSHUM3PBaPa81eyIUZSXBu9yioteC+NWSTkEWEr9oarOyJORJl+16t7vdPQ7E/Ue9sp3hOCcsVL3AaeOXV6VaaOxb1qd/1poVWejsA7rPxaCNa5uVOz1Grw7BHSMlS49bXX56UiFrrAujPKhNd7kyVBBZ2HMKKkYDdzhtOuBVH3HnOZHjV7+05xmzoeKX+1DXse+K9GBt6XNbURaC+u1hTGjetmEMM9QqARUmzeaHzWavxwq6rT1e5X6wzrYp0ZxyavQ4ZttqmoLoTLZ2EtWhS7pyLuSZRXJavNhuxXl1dyu9Bx0yncsf3/nLhmd+3Kk2Ys22y457X9L2hyXd9LoC1bJolH/Hanaeq++eLlEWhg3ah/wa7Zsqi+G5ax90F+zdVZlVjr/lUjdB7yG3uCuPvA1jL9i9dRv5CQjveknauo7uv7uxdpCCNKuNX38BgCAnYttAADsXGwDNoZAyE3AQnjzsQIAgJ2LbQCAnSgcxJS69137QF5tQZobserau7xt0uyIaR5o7T7gJC8lVaOkEg6+DdzhlrVqqi+GViGLY1b1RSlNJJcYuURy9RDIGbzLafg+J+9CyGXhilVtPrRQ6jkQqsuUJ406dy8/QOm9NPqs1YnPxFqcMGrr9UoqRrMjYdg3/q919R1zWhwPFR2inPTs7+R09vPFcNC5WNfsBaPZERuuR1LXHqf2Ia+0HgI9HcNOlRmji1+PVJsPlR+KPeF5kopUXzSaGzHLWi8Ve72KXSEA1KhWk+/wqpfD/fvenOqO9yVaGDM69+VYI0/ZZWGVg+9I1Nbr9epfxEoqS7c/+MGait1eL/5xTqWB0NLqxGdi1RYIugANUcFvqErURhW6vQbvSlWdNRp/xV41ELdhawTUij1eh78tkXfSlRfWrlQVFbwOvDVVdc5o+qxRbd4sW0c05Epeh96ZKN8RWsO1tqnKlUI7qbgQKovFRa8oL8UFyblQGScpG+XbQxWcpGJUX5C69nql9TA+Vw/hoZ5DTlEurN+69zuldaPp01bGehW6pXNfjpqhvX1vSbTrAafqTFh3pvWwLfBe6hwO1bbGXgwVfnbdn2rXg2lWOShU3nnh92O5JIzL5rzu/+Gwfo/yoZra9Bmr8VfC//KEVVwM26hdDzjteShttrwa2JdXx5BUqZc1P2p14eth+FK/U/cBr31vTlWeMDrzxUjeS7sfdBpoBGlMWPfPXw6hoNJAqIB04fFI02dCuGnwbicThfCQS0KFoOkzYf7vf2uq/uNOcyNGtYUQbjI2bHvaer269jrZnJSUFbYxVSmtGk2eMnr+93KaeMWqa7/XgbclOvD2VHseCq3G5KViTxiXS0KlpWpWcakysxReiouh1dnZv4l0/iuRagshYLrnoVQP/f26Bu9aCu14L1VnpJGnIr362VhpVTrwtlRHvyvRxMtWJz8X68xfx7Kx1yM/Wdfx9yWqzkmvfibWxSciFTq9jn5nqt7DLmzvkxBOKnSG5XjqjNXos5EuPBYprUkHvzXVvT9UD+9nNYS35kaMFieMCp3SgbeFwNTYy+FxE6+EYO2BtyUavDu0OmxMtzFhmbjw9UgzZ606djv1HQnv71qVkVwqVaaMFieNbORVGvTKFZUta2F886Mh9Nu5x6l7f1ZJz2vd9me1hfC+tAa40ro0/lII8fbf4dZ97Hq8D6+rNh/et1L/9e1iT6pSlNOqlplpNQSdW6enOi9VZ406d639fbE6p+Z6ozEeae354X34LpgrLd0/e9Ho67+e18w5ozf8QKI7359c9/x4rbwL4cEoJw3de+33ozoX1s39x911z3tsHewHAoCdi23AxhAIuQlYCG8+VgAAsHOxDQCAnWuztwEuUbMiQqk/BE+kcJBpcdyo2B0OwnkfKsysPOt+cSKESoyVSgNeHcPhZ3Nak6bPWk2fNeraG6pCrFSdk87+TazRZ61mL4aWJEP3ps2gTq7kVZ4wGvlGJJ9Khx5N1THsdOpzsWZHQkhHUmjVNOw0cGcIwEyesOGg4nzW1qU7tHYpdnsVe8LB5PkrIfCy95FUXbudXvl0rJnzVnFBSmrSwqjR/GWr+Suhukv/cae0Gg6qeRcOTlbnQjWFXFtj/FKp3+nYd6U69G2JHv9PeV38eqhg432omjB7MRyw3lRUkAFwGzNRCPhsuGrWGjr3hMBjZdpocXx50HFjE/Ha1qM2Du0IF8ZWtw4zUQhzrDdNcVuoLpTWQmUz7yRXXz5slA8BntpCFpBp83KpUWVa6063ibKKPvNL93cMOyW18LjO3WGbPXshhFeiLCDR2EZ17XMhLFUzGn/JNgOcHbtCEDbKZ98ZjDR1ymrqjA2VhIZDdaH6glFtYemyteJaxy6noXucCl2h5ViuFKZz5rxRlAthWxlp+ozR6PORZs5ZxW0hTNrWH0ItjepxpQGn/uNh+IUrJgTSfAh+Dd7t1Lk7hL8WxkNlutmLVjbndfjbUrlEuvBYSNp07XVq6/XKd4SgmPfSxa9Hmr9sVegOlZ1cKo0+tzxcN3RPqv1vTbU4YTT6bCTvwm35ToUA1XwINLf1efUfDZXjps9a5UphmanNG1XnjAodYV4sjIVgcfdBr67dTpVZo7QqFbobVf+8XvlUrPGXw3QP35dq75tCcGzsxaX2ckfelaj3qNPkq1bf/MNcWA6M156HnHY/lGrgDtcMHLskfG9LE8nVQoXBRkW/M38d6XQWoDr8aKrh+1NFubCc1BZDJa9Sv1c5q/w1P2pUnTHq3Bta2dXmQmvOgTuc+o452ShUYJo5Z5vLZqHLq++I06WnI537ciSbl/Y8mKrvqFO+M3yuFkbDMpBvD9UQq3NGHUMhBFWdDcvo+a9FSiohwHb4Xam8k0aeiHTmS+H2noMhDNh72KmtJ7zHE69Yjb9sZaIQ+uo56NS1L8y32RGrviOhGleUC9+NXSMcnoYqVxPZYw+8LVWhy+vK85FkpJ5DToXO0JavcUTJ+xDmad7mw+ei+bcPlR/b+rwmT1q98ulYtbkQ7tvzUKq+oTbZWJqdXdT4K6GNab7Da++bwrI8fcY2W6EO3hUqmNk4fJ9fnAghO2N8M8xnTHifq3NGM2eNvAtVHwudXpVZo0Kn1/C9Tibymr8cqlcujhk99z9yOvOlSGklrLMOvC3VIz9V056HwucwrYXPc20+tDetzWef+yGvqChVpsPvho4hJxuHYfIdvhmOa3BpCJK5RCp0heldnDCaPmOU1IyslQbuTFXokmYuGC2MGtUrRrmiV/+dTsWuEGZrTEex26vQmVXkHDXKd3oVVlQua0gq2f9qCDjaWM2QWW0hTFOuJD3xX3J66jfyyrd7vflDNd3zd9YPiKU16fxXI01krWmH3pCq0Oll81KcXwrxbbbR562mTlkdeHu6LBDmXZjHjd9+6/E+rG+nTme/1d7gmr8LW1s/vhZpPczjmx2u26jps2E9PXTPxgKRK9sXY2MabaUb1XCxHMcCNmZLB0LK5bI+9rGP6VOf+pQuXLig9vZ23XvvvfqxH/sxPfroo5vyHCMjI/qe7/kezc/P63Of+5z27du3KeNtxUJ487ECAICdi20AAOxcbANuby4NB84WxkMVl/pCOEgVF0KLptJAqHgwfykcMKpXwk7FnoNew/ekmh2xmjlnlCuFai7de52KPWFn/9xlo7lLRi4J7VDSutGFxyLVF8OBv3Dwz2n2ktXos1blKaOk3GjjFO5vH/Ya+6bVxAkrl4Sds0NvcOo57DRxIjx3eSo8R/uQUxSHNleuHoIzbX3hAFz7oJdMOHAwdzlU1akvGp3/aqSxFyOVBpz6jjr5NBwoNDbsWN//1lCJYeIVq0vPWF15LhzsuPsDiXoPOT39WzldfjpadoBT0rKDubmS14MfrGvPw6lOfS7SS5/MLatEY+MQclo1jkyh2yspr3/wtqFzj1vVngYAAGCnMpG/4WpkxR4vG4dKafXK8mBcXAzB7vnR1d+74qJfs+LYWlXXOoZDyKoRfCt0e0W5EMQrDYRw9/QZo4Wx1c+TK3kVOteehobuA6EKT64Uvs/PXQrtCE0UQg/XCoXbOAT52vpCYKytP3wnnTkfgvDGhhBW+1AIpzSqJMVtUnVWqs4YFXu8OnaFVoZj34w08mTUnB9H351k4Tur2YshPNl4zrgthLt6jziV+sJh0JnzVldesMvmx+4HU81fDgGwnoNOB9+RKKka1RelUp9X3CbNXw4VvordYTqM1bIgkhRCFuMvR1oYDZXHSgNOux8M4U41wlMKl43QlLFqnqxQX5RmR0J1tbS69DsqzUI8SSUEjbr2OhU6w7y1cZgG+fB+lCeNqrPh5IW+o065UliWJk+FKnbdB5zO/HWsU58LZ0T0HUu1+0Gny09b1StGbT1enXu8+o+HkF2xx+uZ/57Xqc9H6t7vdeTbE81eCPOq/w6no9+RqL5oVJ4wiopeha4QFKzMGJ34TKy5y1ZxPpxo0L3fqzYnjb8SKS56Dd8Xpq8yFWZgI8xVm5faB8PvuPnR8N5OnzOqzRn1H3fqvyNUMo1y4bdLxy6vfCnMC5dINh/aFM+Pht9/8iGkt/Q/LPvlKaOpk1bnH4s0e9Go77DXrjem4f1ok3LFsMxUpkP4rDIdwmlxW/hcdWXB3agQAlAm8iFMetqqrc9r+qzVE/8lp8lXI3UfcHrzP6pp4C7XXBYWx40uPxuChH1HnTr3hM9GvRyWvYWx0L431+41cGcIJObbQ/DP1RUqqKbhtZX6w3REuTAfGi2V40JYLuYvhxaj574S6fxXY5nI69h3prr7A3V17fWqzkmjz0VauBJep41DkLTQmYVK28Nnt2vP5sYJ2A+0MVs2ELK4uKgPfvCDeuaZZ5TL5XT8+HFNT09rZGREkvThD39YH/rQh17Tc3jv9eM//uP66le/KkkEQrYRVgAAsHOxDQCAnYttAG53r/WsOe/DWdeNMzaL2c7K+VGjmXNW/Y0WGZl6ObRwMEbKtXl17Q9n60+eMpq9YJvj6drr1bXPqdARdgbOnAtn7bYPhJ17MmHH/tyIUff+sON17pLRpacjVWbCWfA2DmeUTp4MgZu2bKf83m9JNXRP2CE7f9lo4tVwFq+NvWYvWI29aLUwFnagdh9wOvxoqrQequxIUhRL3QdD64iOXeGsucvPhrOap05ZeS8dfjTV0e9MJBPanbzyqVijz4eWKzb2GrgrnEXfMbS0c37mgtXshdCWqmOXV+eucMBlcdKoMmVkc2G4ejns8FwYDfO90CW1Dzp1DHuNvhDp1F9GzbP/o7zXrvudin1hR21bbzgocvoLkRbHQ0uWrn0hPFTs8trzcKpdDzhVZqWzX4p14tPxUljHeB14e6qBO12z8k9jJ39a1dLftXDGds+hEDS68oLV/OVQBSAuLIV/OveEM1snToT57VItO1hkrM8OOhjFbeEM+7NfjlWd2eDBrfWqTBive38wkU+lF/4wXreiQ9feEL6qTIUqB2sd4LpdFbr9xufjRsbX5VVfVLMVDwAAAHCjbM6vqtC2Uq7kN1zR7tA7E/3g75SvWYVno9gPtDFbNhDyz//5P9cf/dEf6e6779av/uqvavfu3ZKkP/7jP9a//Jf/UkmS6Dd/8zf1tre97Yaf47d+67f00Y9+tHmdQMj2wQoAAHYutgEAsHOxDQBwPeqLIfAS5V+/56jNS5XZUG2mcYbdSi4NZaCLPeFM1PW4JFTNSSvhbOFiz8amwfuls069D687Vwq3lSdDqKVzz+oS1LX5EIwxNrQKMDaUpG+8jupcaDkgI3Xvd8p3hPlpc6HNx+xFq+qcUd8Rp1y714WvRZq7ZFXoDOGaQrdXz0HfLNO+MGZClaCxUCXIRFLnrnAGZ6Fl/2VSlS59I9LUaaPSQHh83rVrYVyaHq/I5rx2PxgCMOUJo0vPWI08EcnG0pFvT1Toki49bbUwGqr6yIRloH0gtFiYGzGaHzXqO+a05yGn6bMhUNVoE2FzXvlSONu5MmV05ouxqrNS177wurxfap+wOBbONJZCC6+46JuXuTZp77ekOvC2VIvjRue/FlpCTJ+1oT2KD49PKiE0UuwOZ5I25l2utNROpjIjde/zOvaeRJ27QxuJb3wip9OfjzV3KbzOtt5w5nRp0Ktzl9PBd6SKC17P/15O86MhMLbnQadD70x04i9iPfvbOZUnQshr6N5Ud7wv0ewlo3NfjuXqoX2DycrYV2dD24Hu/a453xbHjZ76zZzGX7KK20Lrtf7j4X1p6/Maf8Xq3JcjyUtD94bWE4N3O02fsVk7kqxdSz6EqZJqaAkz+AanPQ+mmr8SlpeJV60qU6HFQSN4ZozU1u9V6gsBK5eYEHSqh2WovmhUnZc6hr3673AafymEz4rdYV7PnjdaGDfNClIuDYG9Ur9XUpXGvhllZ76G5W//21JVZ4zOfjladda9icJynpTDMh4XpFx7aAGRaw9nI+faw1mzlRmjy89Eqs5u7IBKruQ19Aan2oI0dWap7VppIITkps+FAJgUzkQevNupfcjpyguRZs8vTx8We7wG7041cSK0m5FC+5q2Xq+5S1bV2eWtbXIlr+F7U81dtpo5F4bPd3jd8d2JBu9K9dj/nW+Op/H81mpZVaooH0JxrVWporxfVoWq9SDURlsoDdyVqr5omtPVGE/vYaep03b5QS3j1XsorIOmTl9/IrPQFdZ3ra9rSzIh8LkqKGbCuui1tKXCcvmOsKxNnwnbQADAaj/4O4s6+u50U8bFfqCN2ZKBkHPnzum9732vvPf60z/9Ux07dmzZ/b/4i7+oX/u1X9NDDz2k3/md37mh5zh79qy+7/u+T1JoTSMRCNlOWAEAwM7FNgAAdi62AQCwM7H+x2ZrDVJt9H6XhMpLaS20A3NJCJHExet73upsOEBfX5Rqi0ZxPoSuavNG4yesbBQCSZ27/LKqUmk9tEnItS9NW9oI76zIOqT1UGY+rYdpzLcvvYaxl6yinNR/h1sVJqvNh2pEnbt8M0y35rxIpdnzRpU5E8Z1LITKps6E1mqde0LVKUmqTIdAS7Hbq/tAaFFRnjQqdHrlSiHkU1sIoTljQzWp8qRRsdcrzkuV2awU/3QIiQ3f5+TTEPyqLxh17Xfq3hemtzonXXo6UnUmVELa/cZUPQfDYY3ypDT2UqSZc0b1xTBvotxSECzKh+veKQSKhrwOfms4mDXyjUjVmRC4iIuhTUBlOrQ/CG0qQpn+Qlco/78wFlpGuMSEyk2jWXCsEN7r0kCYprkRq8lTRm290vH3JrKx1+VnIi1OhHYObb1enXuckmpoj1DsDuGiucshCFbs8eoY8tr9YCpjpFc/G2v2olVcCAGxQ+9IVRr0Whw3mj6TteRbDPOmc7fT8P1OcSG8H+Ov2KwSmFP3Aa8rL1hNnwktP2ws2SiEsmwcpmHgTqfylNG5L0dyqdGu+1PZXKgyllRNWGZM1i5D2d/GN28L15cu07pUmTIykXT421L1HXE6/7VIc5eMSm1FuUSam6mq1B9e7+K40ehzkeI23wyi1ReNTv5lpNFnI+VKISDXPhBCdsZkLTjSUAXNuxBQ6j6QBYZOWaW1EAKaGwkhMhuH+RSXwrI8fE+qO78nUb49LGvP/FZO574SqzYfqm7lSqESWa49tKLId3i5NFQZc2kI/KWJtHDFyrsQvKpMm2YbvtZQYVwM82n2YljOeo84Dd3rVOj0qs4ZjT5nVZ0x6j3s1HMotIhYnDAaeym8x4XOcFuuFCrJTZ2yynd69R9zqi2EVovhAy7NXQrtKDp2hdBfviOEsxrh0IlXItXLUs8hpzgvzVwIgboHP1hXZcroqf+a09xI+Fw1KnzFbb4ZNnWJNHSP08G3p5odMZo6HVqdpPVQ6SythTY5tYUQTC1Pmma4qa3fqdgd3q/GekDSqspk+Q6/qiVi9wGnPQ+nOvHpuDldxV6v7n2hhUdSNqqX1ZwfKwN/bf1Oux9wGrjL6cU/jjU3YhUXvfqOOY29aG+4RZAUPkPdB5wKXV4TJ6zKk9cfWCv2hPDoyjaOr6V90bVE+RCULE+ZNau55Upe9bKW3pv1Ksi9jjZS7WIryrf7rR8+3KAo7/X/+atF9R93mzI+fgdszJYMhPzKr/yK/uN//I/rBj5GR0f1zne+U5L0V3/1V9qzZ891jd85px/5kR/RU089pZ/92Z/Vv/t3/04SgZDthBUAAOxcbAMAYOdiGwAAOxPrfwDYudgGvP4aAanXyqVSUl4eXLte3kvVmVClqhEsa05nTaHyVy78XS9L+fYQVEqqIQhXL0vWSj2HsvZ/i9LcZaP2Ib9qfM3ndNLsiFFSDlXMGhXKWl9XZcoo1x6q7lRmQnvDELwJQZakIrUP+2ZFqMa4Vv5v6/Pq3r8U+PM+hHZcvSUsZcP8a4SmXBoCMUklVIFr6wmBJ2PC66svmmaQyMYhSDc3YpWUQyAnTUKQ0dgQImzr9cq1e02fsZq5YORqodpd90EnOens30SqLRo98CN1Dd7tNHXKqDpnNHiXU1zMpvmy0cQrVuOvWM1etOo/5nT399dVWzAaf8mqY3cI35z6q1gTJ6xK/V4dw05pLVRqmzgRglEH3p7q4NtTeR8q6s2cD9XDBu92qs4aXflmaH/Z1utlovB68h0h7DR3yWjhilH7YAh3de9zivIKAbdLIbBYL0sz563KE0b1cghnRblQraoyE9pHDt7pFBdD2KoyEwJ51bnQFrOYtcHcdX+q/jtCGGjqlFW9bJSUQyU++azaW0+o+NZ43vKE0ewlq/rCUqvHtG7UPhTaWZYnjBYnjfY86HTvD9V1/muRzn8tks9yFMZKuaI0+IZUpf5Qfa0yY5RrC+1Ac6UQDOra41SZMRp/2ao8HcJZxobXaXNhOarMhLBVaDcZAo71cggE1haM8u1h2vuOOg3cGSrNlSeNXvzjWFdetJo5a5UrSbvemKr/WAhVuSQESWsLIbRYmw+V+e74W4kOvn1zqoNIbAM2arMCIfGmjCXz9NNPS5IefvjhNe8fHh7W3r17dfHiRT3++OP6/u///usa/8c//nE99dRT+t7v/V69+93vbgZCAAAAAAAAAAAAAASbEQaRQtggv07oYqOM0bpt81rbAIaWY0vX44LUtXf1ee25ktR35OrnuxsbWrNJaw9nIzUr+0ihWs+u+5eqH7QPLX9csXv9ca16bhOqMl3LesPkSqEyx7Ln75KKXdeuztC9b+2D9oceXX5739Hlr8cYZW3X0lXD5ju8OoaXbrvzu5NrTkdD+6DX0D1L1zt3h2pE6xl6w3q3uxX3bV44IcyzzRtfq0PvTHXoneuPe/eDV3tPvXY9sDkVORo6hr0G765t6jix9V1/vaKrOHv2rCTpwIED6w6zd+9eSdKZM2eua9wnTpzQL//yL2twcFAf+chHbngaAQAAAAAAAAAAAAAAtrtNDYRMTExIkvr6+tYdpqenR5I0NTW14fEmSaKf+ZmfUa1W00c/+lF1d3e/pukEAAAAAAAAAAAAAADYzja1ZUylUpEk5fP5dYcpFArLht2IX/3VX9ULL7ygD3zgA3rXu9712ibyOmxWXx5cP+Y9AOxcbAMAYOdiGwAAOxPrfwDYudgGAMDOxTbg5tjUCiFRFEmSjDHrDuN96Edl7cae+oUXXtCv/dqvaXh4WD/7sz/72icSAAAAAAAAAAAAAABgm9vUCiGlUkkzMzOqVqvrDlOr1SQtVQq5mlqtpp/5mZ9RkiT66Ec/qq6urk2b1o0YG5u7qc+HpSQY8x4Adh62AQCwc7ENAICdifU/AOxcbAMAYOdiG7Axm1VBZVMrhPT29kqSpqen1x1mampKktTf33/N8f3SL/2STpw4oR/4gR/Qo48+uinTCAAAAAAAAAAAAAAAsN1taoWQI0eO6MyZM7pw4cK6w1y8eFGSdOjQoWuO79Of/rQk6Q/+4A/0B3/wB+sO9+53v1uS9KEPfUgf/vCHr2OKAQAAAAAAAAAAAAAAtp9NDYQ88MAD+vznP6+nn356zftHR0c1MjIiSXrwwQevOb57771Xw8PDa95Xq9X0/PPPN4fL5/PavXv3jU04AAAAAAAAAAAAAADANrKpgZD3vve9+sVf/EU9/vjjOnXqlI4cObLs/t/+7d+WJD3yyCPat2/fNcf3y7/8y+ved+HChWZlkF/6pV/a0PgAAAAAAAAAAAAAAAB2AruZIzt06JDe//73K01TffjDH9bZs2eb933yk5/Uxz72MUnST/7kT6567Llz53Ty5ElduXJlMycJAAAAAAAAAAAAAABgx9nUCiGS9JGPfESvvPKKXnnlFb3vfe/THXfcodnZWV28eFGS9NM//dN629vetupxH/zgB3Xx4kV94AMf0M///M9v9mQBAAAAAAAAAAAAAADsGJseCOnt7dXv/u7v6uMf/7g+/elP6+TJk4rjWI888oh+9Ed/VO95z3s2+ykBAAAAAAAAAAAAAADQwnjv/a2eiK1qbGzuVk/CjjM42CmJeQ8AOxHbAADYudgGAMDOxPofAHYutgEAsHOxDdiYxnx6reymjAUAAAAAAAAAAAAAAABbBoEQAAAAAAAAAAAAAACAbYZACAAAAAAAAAAAAAAAwDZDIAQAAAAAAAAAAAAAAGCbIRACAAAAAAAAAAAAAACwzRAIAQAAAAAAAAAAAAAA2GYIhAAAAAAAAAAAAAAAAGwzBEIAAAAAAAAAAAAAAAC2GQIhAAAAAAAAAAAAAAAA2wyBEAAAAAAAAAAAAAAAgG2GQAgAAAAAAAAAAAAAAMA2QyAEAAAAAAAAAAAAAABgmyEQAgAAAAAAAAAAAAAAsM0QCAEAAAAAAAAAAAAAANhmCIQAAAAAAAAAAAAAAABsMwRCAAAAAAAAAAAAAAAAthkCIQAAAAAAAAAAAAAAANsMgRAAAAAAAAAAAAAAAIBthkAIAAAAAAAAAAAAAADANkMgBAAAAAAAAAAAAAAAYJshEAIAAAAAAAAAAAAAALDNEAgBAAAAAAAAAAAAAADYZgiEAAAAAAAAAAAAAAAAbDMEQgAAAAAAAAAAAAAAALYZAiEAAAAAAAAAAAAAAADbDIEQAAAAAAAAAAAAAACAbYZACAAAAAAAAAAAAAAAwDZDIAQAAAAAAAAAAAAAAGCbIRACAAAAAAAAAAAAAACwzRAIAQAAAAAAAAAAAAAA2GYIhAAAAAAAAAAAAAAAAGwzBEIAAAAAAAAAAAAAAAC2GQIhAAAAAAAAAAAAAAAA2wyBEAAAAAAAAAAAAAAAgG2GQAgAAAAAAAAAAAAAAMA2QyAEAAAAAAAAAAAAAABgmyEQAgAAAAAAAAAAAAAAsM0QCAEAAAAAAAAAAAAAANhmCIQAAAAAAAAAAAAAAABsMwRCAAAAAAAAAAAAAAAAthkCIQAAAAAAAAAAAAAAANsMgRAAAAAAAAAAAAAAAIBthkAIAAAAAAAAAAAAAADANkMgBAAAAAAAAAAAAAAAYJshEAIAAAAAAAAAAAAAALDNEAgBAAAAAAAAAAAAAADYZgiEAAAAAAAAAAAAAAAAbDMEQgAAAAAAAAAAAAAAALYZAiEAAAAAAAAAAAAAAADbDIEQAAAAAAAAAAAAAACAbYZACAAAAAAAAAAAAAAAwDZDIAQAAAAAAAAAAAAAAGCbIRACAAAAAAAAAAAAAACwzRAIAQAAAAAAAAAAAAAA2GYIhAAAAAAAAAAAAAAAAGwzBEIAAAAAAAAAAAAAAAC2GQIhAAAAAAAAAAAAAAAA2wyBEAAAAAAAAAAAAAAAgG2GQAgAAAAAAAAAAAAAAMA2QyAEAAAAAAAAAAAAAABgmyEQAgAAAAAAAAAAAAAAsM0QCAEAAAAAAAAAAAAAANhmCIQAAAAAAAAAAAAAAABsMwRCAAAAAAAAAAAAAAAAthkCIQAAAAAAAAAAAAAAANsMgRAAAAAAAAAAAAAAAIBthkAIAAAAAAAAAAAAAADANkMgBAAAAAAAAAAAAAAAYJshEAIAAAAAAAAAAAAAALDNEAgBAAAAAAAAAAAAAADYZgiEAAAAAAAAAAAAAAAAbDMEQgAAAAAAAAAAAAAAALYZAiEAAAAAAAAAAAAAAADbDIEQAAAAAAAAAAAAAACAbYZACAAAAAAAAAAAAAAAwDZDIAQAAAAAAAAAAAAAAGCbIRACAAAAAAAAAAAAAACwzRAIAQAAAAAAAAAAAAAA2GYIhAAAAAAAAAAAAAAAAGwzBEIAAAAAAAAAAAAAAAC2GQIh2DLqkv656vrbquqx+FZPDQAAAAAAAAAAAAAAty8Ou2PL+C9tRr+gRJL0N92Rvj6RqnSLpwkAAAAAAAAAAAAAgNsRFUKwZVyypvn3mDV6JncLJwYAAAAAAAAAAAAAgNsYgRBsGW9I/LLrT8ZmnSEBAAAAAAAAAAAAAMDVEAjBlvHgikDIN3IEQgAAAAAAAAAAAAAAuBEEQrBlHE+lzpbrT1EhBAAAAAAAAAAAAACAG0IgBFtGJOlNLYvkxcholCUUAAAAAAAAAAAAAIDrxuF2bClvXrFIUiUEAAAAAAAAAAAAAIDrRyAEW8ojKxbJbxAIAQAAAAAAAAAAAADguhEIwZayMhDyZO4WTQgAAAAAAAAAAAAAALcxAiHYUvbKaG/L9adjo+SWTQ0AAAAAAAAAAAAAALcnAiHYct7asljOWaOv5mgbAwAAAAAAAAAAAADA9Yhfj5GWy2V97GMf06c+9SlduHBB7e3tuvfee/VjP/ZjevTRR29onM8++6w+8YlP6Mknn9T4+LgKhYKOHTum7/7u79YP//APK5/Pb/KrwK3yvYr0/8o1r/9pwegddX8LpwgAAAAAAAAAAAAAgNuL8d5v6pH2xcVFffCDH9QzzzyjXC6n48ePa3p6WiMjI5KkD3/4w/rQhz50XeP8xCc+oZ//+Z+Xc07FYlEHDx7U1NSUrly5Ikl64IEH9Bu/8Rvq6OjYzJeisbG5TR0frm1wsFPT8hryZdVNqAwylHo9M5kqusXTBgB4fQ0Odkpi+wsAOxHbAADYmVj/A8DOxTYAAHYutgEb05hPr9Wmt4z5N//m3+iZZ57R3Xffrc9+9rP6oz/6I/3VX/2VfuEXfkFxHOtXfuVX9JWvfGXD43vyySf1cz/3c3LO6Sd+4if09a9/XX/yJ3+iL33pS/rEJz6hoaEhPfPMM/pX/+pfbfZLwS3SI6N31pZySlcio6/nbuEEAQAAAAAAAAAAAABwm9nUQMi5c+f0J3/yJ7LW6t//+3+v3bt3N+/7/u//fv3ET/yEJOlXfuVXNjzOj3/84/Le613vepf+2T/7Z8taw7zlLW/RL/zCL0iSPvWpT+nSpUub9Epwq31PbXnhmj/Nb3p2CQAAAAAAAAAAAACAbWtTj7J/8pOfVJqmeuMb36hjx46tuv/v/b2/J0l66qmnmi1kruWxxx6TJL3//e9f8/63vvWtam9vlyQ9//zzNzLZ2ILeW/WKWroZfaZgbuHUAAAAAAAAAAAAAABwe9nUQMjTTz8tSXr44YfXvH94eFh79+6VJD3++OPXHJ9zTr/4i7+oj370o3rTm9605jC+JTSQpul1TjG2qj4vfUt96fr5yGiWTAgAAAAAAAAAAAAAABsSb+bIzp49K0k6cODAusPs3btXFy9e1JkzZ645Pmut3vnOd151mC996UtaWFiQJB0/fnzjE4st767U62taSoG8GkkPJbdwggAAAAAAAAAAAAAAuE1saoWQiYkJSVJfX9+6w/T09EiSpqamXvPzLSws6Od+7uckSffee6+OHj36mseJreN46pddPxFRIgQAAAAAAAAAAAAAgI3Y1AohlUpFkpTP59cdplAoLBv2RtVqNf2Tf/JPdPr0aUVRpJ/92Z99TeNby+Bg56aPExszONiph5VKqjVvu9RV0KByt26iAAA3BdtfANi52AYAwM7E+h8Adi62AQCwc7ENuDk2tUJIFEWSJGPWr+Tgfaj6YO2NP3WlUtGHPvQhffGLX5Qk/bN/9s/08MMP3/D4sDXdpeXL0Utyt2hKAAAAAAAAAAAAAAC4vWxqhZBSqaSZmRlVq9V1h6nVQsWHRqWQ6zUxMaGf+qmf0tNPPy1J+kf/6B/px3/8x29oXNcyNjb3uowX62skwcbG5lSU1DYQqZwFjF5IUo1N8Z4AwHbVug0AAOwsbAMAYGdi/Q8AOxfbAADYudgGbMxmVVDZ1Aohvb29kqTp6el1h5mampIk9ff3X/f4T548qR/8wR/U008/LWOM/sW/+Bf6x//4H9/QtGLrs5KOpEvXT0dScsumBgAAAAAAAAAAAACA28emBkKOHDkiSbpw4cK6w1y8eFGSdOjQoesa92OPPaYf/uEf1sWLF1UoFPQf/sN/0Ac/+MEbnVTcJo4nvvl3zRid29QlFgAAAAAAAAAAAACA7WlTD68/8MADktRs57LS6OioRkZGJEkPPvjghsf7+OOP6x/8g3+g2dlZ9fT06BOf+ITe+973vubpxdZ3NF1+/WRsbs2EAAAAAAAAAAAAAABwG9nUQEgjpPH444/r1KlTq+7/7d/+bUnSI488on379m1onOfPn9dP/dRPqVKpaNeuXfqd3/md6wqT4PZ2PPXLrp+IbtGEAAAAAAAAAAAAAABwG9nUQMihQ4f0/ve/X2ma6sMf/rDOnj3bvO+Tn/ykPvaxj0mSfvInf3LVY8+dO6eTJ0/qypUry27/yEc+orm5ORWLRf36r/96sy0NdoaVgZCTERVCAAAAAAAAAAAAAAC4lnizR/iRj3xEr7zyil555RW9733v0x133KHZ2VldvHhRkvTTP/3Tetvb3rbqcR/84Ad18eJFfeADH9DP//zPS5Kee+45fe1rX5MkFYtF/et//a+v+tz/8B/+Qz366KOb/IpwKx1Jll8/QSAEAAAAAAAAAAAAAIBr2vRASG9vr373d39XH//4x/XpT39aJ0+eVBzHeuSRR/SjP/qjes973rPhcX39619v/j09Pa2nnnrqqsNPTEzc8HRja2qXtDf1upgFQV7d9CUWAAAAAAAAAAAAAIDtx3jv/bUH25nGxuZu9STsOIODnZKWz/u/0231xfxSd6OXxxP1stQCwLaz1jYAALAzsA0AgJ2J9T8A7FxsAwBg52IbsDGN+fRa2WsPAtxax9Pl11+Nbs10AAAAAAAAAAAAAABwuyAQgi3vWLK8HMjJrH0MAAAAAAAAAAAAAABYG4EQbHnHVlQIORETCAEAAAAAAAAAAAAA4GoIhGDLO54urxBygpYxAAAAAAAAAAAAAABcFYEQbHm7nVTyS6EQWsYAAAAAAAAAAAAAAHB1BEKw5RlJx5Kl66cjqX7LpgYAAAAAAAAAAAAAgK2PQAhuC8da2sYkxugcbWMAAAAAAAAAAAAAAFgXgRDcFloDIZJ0grYxAAAAAAAAAAAAAACsi0AIbgvH0+XXX6VCCAAAAAAAAAAAAAAA6yIQgtvC0WR5hZBXqRACAAAAAAAAAAAAAMC6CITgtnAklYxfCoW8EhMIAQAAAAAAAAAAAABgPQRCcFsoSTrglq6/GElu3aEBAAAAAAAAAAAAANjZCITgtnFfS9uYBWt0OrqFEwMAAAAAAAAAAAAAwBZGIAS3jdZAiCQ9R9sYAAAAAAAAAAAAAADWRCAEt437kuXXCYQAAAAAAAAAAAAAALA2AiG4bayuEHKLJgQAAAAAAAAAAAAAgC2OQAhuG8NOGnRLoZDnYyN/leEBAAAAAAAAAAAAANipCITgttJaJWTcGl1mCQYAAAAAAAAAAAAAYBUOp+O2cl+y/Ppzsbk1EwIAAAAAAAAAAAAAwBZGIAS3lfvqy5vEPBffogkBAAAAAAAAAAAAAGALIxCC28q96fJAyLNUCAEAAAAAAAAAAAAAYBUCIbitHEqlLrcUCnkyNvJXGR4AAAAAAAAAAAAAgJ2IQAhuK1bSQ8lSBORKZHSepRgAAAAAAAAAAAAAgGU4lI7bzpvqy68/kaNtDAAAAAAAAAAAAAAArQiE4LbzLfXlTWKeiAmEAAAAAAAAAAAAAADQikAIthSf/buahxMv45eG+ToVQgAAAAAAAAAAAAAAWIZACLaMC1Z6UFV1qqL/1LZ+yKPLS3emS9dfiKXFmzB9AAAAAAAAAAAAAADcLgiEYMv4r21Wz8hrQdL/0RHpmXj9Yd/U0jYmMUbP5F7/6QMAAAAAAAAAAAAA4HZBIARbxqBb3irm/2xff/H8lvryYb8e0zYGAAAAAAAAAAAAAIAGAiHYMn644tXbcv0Leasv59YOerwpWR4IeXyd4QAAAAAAAAAAAAAA2IkIhGDL6PbSz2h5n5j/s93KrzHssVTqb6ko8rWcUfo6Tx8AAAAAAAAAAAAAALcLAiHYUj6sWLtbrj+RM3oyXj2ckfSWlrYxs9bom2sMBwAAAAAAAAAAAADATkQgBFtKSUb/m3LLbvtY29qL6dtry2uHrNdeBgAAAAAAAAAAAACAnYZACLacH1ek9pZ2MH9SMBpdY0l9a315IOQrBEIAAAAAAAAAAAAAAJBEIARbULeM/m51KeyRGKNPFFcvqnenUm9LcOSxnJG7KVMIAAAAAAAAAAAAAMDWRiAEW9LfLy+Pdvx20civGMZKenNLlZApa/Ri9PpPGwAAAAAAAAAAAAAAWx2BEGxJx1Pp7bWlUMhIZHRhjaX1bSvbxuRpGwMAAAAAAAAAAAAAAIEQbFlvrS+//mRuddjj7SsCIV9aYxgAAAAAAAAAAAAAAHYaAiHYst60IuzxZLw67HFPIvW6peG+nDNKXvcpAwAAAAAAAAAAAABgayMQgi3rwWRFIGSN6h9W0jtqS8PNWaNvxK/3lAEAAAAAAAAAAAAAsLURCMGW1euloy2hkOdiqbrGcI+uqCTy13naxgAAAAAAAAAAAAAAdjYCIdjSHm4JhFSN0QtrVP94Z215IOSLORZrAAAAAAAAAAAAAMDOxpFzbGkP1a/dNuagkw6lS8M9kZPmKRICAAAAAAAAAAAAANjBCIRgS3tTsiIQEq+d9Hi0pUpIYoy+skZwBAAAAAAAAAAAAACAnYJACLa0uxOpzS+FPf4mZ1RfY7iVbWM+nycQAgAAAAAAAAAAAADYuQiEYEvLSXpLS9uYK5HRX6wR9nhn3StqCY78Zd7IrxoKAAAAAAAAAAAAAICdgUAItrwfLS+Pdvy3ttWBkG4vPdJSOuRcZPRq9HpPGQAAAAAAAAAAAAAAWxOBEGx57615DbqlUMgXckZn1lhyv6Pmll3/LG1jAAAAAAAAAAAAAAA7FIEQbHk5ST/SUiXEG6Pfalu96H5HbXklkc8RCAEAAAAAAAAAAAAA7FAEQnBb+NGKk/FLgY8/Khj5FcPclUr70qVbv5ozmiMTAgAAAAAAAAAAAADYgQiE4LZwwElvry+FPc5HRs9Hy4cxkt7dUiUkMUZfyJEIAQAAAAAAAAAAAADsPARCcNt434qWMJ8urF58v3PVMARCAAAAAAAAAAAAAAA7D4EQ3DbeW7122OMdNa9SS2uZz+aN6q/7lAEAAAAAAAAAAAAAsLUQCMFtY7+T7mtpG/NCbHR2xRLcJunbW6qEzFijr9A2BgAAAAAAAAAAAACwwxAIwW3lfTW37Ppn1qgS8rdWVBL5nyuGqUmaMdLyMQEAAAAAAAAAAAAAsH3Et3oCgOvxvqrX/9W+dP13C1b/oJyqNfLxnTWv2HslJtz6m21W+1KvL+aNno+NJmy4PfJeJS/VjNTupfsTrwcS6UjidTj1OpxKQ16ivggAAAAAAAAAAAAA4HZDIAS3lTek0rHE69U4xDSez4WWMG9vaSXT7aVvrXt9Ib8U5fhoR7RqXKkxmssGqRrpC3mjL+SXD1PyIRhyMPXan0rDzmvASf1e6ndee5w05Ci1AwAAAAAAAAAAAADYWgiE4LZiJP1/y04/07kU8Pj1tuWBEEl6f9WvCnfciEVj9EIsvRCvXyck772Op9I9ide+VBp0XsNO2u287kqk9nUfCQAAAAAAAAAAAADA64NACG47P1Tx+rl2r+ms9cuf541OWemIWxrmhytef1Zw+kJ+ee2OXanXkVRq916T1qhspIKXLlrpSnRjzWFqVwmNWO91MJU6vdTjw3MfTb2OpdLe1KvHS91OaruhZwYAAAAAAAAAAAAAYG0EQnDbaZf0YxWvXy6FAIY3Rv+lZPVz80uJkLyk35txumCdXoqNJox0f+J1VxqqjKzkJY1Y6URkdDqSTkdGZ1ouq+bGwiLOGJ1ufsqMvrjOcAXv1eWlHhda3vS47Hp2W4/32uWkIedDexovtfsQNOnILgs3NIW3r1TSvJHmjFSX5I3ksttd43/2tuV9CP4UlF1mf7MCBAAAAAAAAAAAALBdcTwUt6W/X3b6T21GSRbU+J2i0c8shABFq31O2lfza4xhOSNpr5P2Oq9vq0shIhI4SWNWOmelcWs0YaUJI41ZowtZaOSVSEpvMDQihcDJmAnPszRF1yfnvXKSqtmj23z4X5SUy15Opw8tbWpGmjVGRlKU3R/LK+elnMKKIdwmWYXARS0LXkSSBrPgStzyuDh7XLxiHJ1e6vJeZWM0Z8K0WZ9dKszpxv8wXSHkMWuNZhp/Z9fnjDSTXZ+3Nz6/GyLvFSmESIpe6vdSqWVx8S2vvWqkJHv9+ew1rr4M70HOh9fWmM+pwnjz8op8GEfjrW4EWNIs0OIlpTLN2xKFeVA24TGRpKjlvWn8HWX/U0m1lmlu/F3MwkOm5fW2Zc+dNP5nz1c3YRgpPKbdSx3eK5U0acO0lbxU8j67DP/rRlrM/pfVWL5CiCnOprMxf1qXsXzjMpvfMya89qLPpjObZ0ZSp/fqddl7k1X4abxnLpvuxn8rqc9LHc7LmzBfG/c5E5bDvMJnp5BNTz6bntbLxu2t9YZ8y7xOss9Gko2zJ1seAAAAAAAAAAAAgFuJQAhuS7ud9H1Vrz8ohlDAojH6f4pGHy5fO/xxvaykYRf+twZFWv+uSDobSVes0RUrjVrpZGT0TGw0EjUOkr/2AMPV1I1RveX6nJHm1hxyvel4fadvK0qzsIYkLRhp4TWP8Vrz8Haex2tN+0Zez+38mpeLs8BPCM1c/XW1eR8CMVk4qPF/rdusQpCk8XesEMIpeq85Y1Q1S8GYNnnl/VI1nDQLwTSCRKkJ489lw60dXFq6TExomTVnpINO2p+G4E9ipJpMM+ximo9bGm8uCxh5hYBNI9DUeqnssf1e6nUhwNMawin6MP3T1qgmqV+pYklz8fJ5ZrN50uWXgjilLKyUmLAOrmXzoi27r83zJQcAAAAAAAAAAOxsHCvBbesflp3+oLh0zv7H26z+YTm9JWfmFyXdmUp3pmsHRqRQ1eJkFIIiJyOjMRsqIcxYacYYTTf/fm3VRgC8PhJjlGxw2HLjM/yaPso3GsJ5zU98C9XCRe/mfD2JvQ+VZhQCIoWs6syslS7ZEHLZ78J9jYCNk5qVfGKFOVlVCJ70Oqnde82bEJbpd17tWUAnaQnnNKrfRFqqjtOoxtRazSdteVyjak+bD1V1JoxR3Ui70/Ac49aoYhqVa/yySjZdXupz0pQNAR+jEIoZ8FK/U7O6UkEhRBMCMz5Ml1+anjkbgkDhdt+s6tOoRBSv+NtmlYFqCtWEuv3SPFN2ebsuiQAAAAAAAAAAbAcEQnDbeiCR3lrz+mo+HG4aiYz+rGD0germVwnZDF1eejCRHkwaDVLW5iXNG4WAiJFmrNGkkS5F4QChUThouaBw8G4+a6uykFUIyWfjqBiprNBqpNEiY8YsVTYo+DANdUnuNgigWO/VmR1w7PRSl5O6vVeHDwdEWysJRFqquOCVtU4x4aBuNbusGaliQvsTo1AhZMKEg5vS0kHM2KvZTiSnrDpBVjWhcSC0cblWkCeXtaWpXOc8NtlB4Ujh9TYOOjdbomTva6Lw3jbavjRa2hS0vPVJxYRlpTGPylnVmsZB39ZWP40D2C6bL/MtIaWSDy2CFs21q2QAt1pijOaNNL/O/XUjvWTXuXNdZp2/cTUm2+YYLQ+KtF7v9KEaV03ShA3rINtyf2slnUghRBMpbC/LJgR7GoEXq7BtjBXawbX5sC5LZJZtgW2jrZVWtONSWBc2nq81wOO1vNJOow1Ya7Ufo7UrAS3d7peG8xsZ/tq3X/+4vMwaFYsaw6113/Xc3jqexIRgrFVoO1dYf1FZk5O0qKw6kZa38AIAAAAAAAAArI9ACG5r/7+y01fzUfP6bxW3biBkoxoHxTq9tF+SrlJ15Hp5hQMy+exM9AanLOCgEDSoS81WEU6NFg/hsiJp3DZCAlJdoWpC0vK4REuBiRkjzRmjUhbeaExHo8VE68HAODvTvdtLnc43/+7KDvJt9UOvqZbmgVc4AFdSmO7Gfakkn4U5mgfnVhzkbBxIe735DT6PV3jfjUKlhYa6Gu2Qwv+45WBsY/lqHkTV6mWrsYy0XlpJPT4cmK2Ypf+pQnhn1kjTdunAbTV7bpMtH43qCzZ7zikbwlKNNi2RfDMw5BXCPTUT2pXUVgR8ai2XdUnVrEJII3QTyzdbp8TZZd1Ik8aonC3fTmFZb7RQSVtua22t0rwv+/zMZSGcYlbhompaqo5chfX+tgh4Yefx2XJ5ta1Y1YTty42aWef2V666puPzcivks7ZaK8NBDaZlm+8lLdjl71Pk/bLvJo3QZi4LT5ZNWFd3ZdfnsjBKKQvwNMbbuDQK269O77MWXCE85EzYXkRZgKexnZZa1vHZ42NJVZlm+HKpRdfqFl6F7MlnbAjv5rXUGiynpe1D6//WbcnK+1o/V2vOzzUu17qtMe+7vLTLhRZiM9Y0w1mtwZ/wHcYvDwBl28FpY7SQfdeJFSoIFf3q17DytRRVU4+M8m0hAN0Iu6bZtjGVaT5npOXLyUZe31Vv82vPt0aYycmoPQsnd/jwvWDGLrVVW/O/X/5cVuG9NlmwtmayylUKl4Xs/TcK3z8a35d8yzjW+tysfB2t9zU+D4lZCvw2vre0BoLrCt+3YoX3qpi1dls00ny2/m5UlGqE5Nyy9yYLy2XjL2TvXety0Aje5RQ+UwtZaLOYzdNUWYg7G2+PC5/JuawqV7cL0zRnjOaa70v23VMhtBxvoZ+BjeW0saw6s/x7scu+3y79HdYRvX6pPaBT+N3Qur5ZfruRkVeXC5/b1oCi/PKA3rSVzlmjmml8X/fNdWJzPaOlz2UjZNm6WV73V+mKTenKtyHOlvOywvq4EeosZq+5ZpZOZqiZtT9HywKiK/7WOsNIS8tUo53gTNYqsfF7uM2HdVc1m89t2e2t24pE4YQCb5a/NtvyWYqU/c5ReF2t86011O+z+xaz7zz1bNr6fPj97yVNmfBZ6riO37+t27SV24aV13MrXp/Lplu6yrplxfVbySt8/iPdHvsIblRdYbkriSAsAAAAgNeOQAhua++pee1NvS5GYTfAl/JW563TfneLJ2yLMgoBi5Wswo7bgrR8z9EauiQNpa23bGTP6xbaO/s6auz0XevlNu9b5/5bYaM7z4yWB4gacgrL01rLVENjJ+lGlq2rey0z7VqP3SJvSIvGzufWFlhOYUd53SxVwWkNwDR2gjce2xpuWe/SS9rjws7Uk1FoS5JT2HGfV2i50piG8LhQiagRkmkcSLXrXWYHByds2LmdNB9rmmEfKbQ7Kcir2F5QIml2sbbsgGGqsOO8sfM3l82LBbN0kDWfzYuKwsGvRhWcqsLzVLPbKgrL85403H7BhgNlkdQMDjUOcjV23hey2+daDkoTvgFuXO1an51r3J0a06yCdjUXr2uqrvbEr+Wzznpi47IvmB3R1QcDgNdZIQsuhiDI9a/Hc95n3yc3/tg275dVnYyySpPSVYIeN/hdNJcFIJPX8F3WZPNI2lhYpPX3SiPs07geqRxu64/WvL/1cRM2fMeXQsC00PJTrnUa1vr7Wvc3fnfkFH4fVRROMMj7cNJCrNVBm9a/17wtG3nsQ/gn1vLfY3mF2xM1fr8sr8YZea9eH36nbOS1rfWerHxvVg3bEooMy8XSNLaeyNIaDFpZFW/lfRud/9fznl11uJbX0PhduPKEnUZ1waTltTVCWnHLb+uoed1nv0dNs7Xo1aZlzesr3rfw32ehTdNsH9paGbHxu9RcZT9F1ZhmMLLHh/WHtHqXS+NklmlrNGHCvJAaLUf9qoMCy+btOsvcyuuV7Hd3CPtlLVv90gGH+ex3eCMU3ZYtHyHcaTSXVYbs9CH82enDcI150/hMlnz4bEybpfc575cqPLYuC63Lo1GYHqOl3w6NipKpwrLQCLcmJjx3yYdh57Phm8Fwv3z/S+s+hMb4FrP3pXEyVdYUd1U13nrLc9VN2N40grVlE5bdHh/m6ZQN+zW6sve6mlU7zss3w71GLScgaSls6rRUGbk15N56Ylrr62vMR6+llrohsBqmv7F+SEwYvqAw7WWzNE8alTYbgcnWlrzSUoCy3liGlchImi+Y5jqkMd8b65iVFSgb465k8661Ra9R676noBHAznuvijGatmGY1ja63T60Bc5lj5+wocV7431pLCshVOqbge5iNr2N/VBlEz69OflmGDrMI5Mtu0vvW2PfW05LodCF7KTKfDavIkndLgzX2EaUTZimPrf8xL3Ge7fSyu3Usm2WWX2bWfG+NaplV7P3rdtJu9xSeH42m6aSD9vPvJZXYF3reWsm7NOLsuUr51uX26V1dJvCeKstr79ilqpkD7mw/pjLHtvbsq5fMMsD6q2vadnfWh6+TFueq6bw2cv78J2govAaO3wIbUqhivuCDc9V8qEqaqTlJ8Iqu96Yh0bLv+u0vncrL/PZ/5UaQfPGtmytbXLrMtC6n1Va+syuta292Rrv50amxWvp+8lar/Vmav3MbPY0rFx+gM1AIAS3tUjSD1W8frF9adX4u0Wjf7q4/g8nANjqjJaHQaTwRb1dumZ+pfHYXOuwG1gl9icbGXCz1q1rj2ewPbzqsYXKJj3P5qor/MhtnI04nf1AzGnpx6S0/Iz2ZdVxWm5v7Chp/ECumtASo2zCDsfebAfdRRvCMwNOas8OFNSMaVawqWY7xSZtVlkqDeNcMNKYDa3GOrxXjw/DL2Q77JrtxLJpMgqtuPLySmWaO8QaO8dW7ihr/N0I5MzZUBFq2Y4FST7blem1/MzalcM5hR/Xo9lO9wG31AJtZTWdxk6CSjZfu7OdDo0f7AvZ83T48GN9zvLzCQAAbFz1NYZ+b6S15spqgKkxjZjcptuM1p/eXO1w9QasNQnX+Z2t8Z349TDR8nfdhIM9t0JqjMb5KnuLvF4zfqPjvZ7nv5FpfT1e32sdJwv7zpTFNroIheO1M9mJbdc8CWWNx8UKa6GNPrYRjl15sliUtYGvGyObVV5M9Nq+fzWqITYrtGtjwWDj/YaGa0yztEaAZ43HW++XBcoawYXWsFlrQKX19lhLVQIbreqr2bxqBPYa+/6kpQBLW7YPddas/q4eZ5XpC355ZdJl/81SZdaqCfsLi1oK5jZCZPU1Zlfr/tvG32UTQlyN99+smCetr92sc/vKQGGahb1SSfPWyHivdr8USm78b1SMbLxf7V7qdEsBrIbmPtgVr6k1jLQySOS1FKhNtRQSrprwvv3tqtfPzTu22Lcp473nyPk6xsbmbvUk7DiDg52Srm/en4qkt/QtZZsOpF6PT6aU1QSA28yNbAOAa5nNftR1ZGcwtWot494o298IuzT+dlo6g6T1rKbGmZ3JGj82r1Xif9ltVxteSy2mrnW7v67hzZq3rwrgrPEc17rdr3F7pBBYSiRdsVLFmGU/ZiWt2ukgLf1o7cwCUY1AVW1FtaJa4/bscaXsPZwx4Ydtowx/IywkrT5TZ96sPlP6RioBlbKfVmEZ4ScyAAAAAAAAtoc/mE71jvrmxAo4FrAxjfn0WlEhBLe9I6n05rrXY7mw0/1cZPTlnNm0lRIAALh9dV3l60Ajjd+syLNWlH49N/w1Yyt8P9kK07C1eIXyrY0zVlqDxY2AS6OqjtfyctFOIQBSUEt7OC2dWbGqZVcWqOlyWflbo2YbrbpWn2XTKH291hkctvFEUjPhsjJos/Lyarc5hYpDo1kLse6sWtGywFAWbFoZhFI2Pb3Z2TmRD6+3Ecpa7zU0/u7ubde0vM7OhNYBjbZzK8sNt74PrWfFbOT1Xe22Zfdn47U+rEOsQqBoLjuDKZbU45bKO7eOc1XQqSUoVc8uSwpn/tSyM5Oq2Rk3jXZsoW1bGP/KMstXe67W52xMf1s2D1urVTXOLGtUgIoVzqZqVF+qZC3XSl7qyEJOS63UjCL5ZeX1l4XlFF6XUQjitZ7JtlQ9K5TCbs+W/3ljFGup9LZRWA7njVGn91mJZqOaQsCr04eS8pGy6lAK1a/SLZLBarzfraXbG+uM1lLqkfyy62UjTZvQQqC1nPvKM8oaJd0b658ZG5bNq4X3Sl46mIZw3aIxWcWwpeVm5Vl8iwrLfKuVs9ds4D6vsIw1zsDrdGG6Klpq6ddoIVBUOOus8bjWkGIjONl6m1pfr1n9GCkrDy81X29PVla81jINqcL622ip9WCzJYlCeflGW4LGa2s8z1LVtLAMR5KqMqqa1vYCfllJbp+9zkG3NG0TJqx3C5IGnVei0OKhdf1kVvyXVrZG8MuG04rLxt+hNYlpOcPRL9vmta5XWq83GsQ0r5uV969/bn/rMKsCqEaKYhvmZ+LWvL/18R1e2pt6eSNNGqOk8Rxm+fO1Xq41PWv9LYX3qa6wri9I6nVheZk2q0t3r3o/Wm/3y++vtXzmcn5pPV83Wcl8hc/p0n/fbPUxbdZ+TWttA9ebBxu9lJamLa+l70PrvXeN7f/Kin6rpmON90drDLfR17PebY3PZV1L3yPibIB6dnujTUPj+38jHN7YzoWQ+PKluXHWb+utK1/H1a6vd3a2zVpDRVr6jpNeZfiGyHt1+6WqlddqT2W9V1+2rm1UZlzcxOBy7P2GpnstHS7Mg1lzY48HAAA3Vyc1Jm5bBEKwLfxwxemx3NIu+F9ot/rW6ZTSRQAAALgmo9X9lxtaS5uuJdLqNl+NcTb7/V7l6FTHyqNvN2qTfpMfcRsZ2ebvABjMDjuN1di5cOtd7T14PRbWGzlse61x3q5uxuvYLvMK28XgYEmSNDbFmYHYGhrhjNag3GaN9//f3p1HyVXX+f9/fWrpfUl3SGIWQpMQlhmWAOfHF37qiOgMOoP5GpSjaIjIoBCR4zIMo47bcMKJjDmOGhDGRI/OAqgIE5wcUQbQE0YlX4QQcPySH1uS7qyk97W66n5+f3w+n6pb1Us6SadDV56Pc/p0d9W9t+76uVW3Xvf9CT/jTXcirXQ8eJeLPRafZlYueFdji0PLkgtCj/X2dLyzb9H/xgXcQlefw/LBUhXCXLU+iBKeG/LBm3rr3gOH+YrkwlFdPmgXAj0J67tXNW46TT4IM6RC4Dq+HPm31aawDjJyoaQq654L4awQGAq/k3599fnXqreFUv6FsE7x/yEYlZMbts6PM+DnKVSoDOX3Q1gpmV8uo7TvUsKFx11oNiWpw3f12hRZVUvqMi4c68JaVsMy+aCTVAiexrshcGFEK1tSpbK04mNOxSG8ED5N+u3WbtxzaVsIVeX8tq6UW7dSccCqtPJmeI203HRC4LuuvkqRpO6ewULIzIwMosXDaCGMWW2tKmKvO2wKwcvQLYU1bn9x+59RhbVq9sHuEN51wT+jg4nC/tRs3boPx1GlX68Z48KfIdAfjqVq+YCrdWHQrEx+m1daqVrWHyeF7ZZVoXveSNIM68LYIZyd9vtOV8L9jgcHh4wLUIftHzda21IaLC0NLsZDjWGdh+BrUoWQbcpKBxOu8meVP44b/HEeuvCNdzUclUw7/FRY1z5kfegyvE7Y7xJ+Hgf88V/p12+NX8dh3exPuBBnOF4PJlw10VofPg/tX74yrBl9P8357RVJqi4Zt9u3XTMj99p9Rur14XVJqo/cDRG11nWnvMeHeiv8cZLxy1Tl99cQgI4fj/l1ZIu3oZHbzzp9+D0cOylJaR8qjG+zMN3QfXNkQptgR2zrME5OJt81c+l2Ct2dxFmpqMrtiGPVjPG4CuHLIb8sdf6n1lplfGhdsdcNbZmV2xcyct1Gh5sWQps/aIx6TfENNkXB+nDuCPuT348GjZtuaC9C+xZf5jDfkQo3V2Tlxp9h3XbNVwIeo+0araJu6favte61M6ZwPpHc/h22bel2Cdu+17gfxYYr/Tu+3cNP6JYnPl/5mz5s2M/c8Ze2Lghy1ZDV0qwwTREIQVn434NWq2utDvo+Z7ekjTZVGF3BBWUAAAAAAAAAE2B0bC6Yx7+IO9RwhzPNseY1/sVjqdKuNI9IbNom9noNJc9J7ku2Gjvy8SB081g/2vOjPFY3xuPTT+lC2Ak+N9r/h/M6bxyz6t0efGDwcOfxSJZpvHEme32ONczRjAsAwJFLHHoQ4I2vTtItfVHRY7fVJTR0fGYHAAAAAAAAAAAAAIDjigohKBsrB62+V231Uspl6V9LGt1Un9Clw1bPpEy+VF4os3RyTmqJrFpyVqdlpdmEbwEAAAAAAAAAAAAAZYJACMpGWtKX+yKtbCz0jPlwVUIPj9UhfIkzslZLclbdRpoRSf8ra5W2LlhSa61OyUl7E9KBhNHZWasrh1zfa/sShbKGh1PWEQAAAAAAAAAAAACAY4VACMrK5RmrawYi/Wv14feG9GLK6MVUIdLx8CGGv1lShbXKGDdOjbU6NSctylk1RlKDlU7OWZ0SSedkreZE409PkjqNVGWlCWZYAAAAAAAAAAAAAAAYFYEQlBUj6Y7eSLsT0mOVhx8KOVwhDCJJ/cboDynpD6nR64Q0RFY1PuxRbd1PlZUq5CqN/DFptDdpZKzV/Eg6NeeqklTIdXGTk5SQ1BxJJ1mrmZH7u8a6vm66Eq5bnJacVYWV2pJSa8JoT0I6KZIuG7aaHUntRno5KbUljebmrM7PSsNG2m9cg1ApKW2lCuv+NpL6jZSwUl1Ybkkdxj2e9vNU6Z8r1GdxspJeT0gzIzdsTlKXkWZYtzyKTzMhJa00aNw4CUnzcu736746y0HjXqPOuunsTEovpIzakkan5qzenrFKSOoxUlPk1vdrCak94dZ3g5XeFLllbfUzcGpOmh9Z1UdSZ0JqSxjVW6szs26ZI0m7E1Jbws333Mitl6z/GfbrIvwkrXRWzm3jnQmpK+GGz//4ajKzI+mkksoy3UYalBQZqca6H/l5CD/W/8gU/u4N68xK8yOpz7jqNhm/TZqs2/5ZSQPGzVuNdeupz7j1UmXdNDrDjHqlvSlVWldFp8mvz3Yj7Uq6bZjwPyb2d8Iv82iPJ/18NIwShAoZqoTfb/qNlIoNl5XbjlTmAQAAAAAAAAAAAEYiEIKyk5K0vjvSLfXSoxVGS7NW1w5YnZGzGpbr2iUnaWfS6LWk9ErSaEva6NmUFJlj99Vyd8Koe9Rnil/TGqPWpNSaNNo8yfOQslbZkmVMWDvh5Z6fs6q20qtJKTfGOA2RVUtOavTVU55PubBMY2T1v4atnk4btSeMaiOrs3KuikqPkf67wmjgKNf/U2mj+ye5vEqltcpJI9bboRhrVSUdcpmaI6t5OalG0itJ6fXEiRtvmBlZNfkUSI8PuOSMUdLaov0tvs9WWJs/prNywaRK64JUCSv1JFywZFYkNfrhkpLqIqu0CmGrnCkEbpJy4ZhGKzVE0kspF65JWRdqGpKU8dV8knKBmmFJ1XIhpNNzVrX+OOk3RhXWhbTC6w0ZF7qqsW6cmlhALOOn1WCtGqz7O2OkjIyyPmzkwjRumlVyw3QlXNdWi7PSjqT0bNooJxe0qY9Ct1Zuvqyfjyq54NeAcT/9PmA0O3LLkZR77IAPas2L3Dz2mcKP8ev8pEh6U+Ta2F5jZPx6TMgqYcPf7ndSLiCUVOHxhJ9uVjkNSuquMKqx0gxrlfXTnGGt5ufcPARZv5/0GLeNG6x0wLiAVJXfjlV+PVfZwpuerKSDfrma/LxYvy07jVsvtf51rNzxeewjhgAAAAAAAAAAAJPLWGtLb/6Gd+BAz/GehRPOrFn1ko7Puu9VoVLCS0npmbSruLE4Z9VrjHYl3BeHexPSvVUJbU+5UMP/O2wVyQU4XklKw8cwVAIAOHIpa1VlXdAlhIqMtUrJh4PGab9rIxc+qrYuQBRCSDlTqBYU/1sqhGDyVXFs8f9J66oONVn3nJWrDhTCSTkVQkz1fj4zMhoyriKP9c8NGlddqMZKcyKp1rrAU9oWquiECj0pK820Un1k1Zkw6jCugtKQDzlVl4Ro3GM2H7AZMFKHMbIqVD1KyBZXB7LF1YCG5cJug7GwzWxfNWvAuNceNNKQ3DB11nWzNiSpPWGU8es7zEtPwqhfLoxUa63mRm66LyWNOhKFCkg11r1GrQ9kScUVj+J/V/vh6vw+0pkw6vaVsyK5dTQoV2GqyVoNyAWf6q1VpZWsKa7klFPhsRofLkuE503x9rV++cJjhWFM/v8qWb0p50JQrQmjKmt1es5NOwS0QqgrJakxcvtWnw98NfuKWVZuvcb3ifB7rL0/I1eRq0KuOleoBDYoF6yaEUm1Y4x7NHJ+fg81fygfx/NzAADg+KH9B4ATF+cAADhxcQ6YmLCejhYVQgCvTu6LDkk6JyedkxvrayNp1UBOrQn3xVtF7PGs3N30fUZ63Ui7kkYvpoyeS0n7EkYDvjuQQf8FTbzLmfrIamnWatAYvXqCV4oAgGMha4x6S5pWa4yGJzBuX8Ko7zBfL3zhnzdKs75/wlOb2Dnh5cMYFieW2si60Mg4waeEtSNCIkMlw9f40Ex77H1KjbVKHUbEPFQISvnQUkbuvVHo2q0jIR0seR+UsLaoy7F4d2QJuXFnRy7M1OsrBfWbMH+FMEyo5hSCVA3W/d9jpN5EoarSsA/uzIisKvw0I7kAVyT3vi4nFwoaMi5cFcI4Wf/aVb46U7okDBbv/i2nQiWoYeOenxkVKjpJhS7UjtSgcfM7aFy1t5P8MmRVCJ3V+ipVXb76U50PSVVbF4ja77u/CxWv0tZV5ErHHquQe5+7I+neC9f6cFaouNRtpO6E1OP3qUrfBeL8KFRhyqhPUnd9QrV+vLAOqq1UKathv54ycttoWIUgVFjPw377h+4Wk7Z4v0nKBddK96eE30f2JFyFsdpYla06vy0juap/XX7/qbXSwpxVxrh9tta67ZeV1JMw6vH7Qtq69VNhQyUzq6RcEM7KHUORD9wNmkI1r7D9Q2DQygXMZvpjzqo4kObGKwTWrNznkS5T2E7F281VHJMKocacH19y8z5g3Dpojtz2qpD0WtJ15Vjhg4NVskra4mnkTGE/j0p+hx9JavYhQOPX/7ApVGPLyrhxzPjTqZTU5I/V+DDxdVcI7blpGrmAX0Pkng9Bz0juGA7/S4V9Pun/TuaPC6O0pMbI5quNhWlXWxfEG5Rbh4N+eSpClTfrXqfDuCpw1dbmuxUNIUaj4kBc6Hay0s/Dft815YxYJbysPz7cPuCOjT6//JV+n8nE1nGoRhcqz1Vb93cIGtbawmfk0vWZ9M9Fxh03GSl/7Nb6ec34+Qnhzvh2i2/XKDZMCKTW+OMm7Cu25G/JHbcpufNJ0flBsdCjLV6Xxc/boi49TWwa/cao3bj5Sqq4LUkpHM9Ww8aFdQf8/pvy0woV+AZi549av50qrHRS5JaiM2GUje1naRW/k4sfR/l2wYy+r8fbivB4Sm5dRn6e3PnStWc9xsiWvC2ISn7iskbq9+tgjm+7dyfcsd8cadSQrFSoVrjHd2mbim3nSkl1yrllTrv1155wXZ82xNt+v34zfj4Sfrun/e8KufB3WoXHjNx7ih5jVOnbqpxM/vwnuccqfducNW7YlKwaI7ftO331w0rrJpj140dy7deMyAVkOxIlgWkrVftqiYPGvWeokJS21v/WiN8pubYivI8ZMIXzWNjPQ1i5K+H20XA+c++tbMn/PjwuqS9U2JTNr7fQrg2Z0P2tW7Z5kdWsyJ3nwmen0mMv/K6UawfC9a9KK83ygen4cVd6nJW2b+FzUwje5+TapJMit9y9phCCTqkQAK+UfMDdBabr/WsUdblbckzEH3fbqbA/mth8RH6dV8od3/0m1ubEt4v/e9i464GDfp9PyFWfzBnl97FKufa3N1Hoije0+YHV6EZ7POG3o/HbMbTvYR0Nq9AWDRqjGmvVHPkbBKyb156wz/ljJZI013ep3OVvHKiIzX9ObjtYhePOLUM436VVfHyH8014XxjOVVm5+cv5c1FoB5v8uarTv0alPz9VWjf9HlMIjTf65eiXWwaj4vfc8fNGRoXPBvW20O31WEzpA3b858f7/3CGzcp1Zd1n3Hw2+mC/kVvHGePW42hf6FgV1nd82Aa/j1gVjzcsumEeS2jPAQAoJ1QIGQeppKl3oiXCcvIffPyb9HiXBD1GaksUurBIyH0waE8YHfQfNDsTrkuS8IVFCJPk5C6uL8hZzY6kbWmjJ9PurezMyGphTpofWf3flNGLSaNGa/WmyF8gVOHC3ZD/oFVr3QfDP6bcY6dn3cXvaj//7Qk3b1bSnoTRrlillPk5q8U5q/+Tdl3CVFmrC4etWpNGO5KFt9ezIqv/Z9jmvyCa5b9Q2Z1w050VuZ+TfBcu3caowd8ZflrWamEk/S5t9IdU4QJ+e8J94FuQc3d1Z4y7WLA34T6sL8i5D5+vJN067fYX/xfkXDcU/1/SXdyTpDl+vXUbd/E1XGQJF27Ch9sa6y46PZdyF/RPy7kuYYLwgXTYSDuT7o72g8Z92Twrsjor6y4quYuQ7id+YXKsCynVfp1l/DqrlHRqzl14c91jGL3uP5xXWzfd8AHTbV/32EmR1GzdBY2xDBj34bwj4S6Q1VvXTVC4IDzqhXgVX4wP6yHjX7fdSG3Jwpf1NX55qvwXba6qgPVfshklrbvg1Wfcly3xizCD/iJIzrj9wEjal5B6E2bUbpMmojlyX0Rm5C5sVChc2HfLnZK7SBS6uQEAAAAAAACORoWvFtofu9ZU5bsENipcvy0N8o+mMbJqtMpfh5RcmC4E0+Jh70juumFOhVBavPLmaGHEUmGOjC35P/Z3MumuhttclL9m6bojdtfeam2s2qZi1TVN8WMhKBSuoRu564T5kFgsTJS27npkn3HLFa7pVshdi9+XcEGak2KVPuPLGZY/3BAQyV0PHDSF8Hm8m+Rkyf/xxxIqBOHioeTwd3g84eczKR9Wta5741AdNZI0LJMP/IaKqu4GBFMUvLJy13QH/XAVtjgsF5Y33HhRYQsh3WG5vytt4YbXSh8CC4H94htAbH45bcmPiv43I0LDGmP4rNx17EHj5j8eQqvyIetav0wdicJNECm//rIqHDPDfvxaa0uqtxaOp9J9O/5/abA2BO9q/EB9PsgWQrJh8Phylv4kVBzmD+uudH2MNw0pFoI0I6/Rp+Suu6et+45pwBTPX2mordEWbqzpMC4IGyrtN/mQc3y/zfrtGbZ/+P6kdF8oXaZS8YrL8e9GcnLfiw3LfbcRwvahanOYjxrrbhyqtOGGg8Lv4Um6fJ+2vt3yx4NVoR2Qn88eY5SUCxiHG0Uq/P444OcnhExTNoSU3TDzI7csB/z3USH8Gl/WIHxvlP879nj893jPF/1dMq1qKy3LRLp4IndWTtCJ9n3wkZqsCiEEQsbBTjj1aADKg1XhDVSooNIn94Z6TlQoLZ+RC1cMSzolKg7EnGjCm1Mce5FcCCWr4ruEwweWrNwb3IPG3REzLyfNmeCZclCuy6lhI7XkXJcNORWCVuEO5vidOoU7dtwbvZRc0CZ8+K2w7s64pA13sBr/Qc+Nm5TUaK1eN67bqkYrXTLs7uLtMe4u5x4fnOk1hTff4Y6RcAdmtXUfJPYbF46K/GuHcFYIqIW7xmv88P1yx/H+hPEf1q1/c24KH85N8Yf0+F1X4fEqSS3VFaqR1NM7pD5j1BkLMR1MuDBXuFtXcm9GXfcnbh66E+4De6MtVBwYNPHKTEaD/rXeFLkPxu3+rsyEpIbIaobfPn2mcBdNf1h/CRf8CR9iwl16pX8n5d40x5cxf+Ei9ljGh6s6/YUY49edib1Gwk9nIHZxJ23d3cXGr0dXhcBt7w6qSwEAAAAAAAAAJlGFtfpVR06n5Q497ETwffDE0GUMgDes8GVmXK2kRSU1ZyskLSitQ3uCIgwydRIqlL4ejUvsulDB4dbpr5J0dskbopDwrYm9ZiitO2Y92LGfGOe5kY+flF+Gqch+jpeZn5hZ1e5+kwMDg0c0/nRldehypOFujkqNH54LdwcNq1CBKp62zxh350qvMWqMrJp9yr/KuvEGjQu9hPL2g777gn65/6utCzql/N0TbhczRcEXqVB+P9zdELrAyBhX4WmfLy1fpcLdHNVyXWt0GaP9Cfdcs68U5FLzbl7qrFVt5Oa3K2G029/NsCgnzc1ZDRlXFjtUWQoBsKA0Ee9CPy4E1OtfozFyQauwTDN9en+PX3fVcl0xdPnS2kajdD/hd+F+46pLqeT5+J1DRsV3DxWGcdWJeowrsZ6UdHLOhX9eSrl5D11y1PiQUKjc1W1cO1NlQ1Ux44ezI+4eifzdG/Ey8+G5GmvzXRH0GDcv/caFxWZHVp0J6eAoJefHEqYbv5shbX05br+uGiMXIE379Zzfx8zIKlTh8R7jgmGhy4IQXgvbNyHXtvcZd4dGtQ9SdRrXtUeNtaqLXIhuwJh8FzVjVZVK+jv1hoyRsVZN1i1Pd8Ioaa1qrDtmhg+jalSltRqWFB2DSlNpX0a//w1QxSrp1+3hrBsAAAAAAAAcPxnjqv6fljtxrtuXEwIhAAAAx9FEvhIN/aEfSqgqkzfK+3OX4h75RM2oIaWJvME/3A8Bhxp+4qGnwxt2Ig53eafqAxAftCYirKXDjRmE8pxBqNpl5SqcDZlCWc4OP/GZPsyTUaEykOQCKqHSj5Xrwiyr4upA8QCR8WGY0H93JBdSGTTFXcMdzS6QkuuP3ch10dZtiisaZeVCM2kfhrRywZk+H6hqtNKcnHyIwy3zsHHLFv8/45dpYeSCZgP5oJV77QZf0alahRKn+xKuq7ukpPlNtaqW1HuwLz9eCMANGGlIRhWySlnly1pXxKYVQnBpW+jOLVSsC+u/UAK4UA45/nhSrpvBGZFb9lAdqscYdfv5CaW2663bH9qSRlXWBd/6jOumr1JSXWTV4Ctw5UyhT/chvyyuOparDDXg99oZvsRxCIrFuwsMpXW7YiGs0I1gvHxuQlY5ucpWrhtJq5OiQlncTNFvV1ZaipeydjublVG1n59u4ypqtftKWfMjaUFk81XVBmWUNYVqWSnZ/DwnSpYlvkzWuApj+0NoS4WAmKuaZpWwo4+fn45caLEjVvGrNJw3MrRnlZXxgTA3TLziV0KF/1Wyf4UwW711XUrmfNAxis2T9fvsgNz+Xu33g5SsMjJ+H3DDNlu3T+cDmLEQY2l4T35fHvTzMtMfa10Jqcv49wr+uMjJHUNZFUqtD/ljsSK2jiv9sod2YMDvoyFoGMpcj7Y+QyljyVXUq1ah7ej1AcP4a+X3a1top+PbsyI2nwN+OcM8h+FL/478vIdy6vHQYNH6M8WPhSBkUTjSFI9bZd32qbTF5elD+fZQajxtfXlyv/5DRbhqW6jCJ7mS5QN+WQeN9Lqv6tYUudLRIfwbjsm4omPJFtZD6TFW+lxoH/uNW/9V1m2bAwmjtFzItrSrUKPicvvx2Un6ZcoYaW/C7ffzIxfoDd3Ixl87nEu7jVGXkWZZq/k+PD+Ybw+lxrpK5STt7ssoY1x3nVVy+1dGRmkfGg7vh1P+XBVK02f9cNnYY8N+mzdFUoN1geGMTL6dS/k2aCh2TIYy3Dm/vSqsa5eN3DEagrwpf0S+bow6E+5YnBXZ/PEvv6+EtrE6BCFlCvPnt3X897Bx2yhekVF++YZV2M+tX64aX/C5sF+aEftpqNoaqjgO+3nKqBDMrbSFLgckaVfSqMMof64L5+LS4zC8z+kxbrhTcq5NPJBwJfjjbVfpcVbatsXb3/D+pF8u1BwZqS5y66XGL9eAKQTWG6zbBkO+7ZFGlpgfcR6whWqLgyqE4a0K3dGG9z1D8lU1/bzG25z43wnrAtPVcudNK7cO07awrw8atz3qrXvdLt+VQalRmoFRHwvVSK3fjpWxc0B4bxe6lKj0x39Hwr8fDIFo//6szhZunGlLurY8fuNAJnachC6v48dcpVx7l39v4PezcJ4J7ytDOfxQPTXcQBPWU6df0EZ/vA2pcONCqIwZ3r92+eWo9cFuqfjGhLDejH/NGv94eI80Fnu0/5txnjvE/0bufWit315dCdeGDsu1R1U23MxQ6HK5Qr67DOuqy4a/XTczhXN1JKPXE24fOCly710jmaK2KJyHhv281MXWd2hvw3zmzze2uO0PyxU//kt/h3Vk5bqMsZJyuUgZueMxLbefZVV4T5GvxqrCMRrvgsXY4nN5/HNAGD68d8j446PWyndNXWhTZkfSPF9xtz1RWN7S9sTI7ePdfphZvpuQ0P7GK+WOViU3/n/SL1M4HsLf8cezcsdRZArvN8INMPEbCir9e8uB2OPG2vzNReH4mOHbq/DZoPQ9s/HDZkyhwnCYn2HjvhSu8J8V3DFq8t0b2bDPHOENAPHquaP9JOTOC1Uq3BQVqgPbw3jNpL9pYvAI5tPEOl0o2u9PkJsekr7bqh5z4izz0Uj7G87KoZv5N2civTVTevbCdEEgBAAAAMC0d6QfrUs/EIUvgY2kOhVXlZpd8rm3ovjfogo+Ru4ilaQJBzoSkprjV1An2Uzrfg6leaxhSq/ojqPC+mpbY0jJBQvm+wv4s/zaOzBmda6pvehQa6VZ4YryGK+9QNI5I+6Mmch8Hsk43oQqf010euMNdzjzeDTbZrK26+FOZzL3p8OZ1qEOLkyN47XfHYt5OF6vPRn7vTOrzlcJ7B88itc5krb3aKd/uG3xsd7mR7oOjmSYYzk+gBPJrFkubnWgne4CjoRVIayVVnE4Ycg/diyqUocwdHw+Rvs8Hg/vjhfyMGOMfzisXHCp3xRuSmiKXGAtHuBL2sJNGWEeB1QIs8bDR4f7+n1yQaKMD/zU+M/ESVv8UXq8dRCpEODPmNHXneyh12XoznpEYN4v776ECybNjoqveZR+5M/JVVltN255mmzhhpmsYjecxEJN8QBWCD3nfNisNLAX/zv+f/goHv8JIe+EpIbIXY/pNS4UlLJ+f48FqrqMqySbU3F4PgSMJ2Ofy/pAWVounJdUIZBmjQtdhlBpTrEbReQCXlV+vnN+XeV8QKveBwDb/A00syM3vz1+3w4By7Cvlgbx8pvUjHxsrNDeof6uljSXav/TGoEQAAAAAAAAAAAAYJoouglhAo9PlkTJ/2N9sZ7QyJsojhWjQtXcGSXZxHwoZpTMYkKuq/vJeP06KV+x6GhUHsaNGEeiRtKpE5zPlFyXvnPGeG7Mm0lUqAJZKR2zPPKMcW6omWmlmeOVhJoMh3HfRFKFyl1jjht7rMpKzSXzP5EbfCY8fzjhlLbdAAAAAAAAAAAAAAAAmOYIhAAAAAAAAAAAAAAAAJQZAiEAAAAAAAAAAAAAAABlhkAIAAAAAAAAAAAAAABAmSEQAgAAAAAAAAAAAAAAUGYIhAAAAAAAAAAAAAAAAJQZAiEAAAAAAAAAAAAAAABlJnUsJjowMKANGzZo06ZNam1tVW1trc4++2ytXLlSb3vb245omrt379Zdd92lzZs3q729XU1NTbrkkkt0ww03aPHixZO8BAAAAAAAAAAAAAAAANPXpFcI6e/v10c+8hHdeeedam1t1ZIlS1RTU6Mnn3xSH//4x3XnnXce9jRfeeUVLV++XA888ID6+/t1xhlnKJPJaOPGjVq+fLk2b9482YsBAAAAAAAAAAAAAAAwbU16IOS2227Tc889p7POOkuPPvqoHnroIT3xxBO64447lEqltG7dOv3mN7+Z8PSy2axuvPFGdXZ2atmyZXryySf105/+VJs3b9aKFSs0NDSkz372s+ro6JjsRQEAAAAAAAAAAAAAAJiWJjUQsnPnTj388MNKJBJau3at5s6dm3/uve99r66//npJ0rp16yY8zYcfflg7duzQvHnzdPvtt6uqqkqSVFFRoS9+8Yu68MIL1d3drR/84AeTuSgAAAAAAAAAAAAAAADT1qQGQjZu3KhcLqelS5fqtNNOG/H8hz70IUnSM888o927d09omg899JAkadmyZaqoqCh6zhijD37wg5KkTZs2Hc2sAwAAAAAAAAAAAAAAlI1JDYRs3bpVknThhReO+vycOXM0f/58SdKWLVsOOb0oirRt27Zxp3nBBRdIknbt2qU9e/Yc7iwDAAAAAAAAAAAAAACUnUkNhOzYsUOStHDhwjGHCYGQ11577ZDT27dvnwYHB8ed5ty5c5VMJic8TQAAAAAAAAAAAAAAgHI3qYGQgwcPSpKam5vHHGbGjBmSpI6OjglPb7xpJpNJ1dfXT3iaAAAAAAAAAAAAAAAA5S41mRML1TwqKirGHKaysrJo2IlMLz7eeNMcGBiY0HxO1KxZ9ZM6PUwc6x4ATlycAwDgxMU5AABOTLT/AHDi4hwAACcuzgFTY1IrhISuW4wxYw5jrXUvnDj0S8eHmaxpAgAAAAAAAAAAAAAAlLtJTVDU1NRIkoaGhsYcJpPJSBq/4kfp9CZzmgAAAAAAAAAAAAAAAOVuUgMhTU1NkqTOzs4xh+no6JAkzZw5c8LTG2+a2WxWPT09E54mAAAAAAAAAAAAAABAuZvUQMiiRYskSa2trWMO09bWJklqaWk55PTmzJmj+vr6cae5Z88e5XK5CU8TAAAAAAAAAAAAAACg3E1qIOS8886TJG3dunXU5/ft26fdu3dLks4///wJTfPcc8+VJD377LOjPh8enz9/vubMmXM4swsAAAAAAAAAAAAAAFCWJjUQ8q53vUuStGXLFr3yyisjnr/33nslSRdddJEWLFgwoWm++93vliQ9+OCDymQyI56///77JUnLly8/onkGAAAAAAAAAAAAAAAoN5MaCGlpadEVV1yhXC6nm2++WTt27Mg/t3HjRm3YsEGStGrVqhHj7ty5Uy+//LL2799f9PiyZcu0cOFC7dq1S7fccot6e3slSZlMRqtXr9bvf/971dfXa8WKFZO5KAAAAAAAAAAAAAAAANOWsdbayZxgR0eHVq5cqe3btyuZTOr0009Xd3e32traJEmf+cxndOONN44Y77LLLlNbW5uWL1+ur33ta0XPbdu2Tdddd516enpUU1OjRYsWqbW1VZ2dnUqn09qwYYMuvvjiyVwMAAAAAAAAAAAAAACAaWtSK4RIUlNTk370ox/pk5/8pFpaWvTyyy+ro6NDF110kb797W+PGgY5lHPPPVcbN27U+9//fjU0NOjFF1+UMUaXX365fvKTnxAGAQAAAAAAAAAAAAAAiJn0CiEAAAAAAAAAAAAAAAA4via9QggAAAAAAAAAAAAAAACOLwIhAAAAAAAAAAAAAAAAZYZACAAAAAAAAAAAAAAAQJkhEAIAAAAAAAAAAAAAAFBmCIQAAAAAAAAAAAAAAACUGQIhAAAAAAAAAAAAAAAAZYZACAAAAAAAAAAAAAAAQJlJHe8ZAAYGBrRhwwZt2rRJra2tqq2t1dlnn62VK1fqbW972/GePQDABO3du1ff//73tXnzZu3evVuStGDBAl166aW67rrrNHPmzBHjdHR06O6779Zjjz2mffv2qaGhQRdccIGuv/56LV26dMzX2r17t+666y5t3rxZ7e3tampq0iWXXKIbbrhBixcvPlaLCACYgGw2qw984AN64YUXtGbNGl155ZUjhqH9B4Dy8Nvf/lb//u//rq1bt6qzs1MzZszQxRdfrFWrVo3ZLnMOAIDpr6OjQ+vXr9djjz2m3bt3K51O6/TTT9fy5ct11VVXKZEY/V5kzgEAMH3df//9+spXvqLVq1frqquuGnWYqW7nt2/frrvvvltPPfWUuru7NXv2bP3Zn/2ZVq1apTlz5hztIpcNY621x3smcOLq7+/Xtddeq+eee07pdFpLlixRZ2dn/ovEm2++WZ/85CeP81wCAA7l6aef1qpVq9Td3a1kMqmFCxcqiiK1trYql8tp1qxZ2rBhg84888z8OK+//rquvvpq7dy5U9XV1Vq0aJH27dun119/XclkUrfddpve//73j3itV155RVdffbU6OztVX1+vU045Ra2trers7FRlZaXuuusuvfWtb53KxQcAxNx5551at26dJI0aCKH9B4DysHbtWq1fv16SNGvWLDU3N+vVV19VJpNRZWWl7r77br35zW8uGodzAABMf21tbVqxYoV2796tVCqllpYW9ff356/pv/3tb9e6deuUTqeLxuMcAADT17Zt23Tttdeqr69vzEDIVLfzTz/9tK677joNDQ2pqalJ8+bN06uvvqr+/n41Njbqhz/8oc4666xjsj6mG7qMwXF122236bnnntNZZ52lRx99VA899JCeeOIJ3XHHHUqlUlq3bp1+85vfHO/ZBACMo7u7WzfffLO6u7v11re+Vb/61a/0yCOP6Je//KV+8Ytf6IILLtCBAwd00003aWhoKD/eZz7zGe3cuVNvfvOb9etf/1oPPvigNm/erL/5m79RLpfTV7/6Vb388stFr5XNZnXjjTeqs7NTy5Yt05NPPqmf/vSn2rx5s1asWKGhoSF99rOfVUdHx1SvBgCApD/+8Y+65557xh2G9h8Apr8HHnhA69evVzqd1po1a7R582Y9/PDD2rx5sy699FINDQ3pb//2b9Xf3180HucAAJj+vvCFL2j37t1asmSJNm3apE2bNumJJ57Q3XffrcrKSj3xxBPasGHDiPE4BwDA9PTUU0/p+uuvV19f37jDTWU739nZqU984hMaGhrSxz72MT355JP517v88svV1dWlm2++WZlMZtLXx3REIATHzc6dO/Xwww8rkUho7dq1mjt3bv659773vbr++uslKX93IQDgjenBBx9Ue3u7Zs+erW9+85uaPXt2/rmTTz5Zd911lxobG9Xa2qpHHnlEknsTuWXLFtXU1Gjt2rVqbGyUJCUSCX384x/Xe97zHg0PD+vuu+8ueq2HH35YO3bs0Lx583T77berqqpKklRRUaEvfvGLuvDCC9Xd3a0f/OAHU7PwAIC8TCajW2+9VblcThUVFaMOQ/sPANPf0NCQvv71r0tyXwpeeeWVMsZIkmbMmKG1a9eqtrZWBw8e1OOPP54fj3MAAEx/e/bs0e9+9ztJ7mbPlpaW/HOXXXZZ/pr+Aw88UDQe5wAAmH6Ghoa0bt06ffSjH1VXV9e4w051O/+v//qv6urq0tKlS3XLLbcolUpJkurq6rR27VotWLBAu3bt0saNGydpbUxvBEJw3GzcuFG5XE5Lly7VaaedNuL5D33oQ5KkZ555Jl9uDgDwxvPUU09JciVB6+rqRjzf3Nys888/X5L0/PPPS5IeeughSdI73vEONTc3jxjn6quvliQ99thjGhwczD8exlu2bNmILxuNMfrgBz8oSdq0adNRLRMA4PB9+9vf1vbt23XNNddo1qxZow5D+w8A09/jjz+uzs5OtbS06AMf+MCI5+vr6/WlL31Jn/vc53TqqafmH+ccAADT3969e/N/x7sFDs4555wRw0mcAwBgutmxY4cuv/xy3XnnnZKkT3/605o/f/6Yw091Ox/GG60LmoqKivzj//mf/znOUp44CITguNm6dask6cILLxz1+Tlz5uQbly1btkzVbAEADtOqVat0xx136H3ve9+Yw1hrJUlRFEmSnn32WUljnwPOPfdcpVIp9ff364UXXsiPu23btnHHu+CCCyRJu3bt0p49e45gaQAAR2Lr1q36/ve/r5aWFn32s58dczjafwCY/kLXvpdddpmSyeSowyxfvlwf/ehH9ad/+qf5xzgHAMD0N2/evPzf//M//zPi+RdffHHEcBLnAACYbvbu3as9e/Zo6dKl+vGPf6xVq1aNO/xUtvP79+9XW1tb0fNjjffMM89oeHh43Hk/ERAIwXGzY8cOSdLChQvHHCYEQl577bWpmCUAwBE499xz9d73vlfnnXfeqM+3t7fng32nnXaaoihSa2urpLHPAel0WnPmzJEkvfrqq5Kkffv25dPDY403d+7c/EVpzh0AMDUGBwf1d3/3d7LWas2aNfnynqVo/wGgPIQv+5YsWSJrrX75y1/qc5/7nK699lp96lOf0k9+8pMRF105BwBAeZgzZ47e8Y53SJL+4R/+QTt37sw/99vf/lb//M//LEm69tpr849zDgCA6edNb3qTvvvd7+pHP/qRzj777HGHnep2Ppx7jI/MDbsAAAsaSURBVDE6+eSTRx0vfL+cyWQIDEpKHe8ZwInr4MGDkjRq6aBgxowZkqSOjo6pmCUAwDFw++23a2BgQNXV1br88svV1dWlbDYr6dDngLa2tvw5IJw3xhsvmUyqvr5enZ2dnDsAYIqsXbtWr732mq677rox78yQRPsPAGUidOubSqW0YsUKPf3000XPP/LII/qXf/kXffe739XcuXMlcQ4AgHLy9a9/XX//93+vRx55RO9+97vV0tKiwcFBtba2qqGhQV/4whf04Q9/OD885wAAmH5OOeUUnXLKKRMadqrb+TBeXV3diG5m4q8VdHR0jFuc4ERAhRAcNyH1NdbBKkmVlZVFwwIAppfvfOc7+X76PvGJT2jmzJlFbfrhnAPi44XnxhtvYGDgyGccADAhTz31lP7t3/5NixYt0qc//elxh6X9B4Dy0NfXJ0las2aNnn/+eX3hC1/Qb3/7W23dulX33HOP5s+fr+3bt+uGG25QJpORxDkAAMqJMUZnnnmmGhsblc1m9dJLL+XvDK+vrx9RMZBzAACUt6lu58Pv8caJn4s4PxAIwXEUSvwYY8YcxlorSUok2FUBYLq588479a1vfUuS61/8Yx/7mKTiNv1wzgFHOh4A4Njo7e3V5z//eSUSCa1Zs2bcD+IS7T8AlItw4ba9vV3f+MY39JGPfETNzc2qrq7W29/+dm3YsEHpdFovvvii/uM//kMS5wAAKBe9vb36yEc+on/6p3/S/Pnz9cMf/lDbtm3T7373O61evVo9PT368pe/rK9+9av5cTgHAEB5m+p2fiLfL481fycq1gCOm5qaGknS0NDQmMOEO0kOdXEZAPDGkc1m9eUvf1nr1q2TJL3lLW/RN7/5zfwbtNra2vywoZ0fTek5IJw3JM4dAPBGsGbNGrW1temjH/2oli5desjhaf8BoDyEu+3OPPNMvfOd7xzx/KJFi/RXf/VXkqTHHntMEucAACgXGzZs0LZt2zR79mz94Ac/0MUXX6zKyko1NTXpqquu0ve+9z0lk0ndd999euqppyRxDgCAcjfV7fxEvl+OVx8prVx1IiIQguOmqalJktTZ2TnmMKE/qJkzZ07FLAEAjlJvb68+/vGP60c/+pEk6S//8i919913F30or6mpyZeOG69/19JzQDhvSGOfO7LZrHp6eorGAwBMvl//+td64IEHtHjxYn3qU5+a0Di0/wBQHhoaGiRJZ5111pjDLFmyRJK0a9cuSZwDAKBcPPLII5KklStX5s8Hceeee64uvfRSScp3Icw5AADK21S382G83t5eDQ8Pj/taktTc3DyRxShrBEJw3CxatEiS8v0LjqatrU2S1NLSMhWzBAA4Cnv37tXVV1+t//7v/5Yk/fVf/7W+8Y1vjOg3MJFI6NRTT5U09jlgeHhY+/fvl1Q4B8yZM0f19fXjjrdnzx7lcrmi8QAAk+/nP/+5JOnll1/WOeecozPOOKPoJ7yP//znP68zzjhD11xzDe0/AJSJxYsXSxr/7r9UKiWp0Ic45wAAKA+7d++WVLi2P5rTTjtNUqHd5hwAAOVtqtv58HkkiiLt2bNn1PHCdanKykrNnTv3MJeo/BAIwXFz3nnnSZK2bt066vP79u3Lv8E8//zzp2q2AABHYP/+/brmmmu0fft2JZNJffWrX9Wtt946Zj9+hzoHbNu2TdlsVpWVlfqTP/mT/OPnnnuuJOnZZ58ddbzw+Pz58zVnzpwjXRwAwCG0tLToggsuGPMnfAEYhjv99NMl0f4DQDkIbfm2bdvGHOaVV16RJC1cuHDEeJwDAGD6qqurkyQdOHBgzGEOHjxYNKzEOQAAyt1UtvONjY35cMihxjvvvPOUTCYPb2HKEIEQHDfvete7JElbtmzJXyiIu/feeyVJF110kRYsWDCl8wYAmLhMJqMbb7xRO3fuVDqd1re+9S1dffXV447z7ne/W5L0i1/8YtRycPfdd58k1+VMvI+/MN6DDz446h2J999/vyRp+fLlR7QsAICJufHGG3XfffeN+TNr1ixJ0g033KD77rtPX/rSlyTR/gNAObjiiiskue5gHn300RHPHzx4MN9NwOWXX55/nHMAAEx/F198sSTpgQceyN+xHdfZ2an/+q//KhpW4hwAAOVuqtv58B3zj3/84xHjZDIZ/fSnPx11vBMVgRAcNy0tLbriiiuUy+V08803a8eOHfnnNm7cqA0bNkiSVq1adbxmEQAwAevXr9cf/vAHSdJXvvIV/fmf//khx7nkkkt04YUXqqenRzfddJNef/11Sa7M2/r16/Wzn/1M6XRaH/vYx4rGW7ZsmRYuXKhdu3bplltuUW9vryT3Jm/16tX6/e9/r/r6eq1YsWKSlxIAMBlo/wFg+lu8eLGuuuoqSa5rsMcffzz/3IEDB/TpT39afX19OuOMM4o+G3AOAIDp74YbblA6ndbzzz+vW2+9Ve3t7fnndu3apRtuuEGdnZ1asGCB3ve+9+Wf4xwAAOVtqtv5lStXqrGxUU8//bRWr16dD5P09vbqlltu0a5du3TyySfrPe95zxQs/Rufsdba4z0TOHF1dHRo5cqV+S4GTj/9dHV3d+f7dvrMZz6jG2+88TjPJQBgLJlMRm95y1vU1dWlVCqVL/E2lre97W35dn3Xrl368Ic/rH379qmiokJLlizR/v37deDAARlj9I//+I9atmzZiGls27ZN1113nXp6elRTU6NFixaptbVVnZ2dSqfT2rBhQ9FdKACAqXfZZZepra1Na9as0ZVXXln0HO0/AEx/g4OD+tSnPqVf/epXkqS5c+equblZ27dv1/DwsObPn6/169fn+/cOOAcAwPT3i1/8QrfeeqsGBweVTqe1ePFiRVGkl156SVEUcQ4AgDIVrvWsXr06HxCPm+p2/oknntDNN9+s4eFhzZgxQwsWLNCrr76qvr4+NTQ06N5779WSJUuOybqYbgiE4Ljr7+/X9773Pf385z/Xrl27lEqldPbZZ2vFihVFpUUBAG88L7zwQtEdH4eyfPlyfe1rX8v/397ernvuuUePP/649u7dq+rqap133nm6/vrrx/0w39bWpu985zt68skndfDgQdXV1emiiy7SqlWrdNZZZx3VMgEAjt54gRCJ9h8AyoG1Vj/72c/0wAMP6I9//KMymYzmzZunv/iLv9C1116rpqamUcfjHAAA09+OHTv0/e9/X7/5zW+0d+9epVIpnXLKKXrnO9+plStXqqGhYdTxOAcAwPR1qECINPXt/Isvvqh77rlHW7ZsUWdnp5qamvSWt7xFN910k04++eSjXuZyQSAEAAAAAAAAAAAAAACgzCSO9wwAAAAAAAAAAAAAAABgchEIAQAAAAAAAAAAAAAAKDMEQgAAAAAAAAAAAAAAAMoMgRAAAAAAAAAAAAAAAIAyQyAEAAAAAAAAAAAAAACgzBAIAQAAAAAAAAAAAAAAKDMEQgAAAAAAAAAAAAAAAMoMgRAAAAAAAAAAAAAAAIAyQyAEAAAAAAAAAAAAAACgzBAIAQAAAAAAAAAAAAAAKDMEQgAAAAAAAAAAAAAAAMoMgRAAAAAAAAAAAAAAAIAyQyAEAAAAAAAAAAAAAACgzBAIAQAAAAAAAAAAAAAAKDMEQgAAAAAAAAAAAAAAAMoMgRAAAAAAAAAAAAAAAIAyQyAEAAAAAAAAAAAAAACgzBAIAQAAAAAAAAAAAAAAKDMEQgAAAAAAAAAAAAAAAMoMgRAAAAAAAAAAAAAAAIAyQyAEAAAAAAAAAAAAAACgzBAIAQAAAAAAAAAAAAAAKDP/P+edciFgUMx1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2700x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_concat_optim = FullFVAE(max_len=23, aa_dim= 20, use_v=use_v, use_j=use_j, v_dim=v_dim, j_dim=j_dim,\n",
    "                 hidden_dim=256, latent_dim=128, activation = nn.SELU())\n",
    "\n",
    "criterion = VAELoss(weight_seq=3, weight_kld=1.5, use_v=use_v, use_j=use_j, v_dim=v_dim, \n",
    "                    j_dim=j_dim,  weight_v=0.5, weight_j=0.3, debug=False)\n",
    "\n",
    "optimizer = optim.Adam(model_concat_optim.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "concat_train_losses, concat_valid_losses = [], []\n",
    "concat_train_rec, concat_valid_rec = [], []\n",
    "concat_train_kld, concat_valid_kld = [], []\n",
    "for n in range(1000):\n",
    "    acum_loss = 0\n",
    "    acum_rec = 0\n",
    "    acum_kld = 0\n",
    "    # if n == 0 or n%10==0 :\n",
    "    #     criterion.set_debug(True)\n",
    "    # else:\n",
    "    #     criterion.set_debug(False)\n",
    "\n",
    "    # 230922 1322 : THIS RUN HAS THE ORIGINAL CONCATENATED DECODER (NOT SEPARATE FOR SEQ/V/J)\n",
    "    for x, _  in train_loader:\n",
    "        x_hat, mu, logvar = model_concat_optim(x)\n",
    "        recon_loss, kld_loss = criterion(x_hat, x, mu, logvar)\n",
    "        loss = recon_loss + kld_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acum_loss += loss.item() * y.shape[0]\n",
    "        acum_rec += recon_loss.item() * y.shape[0]\n",
    "        acum_kld += kld_loss.item() * y.shape[0]\n",
    "    acum_loss /= len(train_loader.dataset)\n",
    "    acum_rec /= len(train_loader.dataset)\n",
    "    acum_kld /= len(train_loader.dataset)\n",
    "    if n>=10:\n",
    "        concat_train_rec.append(acum_rec)\n",
    "        concat_train_kld.append(acum_kld)\n",
    "        concat_train_losses.append(acum_loss)\n",
    "    if n%10==0:\n",
    "        criterion.set_debug(True)\n",
    "        print(f'\\n{n} epochs, Train Rec:\\t{acum_rec:.3e}, KLD:\\t{acum_kld:.3e}')\n",
    "    else:\n",
    "        criterion.set_debug(False)\n",
    "\n",
    "    # Valid\n",
    "    acum_loss = 0\n",
    "    acum_rec = 0\n",
    "    acum_kld = 0\n",
    "    for x, _  in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            x_hat, mu, logvar = model_concat_optim(x)\n",
    "            recon_loss, kld_loss = criterion(x_hat, x, mu, logvar)\n",
    "            loss = recon_loss + kld_loss        \n",
    "            acum_loss += loss.item() * y.shape[0]\n",
    "            acum_rec += recon_loss.item() * y.shape[0]\n",
    "            acum_kld += kld_loss.item() * y.shape[0]\n",
    "    acum_loss /= len(valid_loader.dataset)\n",
    "    acum_rec /= len(valid_loader.dataset)\n",
    "    acum_kld /= len(valid_loader.dataset)\n",
    "    if n>=10:\n",
    "        concat_valid_rec.append(acum_rec)\n",
    "        concat_valid_kld.append(acum_kld)\n",
    "        concat_valid_losses.append(acum_loss)\n",
    "    if n%10==0:\n",
    "        print(f'\\n{n} epochs, Valid Rec:\\t{acum_rec:.3e}, KLD:\\t{acum_kld:.3e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc5212-93e0-43c0-860b-2438d419aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to implement a scheduler or something or maybe split learning rates for kld and for reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b9d970-769e-454c-b81f-f544d76274e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## split optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "id": "08b64049-c565-4afb-8ce2-6150c63f90a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5660377358490566 0.09433962264150944 0.05660377358490566 0.2830188679245283\n",
      "\n",
      "0 epochs, Train Rec:\t1.010e+01, KLD:\t1.277e-01\n",
      "seq_loss tensor(6.1259)\n",
      "v_loss tensor(0.0701)\n",
      "j_loss tensor(0.2579)\n",
      "kld_weight 0.2731132075471698\n",
      "kld_loss tensor(0.3360) \n",
      "\n",
      "seq_loss tensor(6.3631)\n",
      "v_loss tensor(0.0649)\n",
      "j_loss tensor(0.2616)\n",
      "kld_weight 0.2728301886792453\n",
      "kld_loss tensor(0.3448) \n",
      "\n",
      "seq_loss tensor(6.2231)\n",
      "v_loss tensor(0.0614)\n",
      "j_loss tensor(0.2600)\n",
      "kld_weight 0.27254716981132077\n",
      "kld_loss tensor(0.3542) \n",
      "\n",
      "seq_loss tensor(6.4243)\n",
      "v_loss tensor(0.0747)\n",
      "j_loss tensor(0.2537)\n",
      "kld_weight 0.27226415094339623\n",
      "kld_loss tensor(0.3619) \n",
      "\n",
      "seq_loss tensor(6.4124)\n",
      "v_loss tensor(0.0497)\n",
      "j_loss tensor(0.2653)\n",
      "kld_weight 0.2719811320754717\n",
      "kld_loss tensor(0.3869) \n",
      "\n",
      "\n",
      "0 epochs, Valid Rec:\t1.999e+00, KLD:\t1.075e-01\n",
      "seq_loss tensor(6.3786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0694, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27169811320754716\n",
      "kld_loss tensor(0.3815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.0108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0727, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27141509433962263\n",
      "kld_loss tensor(0.3602, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.3025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2711320754716981\n",
      "kld_loss tensor(0.3513, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.3856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0715, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27084905660377356\n",
      "kld_loss tensor(0.3414, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.0093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0789, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27056603773584903\n",
      "kld_loss tensor(0.3572, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.2342, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0764, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2702830188679245\n",
      "kld_loss tensor(0.3668, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.9247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0791, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2148, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.27\n",
      "kld_loss tensor(0.3764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.9178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2697169811320755\n",
      "kld_loss tensor(0.3802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26943396226415095\n",
      "kld_loss tensor(0.3794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.9919, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2691509433962264\n",
      "kld_loss tensor(0.3657, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.0750, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0804, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2688679245283019\n",
      "kld_loss tensor(0.3453, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.8190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0700, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26858490566037735\n",
      "kld_loss tensor(0.3358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0672, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2683018867924528\n",
      "kld_loss tensor(0.3406, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.9597, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0640, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2026, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2680188679245283\n",
      "kld_loss tensor(0.3542, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(6.0356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0784, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26773584905660375\n",
      "kld_loss tensor(0.3600, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.6315, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2674528301886792\n",
      "kld_loss tensor(0.3597, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.8466, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0725, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2020, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26716981132075474\n",
      "kld_loss tensor(0.3511, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.8663, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2668867924528302\n",
      "kld_loss tensor(0.3382, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.5579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2019, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26660377358490567\n",
      "kld_loss tensor(0.3346, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.6480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2018, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26632075471698113\n",
      "kld_loss tensor(0.3315, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2037, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2660377358490566\n",
      "kld_loss tensor(0.3359, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.6456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26575471698113207\n",
      "kld_loss tensor(0.3419, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.8532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26547169811320753\n",
      "kld_loss tensor(0.3443, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0767, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.265188679245283\n",
      "kld_loss tensor(0.3394, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.6540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26490566037735847\n",
      "kld_loss tensor(0.3270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0657, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2013, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26462264150943393\n",
      "kld_loss tensor(0.3194, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.9838, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2020, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26433962264150945\n",
      "kld_loss tensor(0.3170, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.8637, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2021, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2640566037735849\n",
      "kld_loss tensor(0.3228, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2011, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2637735849056604\n",
      "kld_loss tensor(0.3338, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.6604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2022, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26349056603773585\n",
      "kld_loss tensor(0.3283, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7571, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2005, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2632075471698113\n",
      "kld_loss tensor(0.3200, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.8421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2629245283018868\n",
      "kld_loss tensor(0.3094, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.7166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2021, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.26264150943396225\n",
      "kld_loss tensor(0.3080, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(5.3266, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.2623584905660377\n",
      "kld_loss tensor(0.3269, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "10 epochs, Train Rec:\t8.882e-01, KLD:\t9.084e-02\n",
      "seq_loss tensor(1.4625)\n",
      "v_loss tensor(0.0670)\n",
      "j_loss tensor(0.2319)\n",
      "kld_weight 0.16273584905660377\n",
      "kld_loss tensor(0.1684) \n",
      "\n",
      "seq_loss tensor(1.4708)\n",
      "v_loss tensor(0.0612)\n",
      "j_loss tensor(0.2387)\n",
      "kld_weight 0.16245283018867923\n",
      "kld_loss tensor(0.1718) \n",
      "\n",
      "seq_loss tensor(1.4972)\n",
      "v_loss tensor(0.0574)\n",
      "j_loss tensor(0.2343)\n",
      "kld_weight 0.1621698113207547\n",
      "kld_loss tensor(0.1703) \n",
      "\n",
      "seq_loss tensor(1.4989)\n",
      "v_loss tensor(0.0668)\n",
      "j_loss tensor(0.2280)\n",
      "kld_weight 0.1618867924528302\n",
      "kld_loss tensor(0.1717) \n",
      "\n",
      "seq_loss tensor(1.3937)\n",
      "v_loss tensor(0.0469)\n",
      "j_loss tensor(0.2233)\n",
      "kld_weight 0.16160377358490566\n",
      "kld_loss tensor(0.1739) \n",
      "\n",
      "\n",
      "10 epochs, Valid Rec:\t5.292e-01, KLD:\t5.161e-02\n",
      "seq_loss tensor(1.4924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1970, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16132075471698112\n",
      "kld_loss tensor(0.1722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4813, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1967, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16103773584905662\n",
      "kld_loss tensor(0.1735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0709, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2001, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16075471698113208\n",
      "kld_loss tensor(0.1712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0622, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2003, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16047169811320755\n",
      "kld_loss tensor(0.1749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5717, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.16018867924528302\n",
      "kld_loss tensor(0.1788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4508, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15990566037735848\n",
      "kld_loss tensor(0.1657, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0659, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2030, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15962264150943395\n",
      "kld_loss tensor(0.1754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4764, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1974, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15933962264150942\n",
      "kld_loss tensor(0.1753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0711, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15905660377358488\n",
      "kld_loss tensor(0.1714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0727, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1996, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15877358490566038\n",
      "kld_loss tensor(0.1685, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15849056603773584\n",
      "kld_loss tensor(0.1716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4066, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1959, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15820754716981134\n",
      "kld_loss tensor(0.1681, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15792452830188677\n",
      "kld_loss tensor(0.1686, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0670, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1970, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15764150943396227\n",
      "kld_loss tensor(0.1690, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0766, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15735849056603773\n",
      "kld_loss tensor(0.1665, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1981, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1570754716981132\n",
      "kld_loss tensor(0.1691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15679245283018867\n",
      "kld_loss tensor(0.1700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1999, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15650943396226413\n",
      "kld_loss tensor(0.1678, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0567, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15622641509433963\n",
      "kld_loss tensor(0.1698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0593, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1559433962264151\n",
      "kld_loss tensor(0.1664, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0701, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15566037735849056\n",
      "kld_loss tensor(0.1647, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15537735849056603\n",
      "kld_loss tensor(0.1716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4730, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0603, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1970, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1550943396226415\n",
      "kld_loss tensor(0.1621, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4457, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15481132075471699\n",
      "kld_loss tensor(0.1630, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0680, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1995, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15452830188679245\n",
      "kld_loss tensor(0.1686, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0647, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15424528301886792\n",
      "kld_loss tensor(0.1681, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1990, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15396226415094338\n",
      "kld_loss tensor(0.1657, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4691, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0586, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2014, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15367924528301885\n",
      "kld_loss tensor(0.1623, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15339622641509432\n",
      "kld_loss tensor(0.1655, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4562, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0694, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1995, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1531132075471698\n",
      "kld_loss tensor(0.1652, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4580, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0761, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1986, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15283018867924528\n",
      "kld_loss tensor(0.1660, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15254716981132074\n",
      "kld_loss tensor(0.1634, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4750, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0693, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2001, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.1522641509433962\n",
      "kld_loss tensor(0.1683, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4580, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1942, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.15198113207547168\n",
      "kld_loss tensor(0.1575, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "20 epochs, Train Rec:\t7.692e-01, KLD:\t3.910e-02\n",
      "seq_loss tensor(1.2415)\n",
      "v_loss tensor(0.0642)\n",
      "j_loss tensor(0.2230)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738) \n",
      "\n",
      "seq_loss tensor(1.2298)\n",
      "v_loss tensor(0.0592)\n",
      "j_loss tensor(0.2305)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745) \n",
      "\n",
      "seq_loss tensor(1.2530)\n",
      "v_loss tensor(0.0555)\n",
      "j_loss tensor(0.2267)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744) \n",
      "\n",
      "seq_loss tensor(1.2452)\n",
      "v_loss tensor(0.0626)\n",
      "j_loss tensor(0.2188)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748) \n",
      "\n",
      "seq_loss tensor(1.1883)\n",
      "v_loss tensor(0.0461)\n",
      "j_loss tensor(0.2137)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748) \n",
      "\n",
      "\n",
      "20 epochs, Valid Rec:\t4.556e-01, KLD:\t2.244e-02\n",
      "seq_loss tensor(1.2153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0691, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1858, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1890, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2442, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2716, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1893, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2669, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1935, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1847, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2327, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2309, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0656, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1882, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2137, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0673, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1875, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1846, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1892, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2237, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1858, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2489, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0688, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1903, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3224, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1876, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1736, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1848, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2268, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0610, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1886, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1868, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1859, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2272, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0534, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2729, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0638, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1864, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2646, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1878, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1818, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1847, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2539, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1906, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1885, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "30 epochs, Train Rec:\t5.983e-01, KLD:\t4.383e-02\n",
      "seq_loss tensor(0.9233)\n",
      "v_loss tensor(0.0615)\n",
      "j_loss tensor(0.2095)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861) \n",
      "\n",
      "seq_loss tensor(0.9086)\n",
      "v_loss tensor(0.0552)\n",
      "j_loss tensor(0.2164)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869) \n",
      "\n",
      "seq_loss tensor(0.9274)\n",
      "v_loss tensor(0.0518)\n",
      "j_loss tensor(0.2101)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870) \n",
      "\n",
      "seq_loss tensor(0.9277)\n",
      "v_loss tensor(0.0584)\n",
      "j_loss tensor(0.2025)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874) \n",
      "\n",
      "seq_loss tensor(0.8845)\n",
      "v_loss tensor(0.0441)\n",
      "j_loss tensor(0.1955)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872) \n",
      "\n",
      "\n",
      "30 epochs, Valid Rec:\t3.542e-01, KLD:\t2.620e-02\n",
      "seq_loss tensor(0.9295, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0617, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1703, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1700, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1678, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9112, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1692, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1653, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0707, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0563, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1637, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9107, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1668, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0571, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1687, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8765, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8995, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1681, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8849, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1689, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1675, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1645, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1670, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1706, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9063, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1679, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1696, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8785, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1647, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0635, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1694, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0462, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1698, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0690, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1693, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0508, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9107, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0654, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1707, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0574, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1622, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8963, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1673, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0542, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1722, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "40 epochs, Train Rec:\t4.823e-01, KLD:\t4.553e-02\n",
      "seq_loss tensor(0.7295)\n",
      "v_loss tensor(0.0571)\n",
      "j_loss tensor(0.1977)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884) \n",
      "\n",
      "seq_loss tensor(0.7170)\n",
      "v_loss tensor(0.0506)\n",
      "j_loss tensor(0.2051)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890) \n",
      "\n",
      "seq_loss tensor(0.7360)\n",
      "v_loss tensor(0.0488)\n",
      "j_loss tensor(0.1991)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892) \n",
      "\n",
      "seq_loss tensor(0.7360)\n",
      "v_loss tensor(0.0498)\n",
      "j_loss tensor(0.1896)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896) \n",
      "\n",
      "seq_loss tensor(0.6979)\n",
      "v_loss tensor(0.0358)\n",
      "j_loss tensor(0.1826)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891) \n",
      "\n",
      "\n",
      "40 epochs, Valid Rec:\t2.913e-01, KLD:\t2.684e-02\n",
      "seq_loss tensor(0.7601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1528, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7315, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0468, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1604, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0489, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1549, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7055, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1598, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1609, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1568, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1549, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0520, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1562, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1619, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1529, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6944, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0437, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1531, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1563, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0472, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1535, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7084, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0402, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1610, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0590, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0899, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1588, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0473, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1563, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0566, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1590, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7151, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0565, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1573, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6874, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0572, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1585, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1604, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7082, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1553, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1552, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6799, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1565, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1586, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0508, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1619, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0454, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1545, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0487, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1561, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1535, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0456, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1514, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1593, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0745, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1656, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "50 epochs, Train Rec:\t4.052e-01, KLD:\t4.570e-02\n",
      "seq_loss tensor(0.6130)\n",
      "v_loss tensor(0.0371)\n",
      "j_loss tensor(0.1899)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879) \n",
      "\n",
      "seq_loss tensor(0.6004)\n",
      "v_loss tensor(0.0359)\n",
      "j_loss tensor(0.1956)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886) \n",
      "\n",
      "seq_loss tensor(0.6193)\n",
      "v_loss tensor(0.0336)\n",
      "j_loss tensor(0.1915)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884) \n",
      "\n",
      "seq_loss tensor(0.6164)\n",
      "v_loss tensor(0.0344)\n",
      "j_loss tensor(0.1825)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891) \n",
      "\n",
      "seq_loss tensor(0.5851)\n",
      "v_loss tensor(0.0230)\n",
      "j_loss tensor(0.1759)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "\n",
      "50 epochs, Valid Rec:\t2.492e-01, KLD:\t2.667e-02\n",
      "seq_loss tensor(0.6667, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0405, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1454, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0332, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1451, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0374, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1402, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5938, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0363, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1433, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0366, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1441, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0337, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1423, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5887, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0288, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1460, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0296, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1382, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5742, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0352, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1417, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6062, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0324, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1376, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0311, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1340, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0373, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1356, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5955, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0293, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1429, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0304, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1359, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0907, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0345, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1428, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0352, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1459, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0335, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0311, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0279, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1422, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5821, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0297, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1401, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0289, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1399, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0908, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0345, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1397, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0377, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1416, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0303, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1401, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0356, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1391, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6204, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0338, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1378, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0261, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1370, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5761, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0332, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1377, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0315, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1374, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5575, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0312, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1355, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0259, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1376, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0303, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1354, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0255, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1369, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0260, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1342, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "60 epochs, Train Rec:\t3.417e-01, KLD:\t4.609e-02\n",
      "seq_loss tensor(0.5101)\n",
      "v_loss tensor(0.0261)\n",
      "j_loss tensor(0.1693)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902) \n",
      "\n",
      "seq_loss tensor(0.5002)\n",
      "v_loss tensor(0.0244)\n",
      "j_loss tensor(0.1789)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0908) \n",
      "\n",
      "seq_loss tensor(0.5177)\n",
      "v_loss tensor(0.0247)\n",
      "j_loss tensor(0.1710)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906) \n",
      "\n",
      "seq_loss tensor(0.5190)\n",
      "v_loss tensor(0.0234)\n",
      "j_loss tensor(0.1619)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0915) \n",
      "\n",
      "seq_loss tensor(0.4812)\n",
      "v_loss tensor(0.0134)\n",
      "j_loss tensor(0.1607)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0910) \n",
      "\n",
      "\n",
      "60 epochs, Valid Rec:\t2.099e-01, KLD:\t2.737e-02\n",
      "seq_loss tensor(0.5108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0164, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0224, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0237, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1205, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5055, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0236, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1224, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1259, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0905, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0236, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1229, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1265, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0905, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0279, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0908, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0223, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0218, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5062, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0265, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1187, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0252, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1238, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0245, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1203, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1212, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1224, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1201, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5163, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1196, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0908, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5369, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0915, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5086, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0905, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1236, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1182, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0913, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5168, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0233, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1239, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0909, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0219, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1170, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0910, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5055, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0227, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0245, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0904, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0915, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0905, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1196, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4960, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0899, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4343, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0357, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1148, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "70 epochs, Train Rec:\t2.948e-01, KLD:\t4.600e-02\n",
      "seq_loss tensor(0.4516)\n",
      "v_loss tensor(0.0207)\n",
      "j_loss tensor(0.1516)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892) \n",
      "\n",
      "seq_loss tensor(0.4409)\n",
      "v_loss tensor(0.0194)\n",
      "j_loss tensor(0.1596)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898) \n",
      "\n",
      "seq_loss tensor(0.4579)\n",
      "v_loss tensor(0.0193)\n",
      "j_loss tensor(0.1536)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895) \n",
      "\n",
      "seq_loss tensor(0.4617)\n",
      "v_loss tensor(0.0195)\n",
      "j_loss tensor(0.1465)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903) \n",
      "\n",
      "seq_loss tensor(0.4189)\n",
      "v_loss tensor(0.0108)\n",
      "j_loss tensor(0.1492)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901) \n",
      "\n",
      "\n",
      "70 epochs, Valid Rec:\t1.857e-01, KLD:\t2.706e-02\n",
      "seq_loss tensor(0.4458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0201, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4533, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1026, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0172, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0907, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4561, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0198, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1016, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4428, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0217, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1029, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4542, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4433, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1035, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0999, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1018, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0198, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0978, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1016, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4415, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0141, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0976, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0187, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1002, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0186, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0997, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4652, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0997, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4740, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0958, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0910, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4640, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0186, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4651, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1015, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0963, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4838, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0196, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4725, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0155, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4536, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1009, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0913, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "80 epochs, Train Rec:\t3.749e-01, KLD:\t4.674e-02\n",
      "seq_loss tensor(0.4316)\n",
      "v_loss tensor(0.0322)\n",
      "j_loss tensor(0.1578)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0919) \n",
      "\n",
      "seq_loss tensor(0.4212)\n",
      "v_loss tensor(0.0305)\n",
      "j_loss tensor(0.1656)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0923) \n",
      "\n",
      "seq_loss tensor(0.4399)\n",
      "v_loss tensor(0.0277)\n",
      "j_loss tensor(0.1591)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0922) \n",
      "\n",
      "seq_loss tensor(0.4469)\n",
      "v_loss tensor(0.0287)\n",
      "j_loss tensor(0.1518)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0927) \n",
      "\n",
      "seq_loss tensor(0.4114)\n",
      "v_loss tensor(0.0224)\n",
      "j_loss tensor(0.1493)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0926) \n",
      "\n",
      "\n",
      "80 epochs, Valid Rec:\t1.854e-01, KLD:\t2.783e-02\n",
      "seq_loss tensor(0.4325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0266, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0919, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4468, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0308, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0913, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0250, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4277, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0981, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4186, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1015, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0200, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0289, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4076, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0258, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0218, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0215, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4315, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0261, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1014, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1035, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1009, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0200, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1037, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0173, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0955, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4103, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0252, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0998, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0947, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0969, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0980, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4064, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0201, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0932, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4076, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0206, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0925, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3893, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0188, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1011, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "90 epochs, Train Rec:\t2.386e-01, KLD:\t4.559e-02\n",
      "seq_loss tensor(0.3712)\n",
      "v_loss tensor(0.0137)\n",
      "j_loss tensor(0.1246)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892) \n",
      "\n",
      "seq_loss tensor(0.3644)\n",
      "v_loss tensor(0.0132)\n",
      "j_loss tensor(0.1339)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896) \n",
      "\n",
      "seq_loss tensor(0.3799)\n",
      "v_loss tensor(0.0131)\n",
      "j_loss tensor(0.1277)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894) \n",
      "\n",
      "seq_loss tensor(0.3868)\n",
      "v_loss tensor(0.0130)\n",
      "j_loss tensor(0.1229)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902) \n",
      "\n",
      "seq_loss tensor(0.3440)\n",
      "v_loss tensor(0.0072)\n",
      "j_loss tensor(0.1286)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0899) \n",
      "\n",
      "\n",
      "90 epochs, Valid Rec:\t1.533e-01, KLD:\t2.702e-02\n",
      "seq_loss tensor(0.3717, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0706, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3756, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3653, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0803, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3821, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0141, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0800, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3681, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0162, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3693, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0737, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3830, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0767, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3663, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0772, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3682, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3808, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3764, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0750, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3700, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0739, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3572, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3852, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3736, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0734, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0805, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3538, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0740, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0725, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3706, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0708, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3609, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3810, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3825, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0759, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3699, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3745, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0728, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "100 epochs, Train Rec:\t3.641e-01, KLD:\t4.580e-02\n",
      "seq_loss tensor(0.3781)\n",
      "v_loss tensor(0.0214)\n",
      "j_loss tensor(0.1443)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878) \n",
      "\n",
      "seq_loss tensor(0.3712)\n",
      "v_loss tensor(0.0195)\n",
      "j_loss tensor(0.1552)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881) \n",
      "\n",
      "seq_loss tensor(0.3871)\n",
      "v_loss tensor(0.0199)\n",
      "j_loss tensor(0.1463)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882) \n",
      "\n",
      "seq_loss tensor(0.3926)\n",
      "v_loss tensor(0.0178)\n",
      "j_loss tensor(0.1394)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889) \n",
      "\n",
      "seq_loss tensor(0.3597)\n",
      "v_loss tensor(0.0130)\n",
      "j_loss tensor(0.1384)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884) \n",
      "\n",
      "\n",
      "100 epochs, Valid Rec:\t1.630e-01, KLD:\t2.661e-02\n",
      "seq_loss tensor(0.4004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0901, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3849, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3716, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0200, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0932, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3780, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0162, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0834, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0949, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3734, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0944, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3798, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0890, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0908, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0824, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3705, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0166, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0774, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3515, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0808, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3428, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0823, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3733, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0810, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3628, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0796, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3619, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0850, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0159, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0708, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3663, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0148, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0756, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3561, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0782, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3525, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3312, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3534, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0806, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0753, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0145, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0742, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0771, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0779, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3475, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0223, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "110 epochs, Train Rec:\t2.008e-01, KLD:\t4.487e-02\n",
      "seq_loss tensor(0.3267)\n",
      "v_loss tensor(0.0124)\n",
      "j_loss tensor(0.1070)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872) \n",
      "\n",
      "seq_loss tensor(0.3215)\n",
      "v_loss tensor(0.0119)\n",
      "j_loss tensor(0.1163)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875) \n",
      "\n",
      "seq_loss tensor(0.3322)\n",
      "v_loss tensor(0.0125)\n",
      "j_loss tensor(0.1102)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876) \n",
      "\n",
      "seq_loss tensor(0.3460)\n",
      "v_loss tensor(0.0119)\n",
      "j_loss tensor(0.1065)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "seq_loss tensor(0.3048)\n",
      "v_loss tensor(0.0082)\n",
      "j_loss tensor(0.1149)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880) \n",
      "\n",
      "\n",
      "110 epochs, Valid Rec:\t1.352e-01, KLD:\t2.644e-02\n",
      "seq_loss tensor(0.3205, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0619, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3309, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3217, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0516, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0537, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3439, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3269, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3149, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0490, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0556, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3366, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3295, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0533, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0511, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0604, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "120 epochs, Train Rec:\t1.834e-01, KLD:\t4.447e-02\n",
      "seq_loss tensor(0.3032)\n",
      "v_loss tensor(0.0084)\n",
      "j_loss tensor(0.0954)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862) \n",
      "\n",
      "seq_loss tensor(0.3012)\n",
      "v_loss tensor(0.0083)\n",
      "j_loss tensor(0.1036)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864) \n",
      "\n",
      "seq_loss tensor(0.3089)\n",
      "v_loss tensor(0.0085)\n",
      "j_loss tensor(0.0994)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864) \n",
      "\n",
      "seq_loss tensor(0.3303)\n",
      "v_loss tensor(0.0089)\n",
      "j_loss tensor(0.0979)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870) \n",
      "\n",
      "seq_loss tensor(0.2846)\n",
      "v_loss tensor(0.0059)\n",
      "j_loss tensor(0.1092)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865) \n",
      "\n",
      "\n",
      "120 epochs, Valid Rec:\t1.250e-01, KLD:\t2.607e-02\n",
      "seq_loss tensor(0.2951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0432, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3063, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0436, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0441, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0424, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0383, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0408, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0457, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3064, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0450, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0452, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0436, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0434, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0433, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0482, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0497, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0419, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0423, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0442, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2894, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0462, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0423, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3129, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0438, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0449, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0449, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0431, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3027, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0433, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0459, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0415, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0437, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2898, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0438, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2919, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0410, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0230, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1036, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "130 epochs, Train Rec:\t2.909e-01, KLD:\t4.686e-02\n",
      "seq_loss tensor(0.3434)\n",
      "v_loss tensor(0.0322)\n",
      "j_loss tensor(0.1190)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868) \n",
      "\n",
      "seq_loss tensor(0.3395)\n",
      "v_loss tensor(0.0254)\n",
      "j_loss tensor(0.1292)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874) \n",
      "\n",
      "seq_loss tensor(0.3500)\n",
      "v_loss tensor(0.0251)\n",
      "j_loss tensor(0.1234)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869) \n",
      "\n",
      "seq_loss tensor(0.3500)\n",
      "v_loss tensor(0.0211)\n",
      "j_loss tensor(0.1169)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877) \n",
      "\n",
      "seq_loss tensor(0.3282)\n",
      "v_loss tensor(0.0196)\n",
      "j_loss tensor(0.1208)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865) \n",
      "\n",
      "\n",
      "130 epochs, Valid Rec:\t1.473e-01, KLD:\t2.624e-02\n",
      "seq_loss tensor(0.3441, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0229, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0668, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3216, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0192, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0201, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0677, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3409, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0611, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3319, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0187, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0166, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0616, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0189, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0630, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3241, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0597, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0162, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3067, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0608, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3306, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0159, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0568, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0545, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0605, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0607, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3433, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "140 epochs, Train Rec:\t4.204e-01, KLD:\t4.718e-02\n",
      "seq_loss tensor(0.3514)\n",
      "v_loss tensor(0.0188)\n",
      "j_loss tensor(0.1199)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874) \n",
      "\n",
      "seq_loss tensor(0.3526)\n",
      "v_loss tensor(0.0191)\n",
      "j_loss tensor(0.1268)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875) \n",
      "\n",
      "seq_loss tensor(0.3596)\n",
      "v_loss tensor(0.0172)\n",
      "j_loss tensor(0.1212)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876) \n",
      "\n",
      "seq_loss tensor(0.3769)\n",
      "v_loss tensor(0.0194)\n",
      "j_loss tensor(0.1216)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "seq_loss tensor(0.3340)\n",
      "v_loss tensor(0.0107)\n",
      "j_loss tensor(0.1272)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876) \n",
      "\n",
      "\n",
      "140 epochs, Valid Rec:\t1.493e-01, KLD:\t2.642e-02\n",
      "seq_loss tensor(0.3527, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0671, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0697, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3412, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0203, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0631, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3326, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0587, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0559, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3248, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0557, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2990, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0599, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2963, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0513, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0527, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2917, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2988, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2829, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0451, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0131, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0477, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0489, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3033, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0495, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0483, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0480, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0434, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "150 epochs, Train Rec:\t1.477e-01, KLD:\t4.396e-02\n",
      "seq_loss tensor(0.2535)\n",
      "v_loss tensor(0.0062)\n",
      "j_loss tensor(0.0764)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853) \n",
      "\n",
      "seq_loss tensor(0.2556)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0850)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852) \n",
      "\n",
      "seq_loss tensor(0.2593)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0807)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855) \n",
      "\n",
      "seq_loss tensor(0.2689)\n",
      "v_loss tensor(0.0061)\n",
      "j_loss tensor(0.0798)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860) \n",
      "\n",
      "seq_loss tensor(0.2413)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0878)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853) \n",
      "\n",
      "\n",
      "150 epochs, Valid Rec:\t1.035e-01, KLD:\t2.576e-02\n",
      "seq_loss tensor(0.2469, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0267, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0259, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0846, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2533, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0289, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0267, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2582, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0276, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0309, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2759, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0268, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2606, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0285, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0261, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2503, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0259, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2478, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0258, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0266, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2635, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2598, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0276, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0268, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0280, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0313, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2654, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0272, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0276, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0260, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0308, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2640, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0275, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0252, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2635, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0317, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0257, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2671, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0260, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0251, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0245, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2543, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0243, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "160 epochs, Train Rec:\t2.662e-01, KLD:\t4.812e-02\n",
      "seq_loss tensor(0.3950)\n",
      "v_loss tensor(0.0158)\n",
      "j_loss tensor(0.1119)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0918) \n",
      "\n",
      "seq_loss tensor(0.3869)\n",
      "v_loss tensor(0.0143)\n",
      "j_loss tensor(0.1189)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0927) \n",
      "\n",
      "seq_loss tensor(0.3994)\n",
      "v_loss tensor(0.0141)\n",
      "j_loss tensor(0.1143)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0924) \n",
      "\n",
      "seq_loss tensor(0.4251)\n",
      "v_loss tensor(0.0148)\n",
      "j_loss tensor(0.1099)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930) \n",
      "\n",
      "seq_loss tensor(0.3801)\n",
      "v_loss tensor(0.0107)\n",
      "j_loss tensor(0.1178)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0914) \n",
      "\n",
      "\n",
      "160 epochs, Valid Rec:\t1.585e-01, KLD:\t2.781e-02\n",
      "seq_loss tensor(0.3986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0626, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0914, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0933, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0598, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0918, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0612, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0919, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0920, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3978, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0592, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0899, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3936, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0606, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0932, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0933, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3832, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0553, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0915, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3755, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0614, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0924, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3803, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0589, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0934, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3795, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0938, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0620, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0938, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3750, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0579, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0935, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0936, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0924, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0588, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0918, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0564, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0940, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3695, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3740, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0577, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0914, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0907, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3851, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0552, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0919, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3538, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3745, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0915, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3760, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0915, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0535, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0909, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3550, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0916, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0569, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0901, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0907, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0914, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3686, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0911, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3439, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0909, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3854, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0933, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "170 epochs, Train Rec:\t1.515e-01, KLD:\t4.546e-02\n",
      "seq_loss tensor(0.2563)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0765)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "seq_loss tensor(0.2573)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0854)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887) \n",
      "\n",
      "seq_loss tensor(0.2624)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0822)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888) \n",
      "\n",
      "seq_loss tensor(0.2933)\n",
      "v_loss tensor(0.0077)\n",
      "j_loss tensor(0.0810)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892) \n",
      "\n",
      "seq_loss tensor(0.2437)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0914)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880) \n",
      "\n",
      "\n",
      "170 epochs, Valid Rec:\t1.061e-01, KLD:\t2.670e-02\n",
      "seq_loss tensor(0.2677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2504, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0265, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2636, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0290, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2702, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2671, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0261, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2511, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0294, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0315, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2551, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0248, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2572, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0281, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2500, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0272, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0293, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2675, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0275, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2609, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0288, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0296, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2533, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2651, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0270, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0282, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2683, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0272, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2430, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2506, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0310, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0266, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0328, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0889, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0365, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0413, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0675, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0977, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.3969, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(1.0311, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1017, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0426, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0943, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "180 epochs, Train Rec:\t1.509e-01, KLD:\t4.492e-02\n",
      "seq_loss tensor(0.2525)\n",
      "v_loss tensor(0.0076)\n",
      "j_loss tensor(0.0745)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877) \n",
      "\n",
      "seq_loss tensor(0.2541)\n",
      "v_loss tensor(0.0066)\n",
      "j_loss tensor(0.0826)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880) \n",
      "\n",
      "seq_loss tensor(0.2580)\n",
      "v_loss tensor(0.0061)\n",
      "j_loss tensor(0.0793)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881) \n",
      "\n",
      "seq_loss tensor(0.2992)\n",
      "v_loss tensor(0.0076)\n",
      "j_loss tensor(0.0788)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "seq_loss tensor(0.2421)\n",
      "v_loss tensor(0.0053)\n",
      "j_loss tensor(0.0871)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871) \n",
      "\n",
      "\n",
      "180 epochs, Valid Rec:\t1.050e-01, KLD:\t2.648e-02\n",
      "seq_loss tensor(0.2514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2727, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0361, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0308, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0299, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0333, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0307, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0276, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0286, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2439, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0275, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0287, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2564, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0320, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0291, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2511, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0313, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0301, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2575, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0301, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2614, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0280, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2442, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0273, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2690, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0294, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0301, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2591, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0290, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0279, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0300, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2430, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0313, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0312, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0257, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0296, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2626, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0275, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0262, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0276, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2592, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0257, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2640, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0256, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "190 epochs, Train Rec:\t1.329e-01, KLD:\t4.421e-02\n",
      "seq_loss tensor(0.2330)\n",
      "v_loss tensor(0.0052)\n",
      "j_loss tensor(0.0638)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861) \n",
      "\n",
      "seq_loss tensor(0.2358)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0717)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862) \n",
      "\n",
      "seq_loss tensor(0.2384)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0689)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864) \n",
      "\n",
      "seq_loss tensor(0.2754)\n",
      "v_loss tensor(0.0056)\n",
      "j_loss tensor(0.0683)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866) \n",
      "\n",
      "seq_loss tensor(0.2243)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0782)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854) \n",
      "\n",
      "\n",
      "190 epochs, Valid Rec:\t9.538e-02, KLD:\t2.596e-02\n",
      "seq_loss tensor(0.2290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0223, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2319, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2237, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0188, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2207, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2342, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2439, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0187, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2364, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0190, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2472, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0190, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0190, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2310, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0172, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2342, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2270, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0190, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0200, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2386, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0196, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2446, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0186, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0192, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0201, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "200 epochs, Train Rec:\t7.390e-01, KLD:\t8.315e-02\n",
      "seq_loss tensor(0.6308)\n",
      "v_loss tensor(0.0854)\n",
      "j_loss tensor(0.2850)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1202) \n",
      "\n",
      "seq_loss tensor(0.6305)\n",
      "v_loss tensor(0.0785)\n",
      "j_loss tensor(0.2922)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210) \n",
      "\n",
      "seq_loss tensor(0.6398)\n",
      "v_loss tensor(0.0736)\n",
      "j_loss tensor(0.2889)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207) \n",
      "\n",
      "seq_loss tensor(0.6850)\n",
      "v_loss tensor(0.0658)\n",
      "j_loss tensor(0.2641)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1210) \n",
      "\n",
      "seq_loss tensor(0.6224)\n",
      "v_loss tensor(0.0644)\n",
      "j_loss tensor(0.2815)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1192) \n",
      "\n",
      "\n",
      "200 epochs, Valid Rec:\t3.007e-01, KLD:\t3.629e-02\n",
      "seq_loss tensor(0.6263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2557, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1197, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0538, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0470, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1176, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5568, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0562, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1150, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0540, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1751, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1128, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0489, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1096, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0351, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1526, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1103, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0401, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1511, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1075, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0338, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1461, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1044, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4801, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0383, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1395, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1042, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4627, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0302, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1029, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0321, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1196, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1022, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4527, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0311, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1308, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1020, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0224, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0986, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0266, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0974, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0968, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0958, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0233, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1038, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0953, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4122, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0163, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0972, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0965, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3938, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0951, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0925, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0961, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3610, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0872, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0933, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0245, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0830, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0953, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3765, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0826, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0947, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0196, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0949, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0762, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0946, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0946, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0956, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0704, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0936, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0936, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0760, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0935, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3326, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0669, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0929, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0755, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0916, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0698, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0934, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "210 epochs, Train Rec:\t1.272e-01, KLD:\t4.331e-02\n",
      "seq_loss tensor(0.2227)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0635)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842) \n",
      "\n",
      "seq_loss tensor(0.2263)\n",
      "v_loss tensor(0.0052)\n",
      "j_loss tensor(0.0728)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843) \n",
      "\n",
      "seq_loss tensor(0.2280)\n",
      "v_loss tensor(0.0057)\n",
      "j_loss tensor(0.0695)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845) \n",
      "\n",
      "seq_loss tensor(0.2629)\n",
      "v_loss tensor(0.0068)\n",
      "j_loss tensor(0.0685)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847) \n",
      "\n",
      "seq_loss tensor(0.2156)\n",
      "v_loss tensor(0.0052)\n",
      "j_loss tensor(0.0779)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836) \n",
      "\n",
      "\n",
      "210 epochs, Valid Rec:\t9.265e-02, KLD:\t2.539e-02\n",
      "seq_loss tensor(0.2297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2294, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0217, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2151, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2241, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0217, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2132, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2230, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0158, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0846, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0164, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2302, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0212, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2267, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0839, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0212, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0164, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0839, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2259, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0206, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2324, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2306, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2134, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2375, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0188, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2237, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0225, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "220 epochs, Train Rec:\t7.894e-01, KLD:\t5.647e-02\n",
      "seq_loss tensor(0.9463)\n",
      "v_loss tensor(0.0614)\n",
      "j_loss tensor(0.2197)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1025) \n",
      "\n",
      "seq_loss tensor(0.9427)\n",
      "v_loss tensor(0.0541)\n",
      "j_loss tensor(0.2339)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1042) \n",
      "\n",
      "seq_loss tensor(0.9551)\n",
      "v_loss tensor(0.0505)\n",
      "j_loss tensor(0.2233)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1032) \n",
      "\n",
      "seq_loss tensor(0.9823)\n",
      "v_loss tensor(0.0567)\n",
      "j_loss tensor(0.2112)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1029) \n",
      "\n",
      "seq_loss tensor(0.9215)\n",
      "v_loss tensor(0.0441)\n",
      "j_loss tensor(0.2203)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1043) \n",
      "\n",
      "\n",
      "220 epochs, Valid Rec:\t3.691e-01, KLD:\t3.117e-02\n",
      "seq_loss tensor(0.9363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0623, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1014, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9248, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1022, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0646, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1822, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1016, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1020, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0509, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1010, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0580, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1033, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0576, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1695, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1045, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1018, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8811, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0519, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1019, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8868, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1001, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8634, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0518, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1647, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0989, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8684, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1686, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1024, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8443, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0452, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1700, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0992, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0550, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1706, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1001, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8706, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1691, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1014, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0986, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0448, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1673, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0982, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0600, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1672, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0999, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0526, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0987, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8303, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0584, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1665, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0991, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1010, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8373, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0532, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1662, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0989, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1588, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0992, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0504, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1624, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1000, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7749, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0464, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1598, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1009, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8839, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0531, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1641, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0996, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1595, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0998, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7573, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0454, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1504, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0996, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1583, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0993, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0476, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1519, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0984, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7791, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0426, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1536, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0999, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7399, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0457, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1594, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0987, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0449, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1510, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0995, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0408, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1570, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0995, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "230 epochs, Train Rec:\t2.189e-01, KLD:\t4.785e-02\n",
      "seq_loss tensor(0.3408)\n",
      "v_loss tensor(0.0377)\n",
      "j_loss tensor(0.1624)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0927) \n",
      "\n",
      "seq_loss tensor(0.3361)\n",
      "v_loss tensor(0.0328)\n",
      "j_loss tensor(0.1753)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0934) \n",
      "\n",
      "seq_loss tensor(0.3481)\n",
      "v_loss tensor(0.0333)\n",
      "j_loss tensor(0.1680)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0933) \n",
      "\n",
      "seq_loss tensor(0.3743)\n",
      "v_loss tensor(0.0342)\n",
      "j_loss tensor(0.1583)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0936) \n",
      "\n",
      "seq_loss tensor(0.3292)\n",
      "v_loss tensor(0.0229)\n",
      "j_loss tensor(0.1543)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0935) \n",
      "\n",
      "\n",
      "230 epochs, Valid Rec:\t1.632e-01, KLD:\t2.812e-02\n",
      "seq_loss tensor(0.3410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0300, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0932, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0858, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0929, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0925, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0148, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0815, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0914, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0889, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0939, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0798, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0923, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3415, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0687, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0914, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0189, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0280, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0939, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0928, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0341, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0838, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3394, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0718, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0939, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3319, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0337, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0845, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0946, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0431, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0932, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3437, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0827, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0943, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0942, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0229, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0944, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0813, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0925, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0702, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0935, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0938, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0766, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0928, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0717, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0935, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3295, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0938, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0716, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0934, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0205, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0711, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0666, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0932, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3490, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0155, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0655, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0937, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0705, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0922, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0729, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0921, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3249, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0685, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0928, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0921, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0684, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0925, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0639, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0927, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0653, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0941, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "240 epochs, Train Rec:\t1.632e-01, KLD:\t4.597e-02\n",
      "seq_loss tensor(0.2641)\n",
      "v_loss tensor(0.0114)\n",
      "j_loss tensor(0.0908)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891) \n",
      "\n",
      "seq_loss tensor(0.2643)\n",
      "v_loss tensor(0.0110)\n",
      "j_loss tensor(0.1006)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895) \n",
      "\n",
      "seq_loss tensor(0.2718)\n",
      "v_loss tensor(0.0108)\n",
      "j_loss tensor(0.0986)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895) \n",
      "\n",
      "seq_loss tensor(0.3004)\n",
      "v_loss tensor(0.0103)\n",
      "j_loss tensor(0.0926)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0899) \n",
      "\n",
      "seq_loss tensor(0.2510)\n",
      "v_loss tensor(0.0059)\n",
      "j_loss tensor(0.0997)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894) \n",
      "\n",
      "\n",
      "240 epochs, Valid Rec:\t1.135e-01, KLD:\t2.697e-02\n",
      "seq_loss tensor(0.2688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0443, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2619, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0393, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2570, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0379, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0905, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2736, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0444, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2682, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0403, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2615, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0424, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2660, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0432, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0906, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2669, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0380, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2607, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0421, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0415, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0409, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0428, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0435, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2752, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0420, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0405, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2598, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0428, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2665, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0373, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2593, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0426, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2719, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0406, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0892, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0358, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0458, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2542, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0396, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0900, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2607, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0379, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0899, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0396, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0904, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0401, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2683, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0380, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0904, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0386, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0893, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2718, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0370, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0902, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2662, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0388, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0396, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0387, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2629, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0372, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2665, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0407, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0336, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0895, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "250 epochs, Train Rec:\t1.373e-01, KLD:\t4.522e-02\n",
      "seq_loss tensor(0.2369)\n",
      "v_loss tensor(0.0067)\n",
      "j_loss tensor(0.0759)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "seq_loss tensor(0.2404)\n",
      "v_loss tensor(0.0075)\n",
      "j_loss tensor(0.0847)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885) \n",
      "\n",
      "seq_loss tensor(0.2438)\n",
      "v_loss tensor(0.0066)\n",
      "j_loss tensor(0.0824)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886) \n",
      "\n",
      "seq_loss tensor(0.2778)\n",
      "v_loss tensor(0.0072)\n",
      "j_loss tensor(0.0789)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890) \n",
      "\n",
      "seq_loss tensor(0.2292)\n",
      "v_loss tensor(0.0056)\n",
      "j_loss tensor(0.0911)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883) \n",
      "\n",
      "\n",
      "250 epochs, Valid Rec:\t1.009e-01, KLD:\t2.668e-02\n",
      "seq_loss tensor(0.2461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0244, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0894, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0275, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0885, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2543, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0265, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0886, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0890, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2286, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0227, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0898, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0233, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0281, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2294, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2384, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0268, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0221, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0243, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2443, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0888, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0243, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2324, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0241, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0275, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2350, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0249, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0225, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2525, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0273, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0883, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2564, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2350, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0242, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0231, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0251, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2320, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0878, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0244, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0879, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "260 epochs, Train Rec:\t1.503e-01, KLD:\t4.523e-02\n",
      "seq_loss tensor(0.2302)\n",
      "v_loss tensor(0.0064)\n",
      "j_loss tensor(0.0698)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0874) \n",
      "\n",
      "seq_loss tensor(0.2355)\n",
      "v_loss tensor(0.0062)\n",
      "j_loss tensor(0.0776)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876) \n",
      "\n",
      "seq_loss tensor(0.2366)\n",
      "v_loss tensor(0.0062)\n",
      "j_loss tensor(0.0755)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0877) \n",
      "\n",
      "seq_loss tensor(0.2707)\n",
      "v_loss tensor(0.0070)\n",
      "j_loss tensor(0.0739)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880) \n",
      "\n",
      "seq_loss tensor(0.2226)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0850)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0871) \n",
      "\n",
      "\n",
      "260 epochs, Valid Rec:\t9.694e-02, KLD:\t2.639e-02\n",
      "seq_loss tensor(0.2196, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0205, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2268, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0881, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0224, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2216, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0205, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0227, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2342, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0240, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0221, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0233, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2299, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0236, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0866, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0223, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0212, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2326, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0224, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0221, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2264, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2350, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0205, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0873, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0875, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "270 epochs, Train Rec:\t1.192e-01, KLD:\t4.372e-02\n",
      "seq_loss tensor(0.2109)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0562)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848) \n",
      "\n",
      "seq_loss tensor(0.2160)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0640)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849) \n",
      "\n",
      "seq_loss tensor(0.2174)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0625)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851) \n",
      "\n",
      "seq_loss tensor(0.2519)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0607)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854) \n",
      "\n",
      "seq_loss tensor(0.2036)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0727)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845) \n",
      "\n",
      "\n",
      "270 epochs, Valid Rec:\t8.661e-02, KLD:\t2.560e-02\n",
      "seq_loss tensor(0.2015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0166, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0162, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0145, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2196, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2062, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0846, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2248, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0164, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1995, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0159, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0864, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0159, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "280 epochs, Train Rec:\t1.266e-01, KLD:\t4.433e-02\n",
      "seq_loss tensor(0.2135)\n",
      "v_loss tensor(0.0062)\n",
      "j_loss tensor(0.0628)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856) \n",
      "\n",
      "seq_loss tensor(0.2186)\n",
      "v_loss tensor(0.0064)\n",
      "j_loss tensor(0.0716)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857) \n",
      "\n",
      "seq_loss tensor(0.2207)\n",
      "v_loss tensor(0.0061)\n",
      "j_loss tensor(0.0713)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858) \n",
      "\n",
      "seq_loss tensor(0.2517)\n",
      "v_loss tensor(0.0062)\n",
      "j_loss tensor(0.0685)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862) \n",
      "\n",
      "seq_loss tensor(0.2073)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0793)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854) \n",
      "\n",
      "\n",
      "280 epochs, Valid Rec:\t9.013e-02, KLD:\t2.584e-02\n",
      "seq_loss tensor(0.2198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0221, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0872, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0198, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0870, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0862, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0868, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2148, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0189, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0865, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0867, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2175, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0180, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0859, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2089, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0853, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2151, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0180, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0203, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0851, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0163, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2046, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0173, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0855, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "290 epochs, Train Rec:\t1.183e-01, KLD:\t4.359e-02\n",
      "seq_loss tensor(0.2049)\n",
      "v_loss tensor(0.0054)\n",
      "j_loss tensor(0.0599)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849) \n",
      "\n",
      "seq_loss tensor(0.2098)\n",
      "v_loss tensor(0.0053)\n",
      "j_loss tensor(0.0669)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849) \n",
      "\n",
      "seq_loss tensor(0.2118)\n",
      "v_loss tensor(0.0055)\n",
      "j_loss tensor(0.0670)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850) \n",
      "\n",
      "seq_loss tensor(0.2442)\n",
      "v_loss tensor(0.0059)\n",
      "j_loss tensor(0.0638)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854) \n",
      "\n",
      "seq_loss tensor(0.1994)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0745)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848) \n",
      "\n",
      "\n",
      "290 epochs, Valid Rec:\t8.616e-02, KLD:\t2.562e-02\n",
      "seq_loss tensor(0.2071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0155, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0183, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2055, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2044, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0840, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0162, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2168, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2018, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0203, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0839, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0172, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2064, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0852, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0170, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2058, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0860, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2178, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0180, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0856, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2089, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0155, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0173, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0843, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0163, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0846, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2143, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "300 epochs, Train Rec:\t1.101e-01, KLD:\t4.285e-02\n",
      "seq_loss tensor(0.1980)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0507)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828) \n",
      "\n",
      "seq_loss tensor(0.2023)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0588)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828) \n",
      "\n",
      "seq_loss tensor(0.2037)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0574)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0831) \n",
      "\n",
      "seq_loss tensor(0.2360)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0567)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834) \n",
      "\n",
      "seq_loss tensor(0.1922)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0674)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826) \n",
      "\n",
      "\n",
      "300 epochs, Valid Rec:\t8.104e-02, KLD:\t2.500e-02\n",
      "seq_loss tensor(0.2097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0145, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1865, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0830, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0155, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1885, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0824, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0830, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0829, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0830, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0829, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1839, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "310 epochs, Train Rec:\t1.276e-01, KLD:\t4.311e-02\n",
      "seq_loss tensor(0.2085)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0685)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832) \n",
      "\n",
      "seq_loss tensor(0.2140)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0764)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832) \n",
      "\n",
      "seq_loss tensor(0.2150)\n",
      "v_loss tensor(0.0061)\n",
      "j_loss tensor(0.0766)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833) \n",
      "\n",
      "seq_loss tensor(0.2490)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0719)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838) \n",
      "\n",
      "seq_loss tensor(0.2045)\n",
      "v_loss tensor(0.0055)\n",
      "j_loss tensor(0.0817)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0830) \n",
      "\n",
      "\n",
      "310 epochs, Valid Rec:\t9.019e-02, KLD:\t2.510e-02\n",
      "seq_loss tensor(0.2174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0217, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2124, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0227, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0831, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0196, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0831, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1998, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0829, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2044, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0831, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0219, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2046, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1934, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0840, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0189, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0187, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2099, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0186, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0841, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0198, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1978, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0830, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0840, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2142, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0189, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2127, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0839, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0849, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "320 epochs, Train Rec:\t1.050e-01, KLD:\t4.208e-02\n",
      "seq_loss tensor(0.1893)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0497)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820) \n",
      "\n",
      "seq_loss tensor(0.1945)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0575)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820) \n",
      "\n",
      "seq_loss tensor(0.1952)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0572)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822) \n",
      "\n",
      "seq_loss tensor(0.2259)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0555)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0824) \n",
      "\n",
      "seq_loss tensor(0.1838)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0658)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816) \n",
      "\n",
      "\n",
      "320 epochs, Valid Rec:\t7.801e-02, KLD:\t2.472e-02\n",
      "seq_loss tensor(0.1910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1918, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0824, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1852, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1905, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0830, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0131, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1887, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0824, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0824, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1882, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2050, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1885, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "330 epochs, Train Rec:\t1.015e-01, KLD:\t4.173e-02\n",
      "seq_loss tensor(0.1849)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0515)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812) \n",
      "\n",
      "seq_loss tensor(0.1902)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0592)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812) \n",
      "\n",
      "seq_loss tensor(0.1906)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0588)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814) \n",
      "\n",
      "seq_loss tensor(0.2240)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0573)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817) \n",
      "\n",
      "seq_loss tensor(0.1797)\n",
      "v_loss tensor(0.0033)\n",
      "j_loss tensor(0.0702)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807) \n",
      "\n",
      "\n",
      "330 epochs, Valid Rec:\t7.737e-02, KLD:\t2.449e-02\n",
      "seq_loss tensor(0.1927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0824, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1859, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0455, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0836, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1989, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.7970, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(1.0718, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.9081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0930, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.2681, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(1.1684, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1380, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.2882, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(1.2509, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2250, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.2333, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.9655, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2632, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1599, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.6777, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.5059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1367, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.4113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.3220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0696, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0830, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2548, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2641, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9775, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0618, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2339, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2495, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0724, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2230, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2377, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8292, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0628, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1912, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2232, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9030, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0853, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2089, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1981, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7810, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7664, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0661, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7362, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0699, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1675, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0455, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1632, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6150, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2256, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1571, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6314, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0667, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2015, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1541, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0575, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1462, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0539, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1672, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1387, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1694, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1358, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5134, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0660, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1835, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1320, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0307, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1533, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1262, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "340 epochs, Train Rec:\t1.042e-01, KLD:\t4.194e-02\n",
      "seq_loss tensor(0.1853)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0533)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812) \n",
      "\n",
      "seq_loss tensor(0.1906)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0628)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813) \n",
      "\n",
      "seq_loss tensor(0.1905)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0615)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815) \n",
      "\n",
      "seq_loss tensor(0.2231)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0590)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818) \n",
      "\n",
      "seq_loss tensor(0.1795)\n",
      "v_loss tensor(0.0054)\n",
      "j_loss tensor(0.0698)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811) \n",
      "\n",
      "\n",
      "340 epochs, Valid Rec:\t7.824e-02, KLD:\t2.453e-02\n",
      "seq_loss tensor(0.1894, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1884, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1907, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1868, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1960, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1932, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1899, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1895, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "350 epochs, Train Rec:\t9.817e-02, KLD:\t4.133e-02\n",
      "seq_loss tensor(0.1791)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0519)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805) \n",
      "\n",
      "seq_loss tensor(0.1850)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0611)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805) \n",
      "\n",
      "seq_loss tensor(0.1851)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0601)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807) \n",
      "\n",
      "seq_loss tensor(0.2164)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0582)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810) \n",
      "\n",
      "seq_loss tensor(0.1764)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0698)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803) \n",
      "\n",
      "\n",
      "350 epochs, Valid Rec:\t7.610e-02, KLD:\t2.429e-02\n",
      "seq_loss tensor(0.1848, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1806, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1781, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1779, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1768, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1670, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1829, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1838, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1944, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1888, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1808, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1786, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1736, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1810, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1734, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1900, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "360 epochs, Train Rec:\t9.893e-02, KLD:\t4.114e-02\n",
      "seq_loss tensor(0.1774)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0487)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799) \n",
      "\n",
      "seq_loss tensor(0.1831)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0580)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799) \n",
      "\n",
      "seq_loss tensor(0.1828)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0558)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801) \n",
      "\n",
      "seq_loss tensor(0.2197)\n",
      "v_loss tensor(0.0041)\n",
      "j_loss tensor(0.0546)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803) \n",
      "\n",
      "seq_loss tensor(0.1734)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0670)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796) \n",
      "\n",
      "\n",
      "360 epochs, Valid Rec:\t7.472e-02, KLD:\t2.410e-02\n",
      "seq_loss tensor(0.1759, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1899, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1746, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1863, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1797, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1717, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1678, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1836, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0131, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1690, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1682, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1770, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1774, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1766, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1709, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0004, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "370 epochs, Train Rec:\t3.662e-01, KLD:\t5.176e-02\n",
      "seq_loss tensor(0.2625)\n",
      "v_loss tensor(0.0098)\n",
      "j_loss tensor(0.0966)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833) \n",
      "\n",
      "seq_loss tensor(0.2656)\n",
      "v_loss tensor(0.0090)\n",
      "j_loss tensor(0.1100)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833) \n",
      "\n",
      "seq_loss tensor(0.2676)\n",
      "v_loss tensor(0.0094)\n",
      "j_loss tensor(0.1045)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835) \n",
      "\n",
      "seq_loss tensor(0.3039)\n",
      "v_loss tensor(0.0086)\n",
      "j_loss tensor(0.0968)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838) \n",
      "\n",
      "seq_loss tensor(0.2567)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.1079)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828) \n",
      "\n",
      "\n",
      "370 epochs, Valid Rec:\t1.154e-01, KLD:\t2.511e-02\n",
      "seq_loss tensor(0.2651, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0440, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2506, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0313, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2506, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0376, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0322, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0300, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2471, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0282, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0847, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0298, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2266, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0231, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0248, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2309, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0842, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2082, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2091, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0257, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0187, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2149, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0188, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0288, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0188, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2082, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0219, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0145, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0172, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1800, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1837, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0146, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1905, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1918, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "380 epochs, Train Rec:\t1.004e-01, KLD:\t4.176e-02\n",
      "seq_loss tensor(0.1789)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0513)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804) \n",
      "\n",
      "seq_loss tensor(0.1832)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0593)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805) \n",
      "\n",
      "seq_loss tensor(0.1840)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0585)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806) \n",
      "\n",
      "seq_loss tensor(0.2197)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0564)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809) \n",
      "\n",
      "seq_loss tensor(0.1722)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0672)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800) \n",
      "\n",
      "\n",
      "380 epochs, Valid Rec:\t7.552e-02, KLD:\t2.425e-02\n",
      "seq_loss tensor(0.1813, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0820, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1762, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1821, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1628, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1721, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1753, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1696, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1688, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1845, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1840, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1784, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1798, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1789, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1775, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "390 epochs, Train Rec:\t9.152e-02, KLD:\t4.102e-02\n",
      "seq_loss tensor(0.1666)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0551)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803) \n",
      "\n",
      "seq_loss tensor(0.1724)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0632)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803) \n",
      "\n",
      "seq_loss tensor(0.1717)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0628)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805) \n",
      "\n",
      "seq_loss tensor(0.2087)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0606)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809) \n",
      "\n",
      "seq_loss tensor(0.1621)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0706)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798) \n",
      "\n",
      "\n",
      "390 epochs, Valid Rec:\t7.326e-02, KLD:\t2.422e-02\n",
      "seq_loss tensor(0.1646, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1715, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1780, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1630, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1692, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1756, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1752, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1601, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1701, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1671, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1625, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1630, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1624, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1645, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1709, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1705, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1708, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1694, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1582, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1607, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1702, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1745, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1507, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "400 epochs, Train Rec:\t3.698e-01, KLD:\t7.537e-02\n",
      "seq_loss tensor(0.7004)\n",
      "v_loss tensor(0.0985)\n",
      "j_loss tensor(0.2145)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3165) \n",
      "\n",
      "seq_loss tensor(0.7005)\n",
      "v_loss tensor(0.0966)\n",
      "j_loss tensor(0.2272)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3168) \n",
      "\n",
      "seq_loss tensor(0.7024)\n",
      "v_loss tensor(0.0965)\n",
      "j_loss tensor(0.2259)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3167) \n",
      "\n",
      "seq_loss tensor(0.7424)\n",
      "v_loss tensor(0.0867)\n",
      "j_loss tensor(0.2160)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3162) \n",
      "\n",
      "seq_loss tensor(0.6732)\n",
      "v_loss tensor(0.0852)\n",
      "j_loss tensor(0.2214)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3209) \n",
      "\n",
      "\n",
      "400 epochs, Valid Rec:\t3.067e-01, KLD:\t9.567e-02\n",
      "seq_loss tensor(0.7039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0831, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1688, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3154, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0583, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1976, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2936, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6777, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0634, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1587, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6813, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0643, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1749, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2521, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0547, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1613, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2361, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5655, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0515, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1625, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2153, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1485, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1971, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0437, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1477, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1897, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0468, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1499, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4987, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0402, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1325, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1666, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0457, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1355, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1569, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4706, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0358, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1324, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1529, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1205, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1428, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0393, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1388, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0377, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1221, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1329, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0333, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1146, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1271, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0392, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1236, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1036, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0297, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0963, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1143, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3755, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0334, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0947, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1119, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3375, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0299, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1067, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0269, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1080, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0297, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0797, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1037, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0212, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0773, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0996, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3137, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0226, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0977, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0293, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0769, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0968, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0230, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0768, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0957, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0280, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0820, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0942, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2899, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0233, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0937, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2918, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0219, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0721, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0941, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0695, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0935, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0731, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0928, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0664, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0928, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0910, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "410 epochs, Train Rec:\t8.846e-02, KLD:\t4.053e-02\n",
      "seq_loss tensor(0.1591)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0464)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789) \n",
      "\n",
      "seq_loss tensor(0.1649)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0538)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788) \n",
      "\n",
      "seq_loss tensor(0.1640)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0530)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791) \n",
      "\n",
      "seq_loss tensor(0.2073)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0504)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794) \n",
      "\n",
      "seq_loss tensor(0.1555)\n",
      "v_loss tensor(0.0056)\n",
      "j_loss tensor(0.0610)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784) \n",
      "\n",
      "\n",
      "410 epochs, Valid Rec:\t6.840e-02, KLD:\t2.378e-02\n",
      "seq_loss tensor(0.1571, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1609, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1564, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1649, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1491, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1703, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1535, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1628, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1595, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1521, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1616, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1677, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1615, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1654, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1634, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1641, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1778, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "420 epochs, Train Rec:\t8.463e-02, KLD:\t4.007e-02\n",
      "seq_loss tensor(0.1541)\n",
      "v_loss tensor(0.0024)\n",
      "j_loss tensor(0.0422)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776) \n",
      "\n",
      "seq_loss tensor(0.1596)\n",
      "v_loss tensor(0.0026)\n",
      "j_loss tensor(0.0501)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776) \n",
      "\n",
      "seq_loss tensor(0.1587)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0492)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779) \n",
      "\n",
      "seq_loss tensor(0.1996)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0477)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782) \n",
      "\n",
      "seq_loss tensor(0.1501)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0578)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772) \n",
      "\n",
      "\n",
      "420 epochs, Valid Rec:\t6.541e-02, KLD:\t2.342e-02\n",
      "seq_loss tensor(0.1513, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1627, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1701, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1550, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1551, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1685, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1554, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1507, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1572, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1610, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1539, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1579, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1490, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1592, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1599, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1512, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1546, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1569, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1544, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "430 epochs, Train Rec:\t3.724e-01, KLD:\t5.753e-02\n",
      "seq_loss tensor(0.3373)\n",
      "v_loss tensor(0.0579)\n",
      "j_loss tensor(0.1837)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1001) \n",
      "\n",
      "seq_loss tensor(0.3399)\n",
      "v_loss tensor(0.0459)\n",
      "j_loss tensor(0.1905)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1003) \n",
      "\n",
      "seq_loss tensor(0.3451)\n",
      "v_loss tensor(0.0455)\n",
      "j_loss tensor(0.1911)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1007) \n",
      "\n",
      "seq_loss tensor(0.3882)\n",
      "v_loss tensor(0.0447)\n",
      "j_loss tensor(0.1650)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1010) \n",
      "\n",
      "seq_loss tensor(0.3436)\n",
      "v_loss tensor(0.0252)\n",
      "j_loss tensor(0.1741)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1003) \n",
      "\n",
      "\n",
      "430 epochs, Valid Rec:\t1.735e-01, KLD:\t3.029e-02\n",
      "seq_loss tensor(0.3374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0503, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1336, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1005, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0267, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0994, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0952, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0326, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1322, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0917, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0331, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0859, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0913, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0250, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1201, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0912, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0294, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0880, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0163, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0252, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1233, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0891, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0336, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0992, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0887, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0186, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0884, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2737, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0273, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0950, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0876, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0357, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0854, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2640, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0953, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0869, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0613, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0845, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0853, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0839, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2349, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0821, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0844, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2543, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0231, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0780, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2309, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0636, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0190, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0828, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0952, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0496, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0168, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0596, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0536, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2373, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0785, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1320, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2557, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1551, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.3330, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0860, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1978, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0848, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1156, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2607, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0927, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1402, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0896, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2770, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0763, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1280, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0863, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2762, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1278, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1866, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2475, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0340, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0812, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0857, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0525, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0882, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2595, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0850, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "440 epochs, Train Rec:\t8.063e-02, KLD:\t3.958e-02\n",
      "seq_loss tensor(0.1484)\n",
      "v_loss tensor(0.0025)\n",
      "j_loss tensor(0.0391)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774) \n",
      "\n",
      "seq_loss tensor(0.1536)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0457)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773) \n",
      "\n",
      "seq_loss tensor(0.1528)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0462)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776) \n",
      "\n",
      "seq_loss tensor(0.1933)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0446)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779) \n",
      "\n",
      "seq_loss tensor(0.1461)\n",
      "v_loss tensor(0.0058)\n",
      "j_loss tensor(0.0551)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769) \n",
      "\n",
      "\n",
      "440 epochs, Valid Rec:\t6.282e-02, KLD:\t2.333e-02\n",
      "seq_loss tensor(0.1462, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1584, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1502, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1441, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1533, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1493, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1540, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1515, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1474, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1471, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1413, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "450 epochs, Train Rec:\t5.561e-01, KLD:\t7.321e-02\n",
      "seq_loss tensor(2.9952)\n",
      "v_loss tensor(0.1035)\n",
      "j_loss tensor(0.2698)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4193) \n",
      "\n",
      "seq_loss tensor(3.1026)\n",
      "v_loss tensor(0.1081)\n",
      "j_loss tensor(0.2813)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4202) \n",
      "\n",
      "seq_loss tensor(3.0130)\n",
      "v_loss tensor(0.0979)\n",
      "j_loss tensor(0.2740)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4197) \n",
      "\n",
      "seq_loss tensor(3.0599)\n",
      "v_loss tensor(0.1166)\n",
      "j_loss tensor(0.2778)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4196) \n",
      "\n",
      "seq_loss tensor(3.1467)\n",
      "v_loss tensor(0.0875)\n",
      "j_loss tensor(0.2520)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4255) \n",
      "\n",
      "\n",
      "450 epochs, Valid Rec:\t1.036e+00, KLD:\t1.268e-01\n",
      "seq_loss tensor(3.0729, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1113, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2423, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4207, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(2.4572, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1679, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.3748, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3874, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.8442, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1933, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.4243, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3667, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.9027, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1121, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2689, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.9787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.2204, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2612, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3903, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.6423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1426, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2349, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.4279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0870, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2292, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3479, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3787, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0827, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2480, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3237, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.3594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0970, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2373, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.3023, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.2702, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0864, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2819, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1548, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0740, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2179, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2583, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1487, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0789, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2437, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0787, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1965, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2297, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0785, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1910, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2093, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0686, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2071, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9881, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0624, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1795, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1957, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0683, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1922, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0573, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8288, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0801, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1735, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0652, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1663, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1684, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7899, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0796, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1606, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7686, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0429, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1550, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1592, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1684, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1529, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0649, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1527, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1443, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.7260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0637, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1549, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1383, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6689, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0625, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1507, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1383, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0663, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1479, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1381, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.6197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0658, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1447, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1341, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1448, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1304, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0506, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1338, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1270, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0665, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1369, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1247, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5743, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0585, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1333, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1217, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.5456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0682, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1433, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1221, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.4908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0804, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1178, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "460 epochs, Train Rec:\t1.502e-01, KLD:\t4.651e-02\n",
      "seq_loss tensor(0.1793)\n",
      "v_loss tensor(0.0172)\n",
      "j_loss tensor(0.0862)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0829) \n",
      "\n",
      "seq_loss tensor(0.1851)\n",
      "v_loss tensor(0.0178)\n",
      "j_loss tensor(0.0913)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0832) \n",
      "\n",
      "seq_loss tensor(0.1862)\n",
      "v_loss tensor(0.0163)\n",
      "j_loss tensor(0.0904)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833) \n",
      "\n",
      "seq_loss tensor(0.2290)\n",
      "v_loss tensor(0.0208)\n",
      "j_loss tensor(0.0880)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835) \n",
      "\n",
      "seq_loss tensor(0.1786)\n",
      "v_loss tensor(0.0101)\n",
      "j_loss tensor(0.0956)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825) \n",
      "\n",
      "\n",
      "460 epochs, Valid Rec:\t8.993e-02, KLD:\t2.504e-02\n",
      "seq_loss tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0388, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0336, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0837, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1799, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0341, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0834, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1907, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0365, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0833, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1864, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0364, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0827, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0350, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1875, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0342, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0328, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0827, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0280, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1731, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0324, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1711, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0314, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0815, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1823, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0301, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1754, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0294, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0816, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1816, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0300, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1680, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0311, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1687, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0292, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1720, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0314, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0291, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1649, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0273, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0809, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1724, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1723, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0303, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1748, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1657, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0272, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1644, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0290, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0251, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1692, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0259, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1776, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1698, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1695, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1739, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0251, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1712, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0244, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1649, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1907, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0204, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "470 epochs, Train Rec:\t8.462e-02, KLD:\t4.044e-02\n",
      "seq_loss tensor(0.1477)\n",
      "v_loss tensor(0.0049)\n",
      "j_loss tensor(0.0527)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785) \n",
      "\n",
      "seq_loss tensor(0.1530)\n",
      "v_loss tensor(0.0048)\n",
      "j_loss tensor(0.0604)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785) \n",
      "\n",
      "seq_loss tensor(0.1530)\n",
      "v_loss tensor(0.0049)\n",
      "j_loss tensor(0.0573)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787) \n",
      "\n",
      "seq_loss tensor(0.1966)\n",
      "v_loss tensor(0.0069)\n",
      "j_loss tensor(0.0578)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790) \n",
      "\n",
      "seq_loss tensor(0.1445)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0648)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781) \n",
      "\n",
      "\n",
      "470 epochs, Valid Rec:\t6.725e-02, KLD:\t2.368e-02\n",
      "seq_loss tensor(0.1520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1446, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1466, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1583, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1602, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1565, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1378, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1527, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1473, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1415, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1450, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1620, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1451, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1468, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1542, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1476, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1510, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1485, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1460, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1552, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1489, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "480 epochs, Train Rec:\t9.309e-02, KLD:\t4.015e-02\n",
      "seq_loss tensor(0.1625)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0552)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776) \n",
      "\n",
      "seq_loss tensor(0.1621)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0617)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777) \n",
      "\n",
      "seq_loss tensor(0.1666)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0599)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778) \n",
      "\n",
      "seq_loss tensor(0.2042)\n",
      "v_loss tensor(0.0053)\n",
      "j_loss tensor(0.0612)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781) \n",
      "\n",
      "seq_loss tensor(0.1573)\n",
      "v_loss tensor(0.0068)\n",
      "j_loss tensor(0.0672)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770) \n",
      "\n",
      "\n",
      "480 epochs, Valid Rec:\t7.135e-02, KLD:\t2.340e-02\n",
      "seq_loss tensor(0.1475, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1668, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1662, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1614, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0159, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1558, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1574, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0146, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1538, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1667, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1658, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1668, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1524, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1560, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1631, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1563, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1591, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1548, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1614, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0138, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1569, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1587, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0148, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1647, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1679, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "490 epochs, Train Rec:\t8.587e-02, KLD:\t3.970e-02\n",
      "seq_loss tensor(0.1495)\n",
      "v_loss tensor(0.0048)\n",
      "j_loss tensor(0.0544)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767) \n",
      "\n",
      "seq_loss tensor(0.1537)\n",
      "v_loss tensor(0.0052)\n",
      "j_loss tensor(0.0615)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768) \n",
      "\n",
      "seq_loss tensor(0.1535)\n",
      "v_loss tensor(0.0050)\n",
      "j_loss tensor(0.0606)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769) \n",
      "\n",
      "seq_loss tensor(0.1983)\n",
      "v_loss tensor(0.0053)\n",
      "j_loss tensor(0.0587)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772) \n",
      "\n",
      "seq_loss tensor(0.1459)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0677)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761) \n",
      "\n",
      "\n",
      "490 epochs, Valid Rec:\t6.812e-02, KLD:\t2.313e-02\n",
      "seq_loss tensor(0.1471, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1521, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1541, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1501, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1580, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1474, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1536, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1500, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1511, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1498, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1566, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1521, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1487, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1483, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1448, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1503, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1490, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1529, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1698, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "500 epochs, Train Rec:\t7.770e-02, KLD:\t3.935e-02\n",
      "seq_loss tensor(0.1402)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0435)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760) \n",
      "\n",
      "seq_loss tensor(0.1447)\n",
      "v_loss tensor(0.0033)\n",
      "j_loss tensor(0.0502)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760) \n",
      "\n",
      "seq_loss tensor(0.1449)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0499)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762) \n",
      "\n",
      "seq_loss tensor(0.1883)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0494)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765) \n",
      "\n",
      "seq_loss tensor(0.1388)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0592)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754) \n",
      "\n",
      "\n",
      "500 epochs, Valid Rec:\t6.195e-02, KLD:\t2.291e-02\n",
      "seq_loss tensor(0.1428, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1468, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1443, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1432, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1441, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1427, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1449, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1319, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1437, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1395, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1394, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1348, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1463, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1427, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1389, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "510 epochs, Train Rec:\t1.221e-01, KLD:\t4.147e-02\n",
      "seq_loss tensor(0.1626)\n",
      "v_loss tensor(0.0130)\n",
      "j_loss tensor(0.0910)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786) \n",
      "\n",
      "seq_loss tensor(0.1667)\n",
      "v_loss tensor(0.0117)\n",
      "j_loss tensor(0.0978)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787) \n",
      "\n",
      "seq_loss tensor(0.1672)\n",
      "v_loss tensor(0.0117)\n",
      "j_loss tensor(0.0939)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789) \n",
      "\n",
      "seq_loss tensor(0.2119)\n",
      "v_loss tensor(0.0128)\n",
      "j_loss tensor(0.0911)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794) \n",
      "\n",
      "seq_loss tensor(0.1554)\n",
      "v_loss tensor(0.0121)\n",
      "j_loss tensor(0.0977)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783) \n",
      "\n",
      "\n",
      "510 epochs, Valid Rec:\t8.419e-02, KLD:\t2.374e-02\n",
      "seq_loss tensor(0.1657, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0416, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1604, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0393, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1606, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0427, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1696, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0385, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1694, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0377, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1630, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0428, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1623, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0342, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0419, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0374, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0412, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1573, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0362, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1612, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0337, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1570, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0397, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1662, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0404, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1547, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0348, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1549, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0340, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1582, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0352, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1593, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0361, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1589, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0325, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1570, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0375, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1615, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0304, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1520, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0335, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0309, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1509, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0327, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1610, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0335, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1530, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0343, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1576, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0329, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1593, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0331, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1517, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0309, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0360, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1532, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0332, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0366, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1556, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0325, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1385, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0356, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "520 epochs, Train Rec:\t8.192e-02, KLD:\t3.961e-02\n",
      "seq_loss tensor(0.1406)\n",
      "v_loss tensor(0.0055)\n",
      "j_loss tensor(0.0558)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767) \n",
      "\n",
      "seq_loss tensor(0.1450)\n",
      "v_loss tensor(0.0057)\n",
      "j_loss tensor(0.0634)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767) \n",
      "\n",
      "seq_loss tensor(0.1454)\n",
      "v_loss tensor(0.0053)\n",
      "j_loss tensor(0.0598)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770) \n",
      "\n",
      "seq_loss tensor(0.1904)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0618)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773) \n",
      "\n",
      "seq_loss tensor(0.1365)\n",
      "v_loss tensor(0.0063)\n",
      "j_loss tensor(0.0702)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763) \n",
      "\n",
      "\n",
      "520 epochs, Valid Rec:\t6.619e-02, KLD:\t2.315e-02\n",
      "seq_loss tensor(0.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1446, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1396, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1384, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1384, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0172, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1592, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1410, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1345, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1349, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1453, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0200, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1304, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0145, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1566, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1437, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0169, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1429, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "530 epochs, Train Rec:\t7.514e-02, KLD:\t3.895e-02\n",
      "seq_loss tensor(0.1354)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0448)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756) \n",
      "\n",
      "seq_loss tensor(0.1403)\n",
      "v_loss tensor(0.0033)\n",
      "j_loss tensor(0.0528)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755) \n",
      "\n",
      "seq_loss tensor(0.1400)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0508)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758) \n",
      "\n",
      "seq_loss tensor(0.1824)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0511)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762) \n",
      "\n",
      "seq_loss tensor(0.1339)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0624)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752) \n",
      "\n",
      "\n",
      "530 epochs, Valid Rec:\t6.108e-02, KLD:\t2.280e-02\n",
      "seq_loss tensor(0.1409, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1333, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1373, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1400, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1314, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1471, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1341, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1327, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1375, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1224, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1206, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1418, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1298, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1343, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1767, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "540 epochs, Train Rec:\t2.053e-01, KLD:\t4.194e-02\n",
      "seq_loss tensor(0.2347)\n",
      "v_loss tensor(0.0174)\n",
      "j_loss tensor(0.1110)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794) \n",
      "\n",
      "seq_loss tensor(0.2300)\n",
      "v_loss tensor(0.0183)\n",
      "j_loss tensor(0.1184)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797) \n",
      "\n",
      "seq_loss tensor(0.2371)\n",
      "v_loss tensor(0.0167)\n",
      "j_loss tensor(0.1155)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800) \n",
      "\n",
      "seq_loss tensor(0.2740)\n",
      "v_loss tensor(0.0145)\n",
      "j_loss tensor(0.1073)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804) \n",
      "\n",
      "seq_loss tensor(0.2254)\n",
      "v_loss tensor(0.0135)\n",
      "j_loss tensor(0.1190)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792) \n",
      "\n",
      "\n",
      "540 epochs, Valid Rec:\t1.117e-01, KLD:\t2.403e-02\n",
      "seq_loss tensor(0.2197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0595, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2475, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0548, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2285, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0512, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0551, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2309, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0493, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0517, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2426, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0558, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0817, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0529, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0497, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2085, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0445, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2195, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0507, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0469, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0437, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0465, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0483, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0449, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0455, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0407, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0414, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2067, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0426, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0434, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1989, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0392, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0364, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2140, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0392, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0383, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2046, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0390, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0325, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0356, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0373, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0324, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0350, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0799, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "550 epochs, Train Rec:\t8.056e-02, KLD:\t3.966e-02\n",
      "seq_loss tensor(0.1434)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0521)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774) \n",
      "\n",
      "seq_loss tensor(0.1485)\n",
      "v_loss tensor(0.0050)\n",
      "j_loss tensor(0.0582)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775) \n",
      "\n",
      "seq_loss tensor(0.1478)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0575)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777) \n",
      "\n",
      "seq_loss tensor(0.1882)\n",
      "v_loss tensor(0.0050)\n",
      "j_loss tensor(0.0577)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781) \n",
      "\n",
      "seq_loss tensor(0.1404)\n",
      "v_loss tensor(0.0074)\n",
      "j_loss tensor(0.0708)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770) \n",
      "\n",
      "\n",
      "550 epochs, Valid Rec:\t6.574e-02, KLD:\t2.337e-02\n",
      "seq_loss tensor(0.1346, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1522, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0146, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1361, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1430, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1399, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1428, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0147, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1478, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1555, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0166, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1500, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1381, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1496, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1362, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1386, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1482, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1452, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1402, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1494, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1300, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "560 epochs, Train Rec:\t7.421e-02, KLD:\t3.903e-02\n",
      "seq_loss tensor(0.1338)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0445)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760) \n",
      "\n",
      "seq_loss tensor(0.1395)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0514)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760) \n",
      "\n",
      "seq_loss tensor(0.1384)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0514)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762) \n",
      "\n",
      "seq_loss tensor(0.1829)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0517)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766) \n",
      "\n",
      "seq_loss tensor(0.1314)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0610)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755) \n",
      "\n",
      "\n",
      "560 epochs, Valid Rec:\t6.064e-02, KLD:\t2.292e-02\n",
      "seq_loss tensor(0.1311, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1350, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1349, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1316, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1321, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1392, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1248, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1367, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1423, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1304, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1338, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "570 epochs, Train Rec:\t7.658e-02, KLD:\t3.886e-02\n",
      "seq_loss tensor(0.1367)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0481)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752) \n",
      "\n",
      "seq_loss tensor(0.1405)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0542)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751) \n",
      "\n",
      "seq_loss tensor(0.1410)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0525)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754) \n",
      "\n",
      "seq_loss tensor(0.1844)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0534)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758) \n",
      "\n",
      "seq_loss tensor(0.1347)\n",
      "v_loss tensor(0.0058)\n",
      "j_loss tensor(0.0657)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747) \n",
      "\n",
      "\n",
      "570 epochs, Valid Rec:\t6.216e-02, KLD:\t2.268e-02\n",
      "seq_loss tensor(0.1340, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1371, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1551, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1402, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1388, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1236, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1369, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1380, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1365, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1245, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1346, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1427, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1346, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1413, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1411, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "580 epochs, Train Rec:\t7.069e-02, KLD:\t3.838e-02\n",
      "seq_loss tensor(0.1278)\n",
      "v_loss tensor(0.0025)\n",
      "j_loss tensor(0.0448)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746) \n",
      "\n",
      "seq_loss tensor(0.1337)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0511)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746) \n",
      "\n",
      "seq_loss tensor(0.1322)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0504)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749) \n",
      "\n",
      "seq_loss tensor(0.1766)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0513)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752) \n",
      "\n",
      "seq_loss tensor(0.1274)\n",
      "v_loss tensor(0.0048)\n",
      "j_loss tensor(0.0636)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740) \n",
      "\n",
      "\n",
      "580 epochs, Valid Rec:\t5.882e-02, KLD:\t2.250e-02\n",
      "seq_loss tensor(0.1334, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1370, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1313, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1310, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1259, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1327, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1311, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1310, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1332, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1500, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "590 epochs, Train Rec:\t8.018e-02, KLD:\t3.924e-02\n",
      "seq_loss tensor(0.1385)\n",
      "v_loss tensor(0.0057)\n",
      "j_loss tensor(0.0603)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763) \n",
      "\n",
      "seq_loss tensor(0.1442)\n",
      "v_loss tensor(0.0059)\n",
      "j_loss tensor(0.0686)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763) \n",
      "\n",
      "seq_loss tensor(0.1439)\n",
      "v_loss tensor(0.0056)\n",
      "j_loss tensor(0.0674)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766) \n",
      "\n",
      "seq_loss tensor(0.1881)\n",
      "v_loss tensor(0.0060)\n",
      "j_loss tensor(0.0666)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769) \n",
      "\n",
      "seq_loss tensor(0.1341)\n",
      "v_loss tensor(0.0068)\n",
      "j_loss tensor(0.0740)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758) \n",
      "\n",
      "\n",
      "590 epochs, Valid Rec:\t6.726e-02, KLD:\t2.302e-02\n",
      "seq_loss tensor(0.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1434, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1403, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1355, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0146, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1444, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1356, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1477, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1314, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1283, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1343, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1305, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1490, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1352, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0135, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1405, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1275, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1329, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1331, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0141, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1391, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1323, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1330, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1443, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1347, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "600 epochs, Train Rec:\t6.981e-02, KLD:\t3.869e-02\n",
      "seq_loss tensor(0.1263)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0458)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749) \n",
      "\n",
      "seq_loss tensor(0.1317)\n",
      "v_loss tensor(0.0033)\n",
      "j_loss tensor(0.0545)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749) \n",
      "\n",
      "seq_loss tensor(0.1315)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0519)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752) \n",
      "\n",
      "seq_loss tensor(0.1753)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0524)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755) \n",
      "\n",
      "seq_loss tensor(0.1232)\n",
      "v_loss tensor(0.0049)\n",
      "j_loss tensor(0.0623)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744) \n",
      "\n",
      "\n",
      "600 epochs, Valid Rec:\t5.869e-02, KLD:\t2.260e-02\n",
      "seq_loss tensor(0.1226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1327, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1363, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1304, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1279, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1301, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1282, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1243, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1272, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1289, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1353, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1223, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1196, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1255, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1270, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1221, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "610 epochs, Train Rec:\t1.499e-01, KLD:\t4.671e-02\n",
      "seq_loss tensor(0.1768)\n",
      "v_loss tensor(0.0123)\n",
      "j_loss tensor(0.0916)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811) \n",
      "\n",
      "seq_loss tensor(0.1809)\n",
      "v_loss tensor(0.0124)\n",
      "j_loss tensor(0.0999)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812) \n",
      "\n",
      "seq_loss tensor(0.1834)\n",
      "v_loss tensor(0.0118)\n",
      "j_loss tensor(0.0993)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0813) \n",
      "\n",
      "seq_loss tensor(0.2207)\n",
      "v_loss tensor(0.0129)\n",
      "j_loss tensor(0.0934)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0819) \n",
      "\n",
      "seq_loss tensor(0.1724)\n",
      "v_loss tensor(0.0111)\n",
      "j_loss tensor(0.1051)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806) \n",
      "\n",
      "\n",
      "610 epochs, Valid Rec:\t8.946e-02, KLD:\t2.448e-02\n",
      "seq_loss tensor(0.1857, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0418, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0644, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0821, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0560, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1633, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0331, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1626, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0326, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0808, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1646, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0408, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1725, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0280, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1603, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0287, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1648, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0391, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1639, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0276, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1670, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0316, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1658, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0380, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1632, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0795, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1713, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0314, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1718, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0260, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1611, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0264, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1617, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0230, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1598, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0241, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1691, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0286, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0790, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1596, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1621, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1582, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0218, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1535, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0209, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1585, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0224, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1605, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0227, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1594, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0197, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1518, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0203, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0188, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1541, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0215, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1441, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1678, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "620 epochs, Train Rec:\t6.753e-02, KLD:\t3.813e-02\n",
      "seq_loss tensor(0.1221)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0423)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738) \n",
      "\n",
      "seq_loss tensor(0.1277)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0488)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738) \n",
      "\n",
      "seq_loss tensor(0.1268)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0483)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740) \n",
      "\n",
      "seq_loss tensor(0.1673)\n",
      "v_loss tensor(0.0041)\n",
      "j_loss tensor(0.0487)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745) \n",
      "\n",
      "seq_loss tensor(0.1209)\n",
      "v_loss tensor(0.0059)\n",
      "j_loss tensor(0.0595)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732) \n",
      "\n",
      "\n",
      "620 epochs, Valid Rec:\t5.615e-02, KLD:\t2.225e-02\n",
      "seq_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1200, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1216, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1266, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1222, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1280, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1308, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1291, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1277, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1293, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1225, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "630 epochs, Train Rec:\t6.425e-02, KLD:\t3.780e-02\n",
      "seq_loss tensor(0.1187)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0404)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734) \n",
      "\n",
      "seq_loss tensor(0.1246)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0462)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734) \n",
      "\n",
      "seq_loss tensor(0.1233)\n",
      "v_loss tensor(0.0025)\n",
      "j_loss tensor(0.0457)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736) \n",
      "\n",
      "seq_loss tensor(0.1652)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0463)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741) \n",
      "\n",
      "seq_loss tensor(0.1170)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0571)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727) \n",
      "\n",
      "\n",
      "630 epochs, Valid Rec:\t5.427e-02, KLD:\t2.214e-02\n",
      "seq_loss tensor(0.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1183, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1217, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1196, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1216, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1195, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1156, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "640 epochs, Train Rec:\t9.858e-02, KLD:\t3.949e-02\n",
      "seq_loss tensor(0.1460)\n",
      "v_loss tensor(0.0089)\n",
      "j_loss tensor(0.0748)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753) \n",
      "\n",
      "seq_loss tensor(0.1459)\n",
      "v_loss tensor(0.0087)\n",
      "j_loss tensor(0.0845)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755) \n",
      "\n",
      "seq_loss tensor(0.1498)\n",
      "v_loss tensor(0.0081)\n",
      "j_loss tensor(0.0833)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755) \n",
      "\n",
      "seq_loss tensor(0.1918)\n",
      "v_loss tensor(0.0085)\n",
      "j_loss tensor(0.0812)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760) \n",
      "\n",
      "seq_loss tensor(0.1409)\n",
      "v_loss tensor(0.0101)\n",
      "j_loss tensor(0.0955)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745) \n",
      "\n",
      "\n",
      "640 epochs, Valid Rec:\t7.462e-02, KLD:\t2.272e-02\n",
      "seq_loss tensor(0.1451, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0303, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1516, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0278, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1383, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1443, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0242, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0292, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1467, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0294, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1382, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0300, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1419, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0292, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0270, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0269, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1397, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0240, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1458, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0270, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0256, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1445, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1372, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0251, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1407, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0282, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1393, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0267, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1440, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0245, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1461, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1510, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0240, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1358, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0262, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1376, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1369, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1484, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0214, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1377, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0241, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0258, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1471, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0249, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1106, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "650 epochs, Train Rec:\t1.434e-01, KLD:\t4.215e-02\n",
      "seq_loss tensor(0.1603)\n",
      "v_loss tensor(0.0147)\n",
      "j_loss tensor(0.0851)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774) \n",
      "\n",
      "seq_loss tensor(0.1623)\n",
      "v_loss tensor(0.0147)\n",
      "j_loss tensor(0.0941)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775) \n",
      "\n",
      "seq_loss tensor(0.1648)\n",
      "v_loss tensor(0.0134)\n",
      "j_loss tensor(0.0910)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777) \n",
      "\n",
      "seq_loss tensor(0.2068)\n",
      "v_loss tensor(0.0124)\n",
      "j_loss tensor(0.0879)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781) \n",
      "\n",
      "seq_loss tensor(0.1561)\n",
      "v_loss tensor(0.0138)\n",
      "j_loss tensor(0.0971)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769) \n",
      "\n",
      "\n",
      "650 epochs, Valid Rec:\t8.286e-02, KLD:\t2.337e-02\n",
      "seq_loss tensor(0.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0313, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1505, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0306, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1577, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0327, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1676, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0345, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1553, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0307, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1582, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0305, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0279, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1578, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0266, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1564, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0334, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1559, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0284, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1479, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0291, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1420, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0302, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1436, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0241, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0240, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0289, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1456, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1336, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1492, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1357, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1435, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0221, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1537, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1414, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0223, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1480, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1481, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1354, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0266, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1488, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1385, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0230, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1459, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1425, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1454, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0203, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "660 epochs, Train Rec:\t6.729e-02, KLD:\t3.799e-02\n",
      "seq_loss tensor(0.1181)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0470)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735) \n",
      "\n",
      "seq_loss tensor(0.1246)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0548)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735) \n",
      "\n",
      "seq_loss tensor(0.1236)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0532)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737) \n",
      "\n",
      "seq_loss tensor(0.1694)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0530)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741) \n",
      "\n",
      "seq_loss tensor(0.1199)\n",
      "v_loss tensor(0.0071)\n",
      "j_loss tensor(0.0620)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727) \n",
      "\n",
      "\n",
      "660 epochs, Valid Rec:\t5.714e-02, KLD:\t2.215e-02\n",
      "seq_loss tensor(0.1162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1263, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1205, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1153, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1270, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1257, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0117, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1127, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1191, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1267, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1126, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1233, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1424, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "670 epochs, Train Rec:\t6.272e-02, KLD:\t3.753e-02\n",
      "seq_loss tensor(0.1137)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0431)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724) \n",
      "\n",
      "seq_loss tensor(0.1201)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0493)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724) \n",
      "\n",
      "seq_loss tensor(0.1182)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0483)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726) \n",
      "\n",
      "seq_loss tensor(0.1618)\n",
      "v_loss tensor(0.0041)\n",
      "j_loss tensor(0.0492)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729) \n",
      "\n",
      "seq_loss tensor(0.1135)\n",
      "v_loss tensor(0.0052)\n",
      "j_loss tensor(0.0563)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716) \n",
      "\n",
      "\n",
      "670 epochs, Valid Rec:\t5.379e-02, KLD:\t2.181e-02\n",
      "seq_loss tensor(0.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1112, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1183, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1181, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1151, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1067, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1124, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1159, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1134, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "680 epochs, Train Rec:\t5.958e-02, KLD:\t3.718e-02\n",
      "seq_loss tensor(0.1096)\n",
      "v_loss tensor(0.0022)\n",
      "j_loss tensor(0.0411)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718) \n",
      "\n",
      "seq_loss tensor(0.1157)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0482)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718) \n",
      "\n",
      "seq_loss tensor(0.1142)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0462)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721) \n",
      "\n",
      "seq_loss tensor(0.1588)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0467)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723) \n",
      "\n",
      "seq_loss tensor(0.1094)\n",
      "v_loss tensor(0.0061)\n",
      "j_loss tensor(0.0552)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710) \n",
      "\n",
      "\n",
      "680 epochs, Valid Rec:\t5.197e-02, KLD:\t2.164e-02\n",
      "seq_loss tensor(0.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1063, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1122, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1060, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1113, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1207, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1095, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1099, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1099, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1336, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "690 epochs, Train Rec:\t7.158e-02, KLD:\t3.864e-02\n",
      "seq_loss tensor(0.1234)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0483)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745) \n",
      "\n",
      "seq_loss tensor(0.1294)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0552)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746) \n",
      "\n",
      "seq_loss tensor(0.1287)\n",
      "v_loss tensor(0.0041)\n",
      "j_loss tensor(0.0539)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749) \n",
      "\n",
      "seq_loss tensor(0.1715)\n",
      "v_loss tensor(0.0050)\n",
      "j_loss tensor(0.0558)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754) \n",
      "\n",
      "seq_loss tensor(0.1208)\n",
      "v_loss tensor(0.0069)\n",
      "j_loss tensor(0.0658)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741) \n",
      "\n",
      "\n",
      "690 epochs, Valid Rec:\t5.892e-02, KLD:\t2.251e-02\n",
      "seq_loss tensor(0.1274, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1242, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1344, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1243, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1144, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1284, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1251, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1249, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1175, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1276, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1278, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1318, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1210, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1271, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1213, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1292, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "700 epochs, Train Rec:\t6.056e-02, KLD:\t3.776e-02\n",
      "seq_loss tensor(0.1100)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0395)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727) \n",
      "\n",
      "seq_loss tensor(0.1156)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0459)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728) \n",
      "\n",
      "seq_loss tensor(0.1145)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0457)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730) \n",
      "\n",
      "seq_loss tensor(0.1599)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0463)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735) \n",
      "\n",
      "seq_loss tensor(0.1083)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0569)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721) \n",
      "\n",
      "\n",
      "700 epochs, Valid Rec:\t5.178e-02, KLD:\t2.195e-02\n",
      "seq_loss tensor(0.1150, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1082, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1058, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1219, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1103, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1091, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1098, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1054, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1306, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "710 epochs, Train Rec:\t5.744e-02, KLD:\t3.732e-02\n",
      "seq_loss tensor(0.1054)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0405)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726) \n",
      "\n",
      "seq_loss tensor(0.1116)\n",
      "v_loss tensor(0.0022)\n",
      "j_loss tensor(0.0476)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726) \n",
      "\n",
      "seq_loss tensor(0.1104)\n",
      "v_loss tensor(0.0026)\n",
      "j_loss tensor(0.0467)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729) \n",
      "\n",
      "seq_loss tensor(0.1534)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0478)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733) \n",
      "\n",
      "seq_loss tensor(0.1051)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0601)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719) \n",
      "\n",
      "\n",
      "710 epochs, Valid Rec:\t5.080e-02, KLD:\t2.190e-02\n",
      "seq_loss tensor(0.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1056, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0004, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1052, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1050, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1091, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1061, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(8.8703e-05, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "720 epochs, Train Rec:\t7.418e-02, KLD:\t3.797e-02\n",
      "seq_loss tensor(0.1166)\n",
      "v_loss tensor(0.0056)\n",
      "j_loss tensor(0.0581)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733) \n",
      "\n",
      "seq_loss tensor(0.1225)\n",
      "v_loss tensor(0.0049)\n",
      "j_loss tensor(0.0669)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734) \n",
      "\n",
      "seq_loss tensor(0.1217)\n",
      "v_loss tensor(0.0054)\n",
      "j_loss tensor(0.0639)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736) \n",
      "\n",
      "seq_loss tensor(0.1643)\n",
      "v_loss tensor(0.0059)\n",
      "j_loss tensor(0.0624)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741) \n",
      "\n",
      "seq_loss tensor(0.1148)\n",
      "v_loss tensor(0.0067)\n",
      "j_loss tensor(0.0756)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727) \n",
      "\n",
      "\n",
      "720 epochs, Valid Rec:\t6.000e-02, KLD:\t2.212e-02\n",
      "seq_loss tensor(0.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1183, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0172, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1122, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1236, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1154, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1219, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1151, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1129, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1192, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0127, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1128, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0146, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0145, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1107, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0131, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1086, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1140, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0148, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0181, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1115, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1102, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1125, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1094, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "730 epochs, Train Rec:\t5.805e-02, KLD:\t3.728e-02\n",
      "seq_loss tensor(0.1043)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0424)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719) \n",
      "\n",
      "seq_loss tensor(0.1109)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0481)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720) \n",
      "\n",
      "seq_loss tensor(0.1095)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0469)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722) \n",
      "\n",
      "seq_loss tensor(0.1518)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0478)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727) \n",
      "\n",
      "seq_loss tensor(0.1032)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0575)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712) \n",
      "\n",
      "\n",
      "730 epochs, Valid Rec:\t5.065e-02, KLD:\t2.170e-02\n",
      "seq_loss tensor(0.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1117, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1116, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1049, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1093, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1110, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1026, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1027, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1033, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1142, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1086, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0939, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "740 epochs, Train Rec:\t2.098e-01, KLD:\t4.172e-02\n",
      "seq_loss tensor(0.3147)\n",
      "v_loss tensor(0.0699)\n",
      "j_loss tensor(0.2130)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825) \n",
      "\n",
      "seq_loss tensor(0.3167)\n",
      "v_loss tensor(0.0629)\n",
      "j_loss tensor(0.2210)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0826) \n",
      "\n",
      "seq_loss tensor(0.3216)\n",
      "v_loss tensor(0.0590)\n",
      "j_loss tensor(0.2212)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0829) \n",
      "\n",
      "seq_loss tensor(0.3566)\n",
      "v_loss tensor(0.0709)\n",
      "j_loss tensor(0.2062)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0835) \n",
      "\n",
      "seq_loss tensor(0.3047)\n",
      "v_loss tensor(0.0573)\n",
      "j_loss tensor(0.2049)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0829) \n",
      "\n",
      "\n",
      "740 epochs, Valid Rec:\t1.809e-01, KLD:\t2.497e-02\n",
      "seq_loss tensor(0.3072, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0650, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1430, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0823, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2728, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0546, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0814, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0229, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0822, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.3023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1255, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0825, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0984, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0812, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2759, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0909, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2462, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0335, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0868, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2780, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0287, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0764, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2715, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0793, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2447, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0202, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0817, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0810, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2525, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0220, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0802, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0818, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2495, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0201, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0733, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0811, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0720, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2324, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0167, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0700, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0801, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2260, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0198, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0796, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2264, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0651, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0645, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0804, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0581, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0184, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0582, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0794, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0152, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0615, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0544, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0561, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0570, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1949, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0522, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0153, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0500, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0771, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0461, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0433, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2012, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0486, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1890, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0458, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0484, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0761, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1949, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0472, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0461, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1881, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0420, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1811, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0445, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "750 epochs, Train Rec:\t6.628e-02, KLD:\t3.793e-02\n",
      "seq_loss tensor(0.1152)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0480)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734) \n",
      "\n",
      "seq_loss tensor(0.1213)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0553)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735) \n",
      "\n",
      "seq_loss tensor(0.1207)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0549)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737) \n",
      "\n",
      "seq_loss tensor(0.1610)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0547)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742) \n",
      "\n",
      "seq_loss tensor(0.1131)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0640)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730) \n",
      "\n",
      "\n",
      "750 epochs, Valid Rec:\t5.603e-02, KLD:\t2.217e-02\n",
      "seq_loss tensor(0.1247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1215, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1183, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1119, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1142, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1137, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1112, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1175, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1091, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1201, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1157, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1197, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1131, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1188, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1132, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1128, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1111, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1076, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "760 epochs, Train Rec:\t1.718e-01, KLD:\t4.226e-02\n",
      "seq_loss tensor(0.2096)\n",
      "v_loss tensor(0.0267)\n",
      "j_loss tensor(0.1157)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788) \n",
      "\n",
      "seq_loss tensor(0.2098)\n",
      "v_loss tensor(0.0244)\n",
      "j_loss tensor(0.1240)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791) \n",
      "\n",
      "seq_loss tensor(0.2163)\n",
      "v_loss tensor(0.0220)\n",
      "j_loss tensor(0.1180)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793) \n",
      "\n",
      "seq_loss tensor(0.2476)\n",
      "v_loss tensor(0.0238)\n",
      "j_loss tensor(0.1160)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800) \n",
      "\n",
      "seq_loss tensor(0.2071)\n",
      "v_loss tensor(0.0131)\n",
      "j_loss tensor(0.1170)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789) \n",
      "\n",
      "\n",
      "760 epochs, Valid Rec:\t1.080e-01, KLD:\t2.388e-02\n",
      "seq_loss tensor(0.2047, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0216, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0641, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0629, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0196, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0648, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0806, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0162, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0502, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0788, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2050, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0387, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0800, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0434, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2051, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0528, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0578, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2016, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0501, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0805, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0417, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0807, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2052, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0413, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0798, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0432, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0483, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0797, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0405, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1944, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0404, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0789, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1962, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0108, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0420, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0793, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0453, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0803, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0464, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0378, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0113, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0361, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0802, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0375, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1839, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0329, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0776, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0336, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0356, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0781, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0338, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0785, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0339, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0386, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0310, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1854, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0328, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1828, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0375, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1796, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0343, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0321, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1666, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0317, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0309, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "770 epochs, Train Rec:\t6.426e-02, KLD:\t3.787e-02\n",
      "seq_loss tensor(0.1127)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0481)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736) \n",
      "\n",
      "seq_loss tensor(0.1197)\n",
      "v_loss tensor(0.0033)\n",
      "j_loss tensor(0.0557)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738) \n",
      "\n",
      "seq_loss tensor(0.1168)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0554)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741) \n",
      "\n",
      "seq_loss tensor(0.1565)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0558)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745) \n",
      "\n",
      "seq_loss tensor(0.1098)\n",
      "v_loss tensor(0.0050)\n",
      "j_loss tensor(0.0635)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733) \n",
      "\n",
      "\n",
      "770 epochs, Valid Rec:\t5.507e-02, KLD:\t2.226e-02\n",
      "seq_loss tensor(0.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1135, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1168, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1158, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1224, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1060, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1139, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1140, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1103, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1186, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1193, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1120, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1127, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1123, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1141, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1146, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1097, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1161, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0911, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "780 epochs, Train Rec:\t5.797e-02, KLD:\t3.741e-02\n",
      "seq_loss tensor(0.1059)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0463)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725) \n",
      "\n",
      "seq_loss tensor(0.1123)\n",
      "v_loss tensor(0.0026)\n",
      "j_loss tensor(0.0527)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726) \n",
      "\n",
      "seq_loss tensor(0.1093)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0523)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729) \n",
      "\n",
      "seq_loss tensor(0.1412)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0529)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733) \n",
      "\n",
      "seq_loss tensor(0.1039)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0636)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720) \n",
      "\n",
      "\n",
      "780 epochs, Valid Rec:\t5.152e-02, KLD:\t2.190e-02\n",
      "seq_loss tensor(0.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0983, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1015, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1024, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1026, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1074, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1076, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1170, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1100, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1130, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1014, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1059, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "790 epochs, Train Rec:\t5.493e-02, KLD:\t3.708e-02\n",
      "seq_loss tensor(0.1007)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0399)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716) \n",
      "\n",
      "seq_loss tensor(0.1070)\n",
      "v_loss tensor(0.0021)\n",
      "j_loss tensor(0.0468)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717) \n",
      "\n",
      "seq_loss tensor(0.1042)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0460)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720) \n",
      "\n",
      "seq_loss tensor(0.1423)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0471)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724) \n",
      "\n",
      "seq_loss tensor(0.0991)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0583)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710) \n",
      "\n",
      "\n",
      "790 epochs, Valid Rec:\t4.848e-02, KLD:\t2.162e-02\n",
      "seq_loss tensor(0.0986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1036, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1006, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1088, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0989, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1026, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0990, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0995, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0974, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1028, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0952, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1262, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0747, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "800 epochs, Train Rec:\t7.116e-02, KLD:\t3.757e-02\n",
      "seq_loss tensor(0.1192)\n",
      "v_loss tensor(0.0055)\n",
      "j_loss tensor(0.0545)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729) \n",
      "\n",
      "seq_loss tensor(0.1261)\n",
      "v_loss tensor(0.0048)\n",
      "j_loss tensor(0.0606)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730) \n",
      "\n",
      "seq_loss tensor(0.1247)\n",
      "v_loss tensor(0.0055)\n",
      "j_loss tensor(0.0598)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733) \n",
      "\n",
      "seq_loss tensor(0.1607)\n",
      "v_loss tensor(0.0056)\n",
      "j_loss tensor(0.0583)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737) \n",
      "\n",
      "seq_loss tensor(0.1168)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0684)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723) \n",
      "\n",
      "\n",
      "800 epochs, Valid Rec:\t5.881e-02, KLD:\t2.201e-02\n",
      "seq_loss tensor(0.1138, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1185, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1164, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1211, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1195, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0740, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0158, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0743, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0140, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1234, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0142, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1106, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1160, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0136, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1182, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1177, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1209, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1155, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1217, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0139, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1231, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0128, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1219, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1090, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0122, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1167, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1149, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1169, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1066, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0164, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "810 epochs, Train Rec:\t6.134e-02, KLD:\t3.699e-02\n",
      "seq_loss tensor(0.1062)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0483)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716) \n",
      "\n",
      "seq_loss tensor(0.1130)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0547)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717) \n",
      "\n",
      "seq_loss tensor(0.1114)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0537)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720) \n",
      "\n",
      "seq_loss tensor(0.1491)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0532)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724) \n",
      "\n",
      "seq_loss tensor(0.1057)\n",
      "v_loss tensor(0.0048)\n",
      "j_loss tensor(0.0605)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709) \n",
      "\n",
      "\n",
      "810 epochs, Valid Rec:\t5.278e-02, KLD:\t2.162e-02\n",
      "seq_loss tensor(0.1038, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1057, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1052, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1046, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1077, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1105, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1073, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1096, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0990, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1018, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1087, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1034, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1108, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1179, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1099, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1114, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1063, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0119, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0744, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "820 epochs, Train Rec:\t5.482e-02, KLD:\t3.665e-02\n",
      "seq_loss tensor(0.1000)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0490)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714) \n",
      "\n",
      "seq_loss tensor(0.1060)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0561)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715) \n",
      "\n",
      "seq_loss tensor(0.1039)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0563)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717) \n",
      "\n",
      "seq_loss tensor(0.1421)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0550)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721) \n",
      "\n",
      "seq_loss tensor(0.0986)\n",
      "v_loss tensor(0.0051)\n",
      "j_loss tensor(0.0636)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706) \n",
      "\n",
      "\n",
      "820 epochs, Valid Rec:\t5.126e-02, KLD:\t2.154e-02\n",
      "seq_loss tensor(0.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0952, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1018, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1075, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1071, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1078, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1020, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0985, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0990, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0990, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1039, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1069, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0744, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "830 epochs, Train Rec:\t5.222e-02, KLD:\t3.645e-02\n",
      "seq_loss tensor(0.0953)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0418)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709) \n",
      "\n",
      "seq_loss tensor(0.1013)\n",
      "v_loss tensor(0.0028)\n",
      "j_loss tensor(0.0471)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710) \n",
      "\n",
      "seq_loss tensor(0.0993)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0475)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713) \n",
      "\n",
      "seq_loss tensor(0.1354)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0483)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716) \n",
      "\n",
      "seq_loss tensor(0.0936)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0573)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701) \n",
      "\n",
      "\n",
      "830 epochs, Valid Rec:\t4.708e-02, KLD:\t2.140e-02\n",
      "seq_loss tensor(0.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0935, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0932, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0962, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1062, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0955, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0864, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0983, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0960, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0004, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "840 epochs, Train Rec:\t8.367e-02, KLD:\t3.880e-02\n",
      "seq_loss tensor(0.1256)\n",
      "v_loss tensor(0.0089)\n",
      "j_loss tensor(0.0736)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751) \n",
      "\n",
      "seq_loss tensor(0.1297)\n",
      "v_loss tensor(0.0078)\n",
      "j_loss tensor(0.0813)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753) \n",
      "\n",
      "seq_loss tensor(0.1289)\n",
      "v_loss tensor(0.0084)\n",
      "j_loss tensor(0.0797)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754) \n",
      "\n",
      "seq_loss tensor(0.1691)\n",
      "v_loss tensor(0.0104)\n",
      "j_loss tensor(0.0783)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759) \n",
      "\n",
      "seq_loss tensor(0.1236)\n",
      "v_loss tensor(0.0096)\n",
      "j_loss tensor(0.0842)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742) \n",
      "\n",
      "\n",
      "840 epochs, Valid Rec:\t6.745e-02, KLD:\t2.266e-02\n",
      "seq_loss tensor(0.1272, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0298, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0760, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1270, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0219, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0757, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1265, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0283, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1261, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0251, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1194, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1204, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1152, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0246, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1235, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0254, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1175, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0240, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0210, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1214, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0244, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1174, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0198, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0765, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1256, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0242, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0758, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1220, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1218, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0215, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0229, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0745, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1168, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0225, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1184, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0191, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1198, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1253, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0222, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0748, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1250, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0205, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1246, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0206, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1229, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0195, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0752, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0178, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0746, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1239, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0754, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0196, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0193, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0750, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1339, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0192, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0775, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "850 epochs, Train Rec:\t5.757e-02, KLD:\t3.753e-02\n",
      "seq_loss tensor(0.1014)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0501)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724) \n",
      "\n",
      "seq_loss tensor(0.1069)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0570)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725) \n",
      "\n",
      "seq_loss tensor(0.1049)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0545)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727) \n",
      "\n",
      "seq_loss tensor(0.1453)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0558)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732) \n",
      "\n",
      "seq_loss tensor(0.0983)\n",
      "v_loss tensor(0.0042)\n",
      "j_loss tensor(0.0645)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716) \n",
      "\n",
      "\n",
      "850 epochs, Valid Rec:\t5.186e-02, KLD:\t2.184e-02\n",
      "seq_loss tensor(0.1083, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1121, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1033, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0022, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0961, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1079, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0972, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1045, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0997, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0993, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1002, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0749, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "860 epochs, Train Rec:\t5.677e-02, KLD:\t3.696e-02\n",
      "seq_loss tensor(0.1003)\n",
      "v_loss tensor(0.0039)\n",
      "j_loss tensor(0.0465)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717) \n",
      "\n",
      "seq_loss tensor(0.1061)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0533)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717) \n",
      "\n",
      "seq_loss tensor(0.1030)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0517)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720) \n",
      "\n",
      "seq_loss tensor(0.1447)\n",
      "v_loss tensor(0.0049)\n",
      "j_loss tensor(0.0536)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724) \n",
      "\n",
      "seq_loss tensor(0.0974)\n",
      "v_loss tensor(0.0043)\n",
      "j_loss tensor(0.0619)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708) \n",
      "\n",
      "\n",
      "860 epochs, Valid Rec:\t5.055e-02, KLD:\t2.162e-02\n",
      "seq_loss tensor(0.1044, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0987, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0978, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1031, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0988, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1035, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1040, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1041, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0890, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1070, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0739, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1068, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1037, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1025, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0978, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1092, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1044, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0890, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1021, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0933, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1080, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "870 epochs, Train Rec:\t5.307e-02, KLD:\t3.658e-02\n",
      "seq_loss tensor(0.0950)\n",
      "v_loss tensor(0.0033)\n",
      "j_loss tensor(0.0402)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710) \n",
      "\n",
      "seq_loss tensor(0.1014)\n",
      "v_loss tensor(0.0025)\n",
      "j_loss tensor(0.0467)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711) \n",
      "\n",
      "seq_loss tensor(0.0982)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0462)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713) \n",
      "\n",
      "seq_loss tensor(0.1385)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0482)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717) \n",
      "\n",
      "seq_loss tensor(0.0925)\n",
      "v_loss tensor(0.0035)\n",
      "j_loss tensor(0.0557)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702) \n",
      "\n",
      "\n",
      "870 epochs, Valid Rec:\t4.695e-02, KLD:\t2.143e-02\n",
      "seq_loss tensor(0.0974, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0881, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0919, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0968, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0996, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0998, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1081, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0894, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0986, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0964, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0882, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0876, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0977, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1142, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "880 epochs, Train Rec:\t5.111e-02, KLD:\t3.631e-02\n",
      "seq_loss tensor(0.0930)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0432)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705) \n",
      "\n",
      "seq_loss tensor(0.0986)\n",
      "v_loss tensor(0.0024)\n",
      "j_loss tensor(0.0493)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705) \n",
      "\n",
      "seq_loss tensor(0.0964)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0486)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708) \n",
      "\n",
      "seq_loss tensor(0.1369)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0493)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711) \n",
      "\n",
      "seq_loss tensor(0.0908)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0599)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696) \n",
      "\n",
      "\n",
      "880 epochs, Valid Rec:\t4.707e-02, KLD:\t2.125e-02\n",
      "seq_loss tensor(0.0903, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0829, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0976, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0921, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0036, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0874, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0932, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0864, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0903, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0937, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0868, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0987, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0835, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0886, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0888, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0963, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0958, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0909, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0847, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0783, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "890 epochs, Train Rec:\t9.482e-02, KLD:\t4.124e-02\n",
      "seq_loss tensor(0.1301)\n",
      "v_loss tensor(0.0088)\n",
      "j_loss tensor(0.0713)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778) \n",
      "\n",
      "seq_loss tensor(0.1369)\n",
      "v_loss tensor(0.0069)\n",
      "j_loss tensor(0.0787)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0780) \n",
      "\n",
      "seq_loss tensor(0.1347)\n",
      "v_loss tensor(0.0086)\n",
      "j_loss tensor(0.0754)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782) \n",
      "\n",
      "seq_loss tensor(0.1669)\n",
      "v_loss tensor(0.0080)\n",
      "j_loss tensor(0.0745)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0786) \n",
      "\n",
      "seq_loss tensor(0.1260)\n",
      "v_loss tensor(0.0072)\n",
      "j_loss tensor(0.0851)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768) \n",
      "\n",
      "\n",
      "890 epochs, Valid Rec:\t6.747e-02, KLD:\t2.347e-02\n",
      "seq_loss tensor(0.1307, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0779, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0200, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0228, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0784, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0274, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0767, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1226, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1374, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0232, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0787, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0777, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1312, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0203, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1368, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0240, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0791, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1247, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0185, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0770, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1328, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0217, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1240, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0033, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0774, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1228, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0194, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0772, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1109, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0148, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0763, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1273, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0166, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0782, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0171, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0778, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1199, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0029, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0175, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0769, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1133, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0161, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0753, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1297, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0150, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0783, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1217, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0176, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1287, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0151, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1212, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0766, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1281, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0773, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1190, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0154, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1147, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1171, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0144, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1104, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0125, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0756, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1172, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0141, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1232, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0764, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1173, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0137, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1162, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0149, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0768, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1180, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0030, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0130, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0759, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1187, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0762, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0174, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0755, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "900 epochs, Train Rec:\t5.240e-02, KLD:\t3.687e-02\n",
      "seq_loss tensor(0.0956)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0427)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711) \n",
      "\n",
      "seq_loss tensor(0.1011)\n",
      "v_loss tensor(0.0021)\n",
      "j_loss tensor(0.0493)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712) \n",
      "\n",
      "seq_loss tensor(0.0984)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0487)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714) \n",
      "\n",
      "seq_loss tensor(0.1362)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0494)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718) \n",
      "\n",
      "seq_loss tensor(0.0937)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0580)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703) \n",
      "\n",
      "\n",
      "900 epochs, Valid Rec:\t4.755e-02, KLD:\t2.144e-02\n",
      "seq_loss tensor(0.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0984, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1011, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0994, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0903, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0918, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0999, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0828, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0969, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0992, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0991, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0722, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0981, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0978, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0726, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0953, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0952, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0001, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "910 epochs, Train Rec:\t4.991e-02, KLD:\t3.651e-02\n",
      "seq_loss tensor(0.0905)\n",
      "v_loss tensor(0.0024)\n",
      "j_loss tensor(0.0389)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707) \n",
      "\n",
      "seq_loss tensor(0.0962)\n",
      "v_loss tensor(0.0020)\n",
      "j_loss tensor(0.0443)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708) \n",
      "\n",
      "seq_loss tensor(0.0940)\n",
      "v_loss tensor(0.0024)\n",
      "j_loss tensor(0.0443)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711) \n",
      "\n",
      "seq_loss tensor(0.1293)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0457)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714) \n",
      "\n",
      "seq_loss tensor(0.0886)\n",
      "v_loss tensor(0.0046)\n",
      "j_loss tensor(0.0559)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699) \n",
      "\n",
      "\n",
      "910 epochs, Valid Rec:\t4.477e-02, KLD:\t2.132e-02\n",
      "seq_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0898, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0844, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0894, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0871, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0888, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0887, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0909, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0907, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0874, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0875, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0887, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0927, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0214, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.1107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.2678, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(2.7708, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(2.0486, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0858, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.8203, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1521, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.7791, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.1297, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9193, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1812, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.9166, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.2838, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.9618, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.2006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.7000, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.4425, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0258, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1423, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.3588, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.5861, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0252, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.3045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.6792, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0499, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2844, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.7385, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(1.0176, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.1113, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.2841, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.7876, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "920 epochs, Train Rec:\t5.677e-02, KLD:\t3.678e-02\n",
      "seq_loss tensor(0.0992)\n",
      "v_loss tensor(0.0065)\n",
      "j_loss tensor(0.0570)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711) \n",
      "\n",
      "seq_loss tensor(0.1044)\n",
      "v_loss tensor(0.0050)\n",
      "j_loss tensor(0.0619)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713) \n",
      "\n",
      "seq_loss tensor(0.1026)\n",
      "v_loss tensor(0.0055)\n",
      "j_loss tensor(0.0624)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715) \n",
      "\n",
      "seq_loss tensor(0.1384)\n",
      "v_loss tensor(0.0047)\n",
      "j_loss tensor(0.0616)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719) \n",
      "\n",
      "seq_loss tensor(0.0966)\n",
      "v_loss tensor(0.0068)\n",
      "j_loss tensor(0.0696)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705) \n",
      "\n",
      "\n",
      "920 epochs, Valid Rec:\t5.317e-02, KLD:\t2.149e-02\n",
      "seq_loss tensor(0.0951, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0118, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1009, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0026, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0099, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0123, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0982, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0143, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0035, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1043, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0126, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0724, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1029, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1032, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0038, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0121, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0721, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1018, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0177, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0199, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0751, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0949, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0032, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0734, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1023, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0742, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0034, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0165, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0965, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0106, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1002, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0134, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1027, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0120, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1022, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1000, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1042, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0102, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0963, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0893, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0730, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "930 epochs, Train Rec:\t1.371e-01, KLD:\t4.243e-02\n",
      "seq_loss tensor(0.1549)\n",
      "v_loss tensor(0.0128)\n",
      "j_loss tensor(0.0984)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0732) \n",
      "\n",
      "seq_loss tensor(0.1504)\n",
      "v_loss tensor(0.0112)\n",
      "j_loss tensor(0.1059)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736) \n",
      "\n",
      "seq_loss tensor(0.1561)\n",
      "v_loss tensor(0.0111)\n",
      "j_loss tensor(0.1060)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735) \n",
      "\n",
      "seq_loss tensor(0.1822)\n",
      "v_loss tensor(0.0115)\n",
      "j_loss tensor(0.0993)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0741) \n",
      "\n",
      "seq_loss tensor(0.1429)\n",
      "v_loss tensor(0.0097)\n",
      "j_loss tensor(0.1111)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725) \n",
      "\n",
      "\n",
      "930 epochs, Valid Rec:\t8.218e-02, KLD:\t2.212e-02\n",
      "seq_loss tensor(0.1514, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0101, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0393, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0738, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1387, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0388, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1542, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0447, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1421, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0394, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1534, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0384, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0736, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1359, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0372, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1417, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0380, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0727, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1431, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0362, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1404, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0341, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0735, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1415, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0349, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0737, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1411, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0330, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0733, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1422, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0328, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1379, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0342, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0728, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1439, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0345, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0729, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1322, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0340, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1360, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0334, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1406, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0312, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0718, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1438, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0277, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0731, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1335, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0268, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1317, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0295, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1238, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0298, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1304, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1390, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0723, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1245, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0273, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1325, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0261, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0719, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1337, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0247, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1290, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0037, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0295, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0725, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0230, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1296, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1204, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0213, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0715, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0252, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0713, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1208, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0031, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0271, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0716, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1165, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1048, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0338, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "940 epochs, Train Rec:\t5.119e-02, KLD:\t3.609e-02\n",
      "seq_loss tensor(0.0915)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0418)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698) \n",
      "\n",
      "seq_loss tensor(0.0966)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0464)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699) \n",
      "\n",
      "seq_loss tensor(0.0951)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0474)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701) \n",
      "\n",
      "seq_loss tensor(0.1291)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0474)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704) \n",
      "\n",
      "seq_loss tensor(0.0916)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0566)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691) \n",
      "\n",
      "\n",
      "940 epochs, Valid Rec:\t4.578e-02, KLD:\t2.106e-02\n",
      "seq_loss tensor(0.0945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0886, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0845, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0946, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0925, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0912, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0962, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0905, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0874, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0917, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0910, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0887, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0987, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0717, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0886, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0881, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0837, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0088, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0881, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0948, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0962, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0838, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0685, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "950 epochs, Train Rec:\t4.803e-02, KLD:\t3.588e-02\n",
      "seq_loss tensor(0.0875)\n",
      "v_loss tensor(0.0030)\n",
      "j_loss tensor(0.0454)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695) \n",
      "\n",
      "seq_loss tensor(0.0925)\n",
      "v_loss tensor(0.0023)\n",
      "j_loss tensor(0.0503)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696) \n",
      "\n",
      "seq_loss tensor(0.0905)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0500)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698) \n",
      "\n",
      "seq_loss tensor(0.1268)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0503)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701) \n",
      "\n",
      "seq_loss tensor(0.0858)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0607)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688) \n",
      "\n",
      "\n",
      "950 epochs, Valid Rec:\t4.553e-02, KLD:\t2.096e-02\n",
      "seq_loss tensor(0.0863, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0886, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0847, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0933, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0875, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0932, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0043, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0942, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0712, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0812, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0825, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0947, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0818, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0692, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0902, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0878, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0039, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0893, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0040, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0836, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0803, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0885, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0811, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0887, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0804, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0817, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0865, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0844, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0864, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0692, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0903, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "960 epochs, Train Rec:\t5.387e-02, KLD:\t3.605e-02\n",
      "seq_loss tensor(0.0927)\n",
      "v_loss tensor(0.0044)\n",
      "j_loss tensor(0.0481)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696) \n",
      "\n",
      "seq_loss tensor(0.0980)\n",
      "v_loss tensor(0.0031)\n",
      "j_loss tensor(0.0519)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698) \n",
      "\n",
      "seq_loss tensor(0.0961)\n",
      "v_loss tensor(0.0041)\n",
      "j_loss tensor(0.0533)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700) \n",
      "\n",
      "seq_loss tensor(0.1300)\n",
      "v_loss tensor(0.0041)\n",
      "j_loss tensor(0.0535)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703) \n",
      "\n",
      "seq_loss tensor(0.0909)\n",
      "v_loss tensor(0.0053)\n",
      "j_loss tensor(0.0611)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0690) \n",
      "\n",
      "\n",
      "960 epochs, Valid Rec:\t4.802e-02, KLD:\t2.102e-02\n",
      "seq_loss tensor(0.0943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0980, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0973, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0917, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0870, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0711, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0931, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0888, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0923, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0104, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0945, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0886, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0881, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1007, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0904, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0918, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0971, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0066, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0709, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0025, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0096, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0707, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0943, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0907, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0879, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0890, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0019, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0872, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0793, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0028, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "970 epochs, Train Rec:\t4.756e-02, KLD:\t3.562e-02\n",
      "seq_loss tensor(0.0862)\n",
      "v_loss tensor(0.0032)\n",
      "j_loss tensor(0.0411)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688) \n",
      "\n",
      "seq_loss tensor(0.0920)\n",
      "v_loss tensor(0.0022)\n",
      "j_loss tensor(0.0464)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689) \n",
      "\n",
      "seq_loss tensor(0.0898)\n",
      "v_loss tensor(0.0029)\n",
      "j_loss tensor(0.0471)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691) \n",
      "\n",
      "seq_loss tensor(0.1242)\n",
      "v_loss tensor(0.0036)\n",
      "j_loss tensor(0.0480)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694) \n",
      "\n",
      "seq_loss tensor(0.0877)\n",
      "v_loss tensor(0.0038)\n",
      "j_loss tensor(0.0569)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0681) \n",
      "\n",
      "\n",
      "970 epochs, Valid Rec:\t4.431e-02, KLD:\t2.075e-02\n",
      "seq_loss tensor(0.0819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0685, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0929, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0840, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0874, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0896, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0055, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0810, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0868, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0862, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0903, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0901, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0874, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0685, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0904, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0858, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0812, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0824, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0821, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0823, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0690, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0842, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0845, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0687, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0804, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0684, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0848, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0049, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0867, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0885, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0061, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0841, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0835, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0053, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0883, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "980 epochs, Train Rec:\t5.562e-02, KLD:\t3.606e-02\n",
      "seq_loss tensor(0.0963)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0485)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700) \n",
      "\n",
      "seq_loss tensor(0.1022)\n",
      "v_loss tensor(0.0025)\n",
      "j_loss tensor(0.0541)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702) \n",
      "\n",
      "seq_loss tensor(0.1006)\n",
      "v_loss tensor(0.0037)\n",
      "j_loss tensor(0.0549)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704) \n",
      "\n",
      "seq_loss tensor(0.1324)\n",
      "v_loss tensor(0.0040)\n",
      "j_loss tensor(0.0543)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706) \n",
      "\n",
      "seq_loss tensor(0.0948)\n",
      "v_loss tensor(0.0057)\n",
      "j_loss tensor(0.0598)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693) \n",
      "\n",
      "\n",
      "980 epochs, Valid Rec:\t4.928e-02, KLD:\t2.113e-02\n",
      "seq_loss tensor(0.0975, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0922, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0024, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1053, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0938, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0928, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1067, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0710, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0714, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0906, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0960, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1017, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0979, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0908, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0959, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0089, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1067, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0705, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0700, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0904, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0916, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0954, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0016, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0708, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0877, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0939, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0940, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0017, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0891, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0076, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1010, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0018, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0098, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0701, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.1019, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0021, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0962, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0020, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0097, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0837, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0005, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0706, grad_fn=<MulBackward0>) \n",
      "\n",
      "\n",
      "990 epochs, Train Rec:\t4.812e-02, KLD:\t3.561e-02\n",
      "seq_loss tensor(0.0875)\n",
      "v_loss tensor(0.0027)\n",
      "j_loss tensor(0.0420)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696) \n",
      "\n",
      "seq_loss tensor(0.0938)\n",
      "v_loss tensor(0.0019)\n",
      "j_loss tensor(0.0466)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697) \n",
      "\n",
      "seq_loss tensor(0.0919)\n",
      "v_loss tensor(0.0026)\n",
      "j_loss tensor(0.0475)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699) \n",
      "\n",
      "seq_loss tensor(0.1254)\n",
      "v_loss tensor(0.0034)\n",
      "j_loss tensor(0.0483)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0702) \n",
      "\n",
      "seq_loss tensor(0.0872)\n",
      "v_loss tensor(0.0045)\n",
      "j_loss tensor(0.0569)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689) \n",
      "\n",
      "\n",
      "990 epochs, Valid Rec:\t4.473e-02, KLD:\t2.099e-02\n",
      "seq_loss tensor(0.0860, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0861, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0897, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0889, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0861, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0840, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0058, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0869, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0690, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0703, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0859, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0023, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0695, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0853, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0050, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0841, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0062, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0843, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0060, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0848, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0873, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0056, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0924, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0012, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0064, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0696, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0915, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0691, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0833, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0014, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0685, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0815, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0051, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0694, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0892, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0699, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0846, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0041, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0697, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0880, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0704, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0856, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0044, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0926, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0960, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0013, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0052, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0698, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0820, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0914, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0059, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0693, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0855, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0010, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0054, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0690, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0827, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0048, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0689, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0819, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0009, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0046, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0687, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0816, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0057, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0687, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0826, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0042, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0688, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0850, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0047, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0692, grad_fn=<MulBackward0>) \n",
      "\n",
      "seq_loss tensor(0.0970, grad_fn=<MulBackward0>)\n",
      "v_loss tensor(0.0008, grad_fn=<MulBackward0>)\n",
      "j_loss tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "kld_weight 0.05660377358490566\n",
      "kld_loss tensor(0.0720, grad_fn=<MulBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import FullFVAE\n",
    "from src.metrics import VAELoss\n",
    "\n",
    "criterion = VAELoss(weight_seq=3, weight_kld=1.5, use_v=use_v, use_j=use_j, v_dim=v_dim, \n",
    "                    j_dim=j_dim,  weight_v=0.5, weight_j=0.3, debug=False)\n",
    "\n",
    "model_split_optim = FullFVAE(max_len=max_len, aa_dim= 20, use_v=use_v, use_j=use_j, v_dim=v_dim, j_dim=j_dim,\n",
    "                 hidden_dim=256, latent_dim=128, activation = nn.SELU())\n",
    "\n",
    "optimizer_recon = optim.Adam(model_split_optim.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "optimizer_kld = optim.Adam(model_split_optim.parameters(), lr=1e-7, weight_decay=1e-5)\n",
    "split_train_losses, split_valid_losses = [], []\n",
    "split_train_rec, split_valid_rec = [], []\n",
    "split_train_kld, split_valid_kld = [], []\n",
    "for n in range(1000):\n",
    "    # Train\n",
    "    acum_loss = 0\n",
    "    acum_rec = 0\n",
    "    acum_kld = 0\n",
    "    for x, _  in train_loader:\n",
    "        x_hat, mu, logvar = model_split_optim(x)\n",
    "        recon_loss, kld_loss = criterion(x_hat, x, mu, logvar)\n",
    "        loss = recon_loss + kld_loss\n",
    "        optimizer_recon.zero_grad()\n",
    "        optimizer_kld.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_recon.step()\n",
    "        optimizer_kld.step()\n",
    "        \n",
    "        acum_loss += loss.item() * y.shape[0]\n",
    "        acum_rec += recon_loss.item() * y.shape[0]\n",
    "        acum_kld += kld_loss.item() * y.shape[0]\n",
    "    acum_loss /= len(train_loader.dataset)\n",
    "    acum_rec /= len(train_loader.dataset)\n",
    "    acum_kld /= len(train_loader.dataset)\n",
    "    if n>=10:\n",
    "        split_train_rec.append(acum_rec)\n",
    "        split_train_kld.append(acum_kld)\n",
    "        split_train_losses.append(acum_loss)\n",
    "    if n%10==0:\n",
    "        criterion.set_debug(True)\n",
    "        print(f'\\n{n} epochs, Train Rec:\\t{acum_rec:.3e}, KLD:\\t{acum_kld:.3e}')\n",
    "    else:\n",
    "        criterion.set_debug(False)\n",
    "\n",
    "    # Valid\n",
    "    acum_loss = 0\n",
    "    acum_rec = 0\n",
    "    acum_kld = 0\n",
    "    for x, _  in valid_loader:\n",
    "        with torch.no_grad():\n",
    "            x_hat, mu, logvar = model_split_optim(x)\n",
    "            recon_loss, kld_loss = criterion(x_hat, x, mu, logvar)\n",
    "            loss = recon_loss + kld_loss        \n",
    "            acum_loss += loss.item() * y.shape[0]\n",
    "            acum_rec += recon_loss.item() * y.shape[0]\n",
    "            acum_kld += kld_loss.item() * y.shape[0]\n",
    "    acum_loss /= len(valid_loader.dataset)\n",
    "    acum_rec /= len(valid_loader.dataset)\n",
    "    acum_kld /= len(valid_loader.dataset)\n",
    "    if n>=10:\n",
    "        split_valid_rec.append(acum_rec)\n",
    "        split_valid_kld.append(acum_kld)\n",
    "        split_valid_losses.append(acum_loss)\n",
    "    if n%10==0:\n",
    "        print(f'\\n{n} epochs, Valid Rec:\\t{acum_rec:.3e}, KLD:\\t{acum_kld:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc760c36-3ba6-44ad-8c71-30ffc8c2ed9f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "id": "c7b09f3e-00f7-4971-83f2-c2e5ed45b48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d1f31ad0>"
      ]
     },
     "execution_count": 1327,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3600x1260 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_palette('gnuplot2', n_colors=6)\n",
    "f,a = plt.subplots(1,2, figsize=(20,7), sharey=True)\n",
    "a[0].set_title('Concatenated optimizer')\n",
    "plt.ylim([0,1.5])\n",
    "a[0].plot(range(len(concat_train_losses)), concat_train_losses, label='train_total')\n",
    "a[0].plot(range(len(concat_train_losses)), concat_valid_losses, label='valid_total')\n",
    "a[0].plot(range(len(concat_train_losses)), concat_train_kld, label='train_kld')\n",
    "a[0].plot(range(len(concat_train_losses)), concat_valid_kld, label='valid_kld')\n",
    "a[0].plot(range(len(concat_train_losses)), concat_train_rec, label='train_recon')\n",
    "a[0].plot(range(len(concat_train_losses)), concat_valid_rec, label='valid_recon')\n",
    "a[0].legend()\n",
    "\n",
    "a[1].set_title('Split optimizer')\n",
    "a[1].plot(range(len(split_train_losses)), split_train_losses, label='train_total')\n",
    "a[1].plot(range(len(split_train_losses)), split_valid_losses, label='valid_total')\n",
    "a[1].plot(range(len(split_train_losses)), split_train_kld, label='train_kld')\n",
    "a[1].plot(range(len(split_train_losses)), split_valid_kld, label='valid_kld')\n",
    "a[1].plot(range(len(split_train_losses)), split_train_rec, label='train_recon')\n",
    "a[1].plot(range(len(split_train_losses)), split_valid_rec, label='valid_recon')\n",
    "a[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "fc5896d9-87b6-4b80-855c-296ca20f38e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best concat_train_losses 0.7514 Epoch = 961\n",
      "Best split_train_losses 0.0819 Epoch = 984\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Best concat_valid_losses 0.4452 Epoch = 984\n",
      "Best split_valid_losses 0.0638 Epoch = 983\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Best concat_train_kld 0.0585 Epoch = 986\n",
      "Best split_train_kld 0.0354 Epoch = 989\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Best concat_valid_kld 0.0345 Epoch = 971\n",
      "Best split_valid_kld 0.0206 Epoch = 984\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Best concat_train_rec 0.6928 Epoch = 961\n",
      "Best split_train_rec 0.0464 Epoch = 984\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Best concat_valid_rec 0.4104 Epoch = 984\n",
      "Best split_valid_rec 0.043 Epoch = 983\n"
     ]
    }
   ],
   "source": [
    "print('Best concat_train_losses', round(min(concat_train_losses),4), f'Epoch = {concat_train_losses.index(min(concat_train_losses))}')\n",
    "print('Best split_train_losses', round(min(split_train_losses),4), f'Epoch = {split_train_losses.index(min(split_train_losses))}')\n",
    "print('\\n','-'*100)\n",
    "print('Best concat_valid_losses', round(min(concat_valid_losses),4), f'Epoch = {concat_valid_losses.index(min(concat_valid_losses))}')\n",
    "print('Best split_valid_losses', round(min(split_valid_losses),4), f'Epoch = {split_valid_losses.index(min(split_valid_losses))}')\n",
    "print('\\n','-'*100)\n",
    "print('Best concat_train_kld', round(min(concat_train_kld),4), f'Epoch = {concat_train_kld.index(min(concat_train_kld))}')\n",
    "print('Best split_train_kld', round(min(split_train_kld),4), f'Epoch = {split_train_kld.index(min(split_train_kld))}')\n",
    "print('\\n','-'*100)\n",
    "print('Best concat_valid_kld', round(min(concat_valid_kld),4), f'Epoch = {concat_valid_kld.index(min(concat_valid_kld))}')\n",
    "print('Best split_valid_kld', round(min(split_valid_kld),4), f'Epoch = {split_valid_kld.index(min(split_valid_kld))}')\n",
    "print('\\n','-'*100)\n",
    "print('Best concat_train_rec', round(min(concat_train_rec),4), f'Epoch = {concat_train_rec.index(min(concat_train_rec))}')\n",
    "print('Best split_train_rec', round(min(split_train_rec),4), f'Epoch = {split_train_rec.index(min(split_train_rec))}')\n",
    "print('\\n','-'*100)\n",
    "print('Best concat_valid_rec', round(min(concat_valid_rec),4), f'Epoch = {concat_valid_rec.index(min(concat_valid_rec))}')\n",
    "print('Best split_valid_rec', round(min(split_valid_rec),4), f'Epoch = {split_valid_rec.index(min(split_valid_rec))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54086fad-a2ee-4638-a59d-ab79a6c684ff",
   "metadata": {},
   "source": [
    "## Create fct to reconstruct / recover the closest BLOSUM encoding from the decoded sequences seq_hat"
   ]
  },
  {
   "cell_type": "raw",
   "id": "681e3960-8d37-4ad7-95ff-957e4ede5faa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# ChatGPT efficient code THIS HERE WORKS\n",
    "# 230925_1229: this has been copied to models\n",
    "def recover_indices(seq_tensor, MATRIX_VALUES):\n",
    "    # Sample data\n",
    "    N, max_len = seq_tensor.shape[0], seq_tensor.shape[1]\n",
    "    # print(N, max_len)\n",
    "    \n",
    "    \n",
    "    # Expand MATRIX_VALUES to have the same shape as x_seq for broadcasting\n",
    "    expanded_MATRIX_VALUES = MATRIX_VALUES.unsqueeze(0).expand(N, -1, -1, -1)\n",
    "    # print(expanded_MATRIX_VALUES.shape)\n",
    "    # Compute the absolute differences\n",
    "    abs_diff = torch.abs(seq_tensor.unsqueeze(2) - expanded_MATRIX_VALUES)\n",
    "    # print(abs_diff.shape)\n",
    "    # Sum along the last dimension (20) to get the absolute differences for each character\n",
    "    abs_diff_sum = abs_diff.sum(dim=-1)\n",
    "    \n",
    "    # Find the argmin along the character dimension (21)\n",
    "    argmin_indices = torch.argmin(abs_diff_sum, dim=-1)\n",
    "    return argmin_indices\n",
    "\n",
    "def recover_sequences_blosum(seq_tensor, MATRIX_VALUES, NEW_KEYS):\n",
    "    return [''.join([NEW_KEYS[y] for y in x]) for x in recover_indices(seq_tensor, MATRIX_VALUES)]\n",
    "\n",
    "pad_scale = -15\n",
    "new_dict = deepcopy(encoding_matrix_dict['BL50LO'])\n",
    "new_dict['X'] = np.array([pad_scale]).repeat(20)\n",
    "NEW_KEYS = ''.join(new_dict.keys())\n",
    "MATRIX_VALUES = torch.from_numpy(np.stack(list(new_dict.values())))\n",
    "MATRIX_VALUES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1434,
   "id": "2d95f65d-8006-4fd3-b3e7-9d0204bca283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_true\t tensor([31, 46, 46, 20, 45, 47, 20, 20, 22, 31])\n",
      "v_split\t tensor([31, 39,  0, 20, 45, 47, 20, 20, 22, 31])\n",
      "v_cat\t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print('v_true\\t', v_true[:20].argmax(dim=1))\n",
    "print('v_split\\t', v_split[:20].argmax(dim=1))\n",
    "print('v_cat\\t', v_concat[:20].argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1435,
   "id": "9b61af3a-e992-4804-8951-30b99b641d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j_true\t tensor([12, 12,  6,  1, 10,  7,  9,  3,  6, 12])\n",
      "j_split\t tensor([12, 12,  6,  1, 10,  7,  9,  3,  6, 10])\n",
      "j_cat\t tensor([12,  6, 12, 12,  6,  7, 12,  0,  6, 12])\n"
     ]
    }
   ],
   "source": [
    "print('j_true\\t', j_true[:20].argmax(dim=1))\n",
    "print('j_split\\t', j_split[:20].argmax(dim=1))\n",
    "print('j_cat\\t', j_concat[:20].argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "afe51cb9-b144-4733-ae51-7ad44099ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\t ['ASSLAGGRYEQYXXXXXXXXXXX', 'ASTPPKRPRGEQYXXXXXXXXXX', 'ASSSITVDFYNEQFXXXXXXXXX']\n",
      "split\t ['ASSLAGGRYEQYXXXXXXXXXTT', 'ASSPPKRPRGEQYXXXXXXXXTT', 'ASSSITVDFYNEQFXXXXXXXTT']\n",
      "concat\t ['ASSLASSTHEQYXXXXXXXXXTT', 'ASSKAAAATNEQYXXXXXXXXTT', 'ASSKAASGAYNEQFXXXXXXXTT']\n"
     ]
    }
   ],
   "source": [
    "from src.models import FullFVAE\n",
    "from src.data_processing import encoding_matrix_dict\n",
    "from copy import deepcopy\n",
    "\n",
    "pad_scale = -15\n",
    "new_dict = deepcopy(encoding_matrix_dict['BL50LO'])\n",
    "new_dict['X'] = np.array([pad_scale]).repeat(20)\n",
    "NEW_KEYS = ''.join(new_dict.keys())\n",
    "MATRIX_VALUES = torch.from_numpy(np.stack(list(new_dict.values())))\n",
    "MATRIX_VALUES.shape\n",
    "sample, _  = valid_dataset[:10]\n",
    "# _, _ are mu, logvar\n",
    "x_hat_split, _, _ = model_split_optim(sample)\n",
    "x_hat_concat, _, _ = model_concat_optim(sample)\n",
    "\n",
    "seq_true, v_true, j_true = model_split_optim.slice_x(sample)\n",
    "seq_split, v_split, j_split = model_split_optim.slice_x(x_hat_split)\n",
    "seq_concat, v_concat, j_concat = model_concat_optim.slice_x(x_hat_concat)\n",
    "\n",
    "print('true\\t', model_split_optim.recover_sequences_blosum(seq_true, MATRIX_VALUES, NEW_KEYS)[:3])\n",
    "print('split\\t', model_split_optim.recover_sequences_blosum(seq_split, MATRIX_VALUES, NEW_KEYS)[:3])\n",
    "print('concat\\t', model_split_optim.recover_sequences_blosum(seq_concat, MATRIX_VALUES, NEW_KEYS)[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a12c0a-52bb-482a-8eea-39d61042dd1e",
   "metadata": {},
   "source": [
    "## Inference / clustering / latent visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "id": "7f49c24c-6288-4912-8f26-c9c5c79c1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = valid_dataset.x\n",
    "df_valid = valid_dataset.df\n",
    "labels = df_valid['peptide'].values\n",
    "\n",
    "z_split = model_split_optim.embed(x_valid)\n",
    "z_concat = model_concat_optim.embed(x_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pynn] *",
   "language": "python",
   "name": "conda-env-pynn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
