{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "#Allows relative imports\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#imports from files\n",
    "from src.preprocessing import *\n",
    "from src.VAE_train import *\n",
    "from vae_cel.vae_cel import *\n",
    "from vae_cel.vae_cel_train import *\n",
    "from vae_cel.DeepRC_VAE import *\n",
    "from src.embedding_visualisation import * \n",
    "from src.loss_metrics import *\n",
    "from src.pickling import *\n",
    "from src.datasets import *\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#checking gpu status\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using : {}\".format(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using : {}\".format(device))\n",
    "    \n",
    "#Plot and stuff\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Ignore warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-functionality",
   "metadata": {},
   "source": [
    "#### plotting loss & rechecking VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = load_pkl('../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/VAE_tune-weighted0.5_latent100_Pad-before_Annealing-hyper-gamma7.0_losses.pkl')\n",
    "end = load_pkl('../output/HyperbolicContinueTraining/LOWERBETA_DirectlyMax/VAE_tune-weighted0.5_latent100_Pad-before_Annealing-None-gamma1.0_losses.pkl')\n",
    "start['train'] += end['train']\n",
    "start['val'] += end['val']\n",
    "\n",
    "plt.plot(start['train'], 'r-',lw=.8, label = 'Train losses')\n",
    "plt.plot(start['val'], 'b-',lw=.8, label = 'Val losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.axvline(50, color='k',linestyle = '--', \n",
    "            linewidth=0.5, label = 'Resumed training here')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch_util import *\n",
    "model = VAE_cel(latent_dim = 100, aa_dim = 25)\n",
    "model = load_model(model, '../output/HyperbolicContinueTraining/LOWERBETA_DirectlyMax/BEST_VAE_tune-weighted0.5_latent100_Pad-before_Annealing-None-gamma1.0.pth.tar')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('../training_data_new/mixed_vj_dataset/mixed_vj_test.csv', usecols = ['amino_acid', 'v_family', 'j_family']).query('amino_acid.str.len() <= 23 and amino_acid.str.len() >=10')\n",
    "df = test_model(model, VAELoss_cel(beta = 1), test_dataset.values, 2**14, 23, 0.5, 'before', True, False, 'cuda')\n",
    "display(df)\n",
    "#model with 25% hamming seq loss\n",
    "test_decode(model, test_dataset.values, 10, 23, 0.5, 'before', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in tqdm(list(chunks([x for x in top5_merged.columns if 'z_' in x], 5))):\n",
    "    plot_latent(top5_merged, cols, hue='antigen_epitope')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-factor",
   "metadata": {},
   "source": [
    "## LDA/PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-sword",
   "metadata": {},
   "source": [
    "_see https://www.youtube.com/watch?v=9IDXYHhAfGA,_\n",
    "\n",
    "_https://sebastianraschka.com/Articles/2014_python_lda.html,_\n",
    "\n",
    "_https://arxiv.org/pdf/2101.06772.pdf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-soldier",
   "metadata": {},
   "source": [
    "### class def here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.linear_discriminiants = None\n",
    "        self.eig_vals = None\n",
    "    def fit(self, X,y):\n",
    "        n_features = X.shape[1]\n",
    "        class_labels = np.unique(y)\n",
    "        \n",
    "        #S_W, S_B : \n",
    "        mean_overall = np.mean(X,axis=0) #Shape 100, mean across all samples for each feature\n",
    "        #Initializing empty (square) matrices \n",
    "        S_W = np.zeros((n_features, n_features))\n",
    "        S_B = np.zeros((n_features, n_features))\n",
    "        \n",
    "        \n",
    "        for c in class_labels:\n",
    "            # Within class (S_W)\n",
    "            X_c = X[y==c] #X_c for a given class, shape (N_sample, 100)\n",
    "            mean_c = np.mean(X_c, axis=0)#Only for features in this selected class\n",
    "            #       (100, n_c) * (n_c, 100) = (100, 100)\n",
    "            S_W += (X_c-mean_c).T.dot(X_c-mean_c) #sum over all classes\n",
    "            \n",
    "            # Between class (S_B)\n",
    "            n_c = X_c.shape[0] # = N_samples\n",
    "            mean_diff = (mean_c - mean_overall).reshape(n_features, 1) #reshape to column vector\n",
    "            S_B += n_c * (mean_diff).dot(mean_diff.T)\n",
    "        \n",
    "        A = np.linalg.inv(S_W).dot(S_B) #Solve eigval & eigvec of this\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "        eigenvectors = eigenvectors.T \n",
    "        idxs = np.argsort(abs(eigenvalues))[::-1] #inverse with slicing\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "        #Store top_n in linear_discriminants\n",
    "        self.linear_discriminants = eigenvectors[0:self.n_components]\n",
    "        self.eig_vals = eigenvalues[0:self.n_components]\n",
    "        \n",
    "    def transform(self,X):\n",
    "        #project data\n",
    "        return np.dot(X, self.linear_discriminants.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-cotton",
   "metadata": {},
   "source": [
    "### encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = top5_merged.sort_values('antigen_epitope')\n",
    "features = [x for x in df.columns if 'z_' in x]\n",
    "X = df[features].values\n",
    "y = df['antigen_epitope'].values\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "\n",
    "label_dict = dict((x,y) for x,y in zip(range(len(df.antigen_epitope.unique())),\n",
    "                                       df.antigen_epitope.unique()))\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(5)\n",
    "lda.fit(X,y)\n",
    "X_lda = lda.transform(X)\n",
    "eig_pairs = [(np.abs(lda.eig_vals[i]), lda.linear_discriminants[:,i]) for i in range(len(lda.eig_vals))]\n",
    "print('Variance explained:\\n')\n",
    "eigv_sum = sum([x[0] for x in eig_pairs])\n",
    "for i,j in enumerate(eig_pairs):\n",
    "    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame(data= np.concatenate((X_lda,y.reshape(len(y),1)), axis=1), columns = ['LDA_1','LDA_2','LDA_3','LDA_4','LDA_5', 'class'])\n",
    "lda_df['epitope'] = lda_df['class'].apply(lambda x : label_dict[x])\n",
    "lda_df = lda_df.drop(columns=['class'])\n",
    "lda_df = lda_df.merge(df[['cdr3', 'v_segm', 'j_segm', 'species', 'mhc_a',\n",
    "       'mhc_b', 'mhc_class', 'antigen_epitope', 'antigen_gene',\n",
    "       'antigen_species']], left_index = True, right_index = True)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(10)\n",
    "X_pca = pca.fit_transform(X,)\n",
    "tmp = pd.DataFrame(data=X_pca, columns = ['PCA_'+str(i) for i in range(1,X_pca.shape[1]+1)])\n",
    "merged = lda_df.join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cols = [x for x in lda_df.columns if 'LDA' in x]\n",
    "#sns.histplot(data= lda_df, x = cols, hue # 'antigen_epitope')\n",
    "pca_cols = [x for x in merged.columns if 'PCA' in x]\n",
    "cols = lda_cols + pca_cols\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a = plt.subplots(5,3, figsize=( 18,30))\n",
    "sns.set_palette('gist_ncar')\n",
    "for ax, col in zip(a.ravel(), cols):\n",
    "    sns.histplot(data = merged, x=col, ax=ax, hue = 'antigen_epitope', kde = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-baker",
   "metadata": {},
   "source": [
    "# Top5 nn embedding Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score\n",
    "#Getting top epitopes\n",
    "top5_epitopes = vdjdb.groupby('antigen_epitope').agg(count=('gene','count')).sort_values('count',ascending=False).head().index\n",
    "top5 = vdjdb.query('antigen_epitope in @top5_epitopes and cdr3.str.len() <= 23').reset_index()\n",
    "#Getting the DF features for neural net \n",
    "top5_nn = top5[['cdr3','v','j','mhc_a','mhc_class','antigen_epitope','antigen_gene']]\n",
    "top5_nn['mhc_a_main'] = top5_nn['mhc_a'].apply(lambda x : int(x.split('A*')[1].split(':',1)[0]) if ':' in x else int(x.split('A*')[1]))\n",
    "top_nn_emb = get_embedding_df(model, top5_nn[['cdr3','v','j']].values, device)\n",
    "top5_nn = top5_nn.merge(top_nn_emb[['z_'+str(i) for i in range(100)]], left_index = True, right_index = True).sort_values('antigen_epitope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(top5_nn.head())\n",
    "top5_nn.mhc_a.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "top5_nn['epitope_label'] = enc.fit_transform(top5_nn['antigen_epitope'].values)\n",
    "top5_nn['mhc_a_label'] = enc.fit_transform(top5_nn['mhc_a'].values)\n",
    "top5_nn.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_layers, n_hidden, n_output, \n",
    "                 activation = nn.SELU(), p_drop = 0.5, dropout = True):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        if dropout == True :\n",
    "            self.drop = nn.Dropout(p_drop)\n",
    "        else: \n",
    "            self.drop = nn.Identity()\n",
    "            \n",
    "        self.input_layers = nn.Sequential(nn.Linear(100, 200),\n",
    "                                          nn.BatchNorm1d(200),\n",
    "                                          activation,\n",
    "                                          self.drop,\n",
    "                                          nn.Linear(200,150),\n",
    "                                          nn.BatchNorm1d(150),\n",
    "                                          activation,\n",
    "                                          self.drop,\n",
    "                                          nn.Linear(150,n_hidden),\n",
    "                                          activation,\n",
    "                                          self.drop)\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "            layers.append(activation)\n",
    "            layers.append(self.drop) \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(n_hidden, n_output) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layers(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = F.softmax(x, dim= 1) No need because CEL takes logits and has softmax built-in\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss().__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score\n",
    "\n",
    "def train_model(model, X_train, y_train, batch_size, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for b in BatchSampler(RandomSampler(X_train), batch_size = batch_size,\n",
    "                               drop_last = False):\n",
    "                     #position = 1,\n",
    "                     #leave = False):\n",
    "        values = X_train[b]\n",
    "        target = y_train[b]\n",
    "        score = model(values)\n",
    "        t_loss = criterion(score, target)\n",
    "        model.zero_grad()\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += t_loss.item()   \n",
    "        \n",
    "    train_loss /= math.ceil((len(X_train)/batch_size))\n",
    "    return train_loss\n",
    "\n",
    "def val_model(model, X_val, y_val, batch_size, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for b in BatchSampler(RandomSampler(X_val), batch_size = batch_size,\n",
    "                           drop_last = False):\n",
    "        values = X_val[b]\n",
    "        target = y_val[b]\n",
    "        with torch.no_grad():\n",
    "            score = model(values)\n",
    "        v_loss = criterion(score, target)\n",
    "        val_loss += v_loss.item()\n",
    "        \n",
    "    val_loss /= math.ceil((len(X_val)/batch_size))\n",
    "    return val_loss\n",
    "        \n",
    "def train_clf(model, data:tuple, loss_fct = nn.CrossEntropyLoss, \n",
    "              optimizer_module = torch.optim.AdamW,\n",
    "              lr=5e-5, wd=1e-4, nb_epochs=1000, \n",
    "              batch_size=2**9, loss_weight = None, device='cuda'):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = data\n",
    "    \n",
    "    #X_train.to(device)\n",
    "    #X_val.to(device)\n",
    "    #y_train.to(device)\n",
    "    #y_val.to(device)\n",
    "    #\n",
    "    #model.to(device)\n",
    "    \n",
    "    if loss_weight is not None:\n",
    "        weights = loss_weight\n",
    "    else:\n",
    "        tmp = torch.cat((y_train,y_val), dim=0).max().item()+1\n",
    "        weights = torch.ones((tmp,))\n",
    "    #weights = 1/(top5_nn.groupby('antigen_epitope').agg(count=('cdr3','count')).values/len(top5_nn))\n",
    "    criterion = loss_fct(weight=torch.tensor(weights,device='cuda').float())\n",
    "    optimizer = optimizer_module(model.parameters(), lr = lr, weight_decay= wd)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    rocs = []\n",
    "    accs = []\n",
    "    broken = False\n",
    "    for e in tqdm(range(nb_epochs),\n",
    "                 position = 0, leave = False):\n",
    "        train_loss = train_model(model, X_train, y_train, batch_size, criterion, optimizer)\n",
    "        val_loss = val_model(model, X_val, y_val, batch_size, criterion)\n",
    "        #train\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        mlp.eval()\n",
    "        if val_loss > 1 and e > 100:\n",
    "            broken = True\n",
    "            break\n",
    "        if e%50 == 0 or e==nb_epochs-1:\n",
    "            y_true = y_val.cpu().numpy()\n",
    "            with torch.no_grad():\n",
    "                score = F.softmax(model(X_val), dim =1 ).cpu()\n",
    "                preds = torch.argmax(score,dim=1).numpy()\n",
    "            if y_val.max() >1:\n",
    "                multi = 'ovo'\n",
    "                average = 'macro'\n",
    "            else:\n",
    "                multi = 'ovr'\n",
    "                average = 'macro'\n",
    "                score = score[:,1]\n",
    "            roc_auc = roc_auc_score(y_true, score.numpy(), average = average,\n",
    "                                    multi_class = multi)\n",
    "            rocs.append(roc_auc)\n",
    "            acc = accuracy_score(y_true, preds)\n",
    "            accs.append(acc)\n",
    "            print(f'Epoch:{e};\\tTrain: {train_loss:.3e}\\tVal: {val_loss:.3e}'\\\n",
    "                  f'\\n\\t\\tROC AUC :{roc_auc:.3f}, \\taccuracy: {acc:.3f}')\n",
    "    \n",
    "    losses = {'train': train_losses,\n",
    "              'val' : val_losses,\n",
    "              'roc': rocs,\n",
    "              'acc': accs}\n",
    "    cs = ['b-', 'r-']#, 'g-.', 'm-.']\n",
    "    \n",
    "    for k, c in zip(losses.keys(), cs):\n",
    "        plt.plot(losses[k], color = c[0], ls = c[1], label = k)\n",
    "    \n",
    "    if broken == False:\n",
    "        x = np.arange(start=0,stop=nb_epochs,step=50)\n",
    "        x = np.append(x, nb_epochs-1)\n",
    "        plt.plot(x, rocs, color = 'g', ls = '-.', marker = 'o', markersize = 5, label = 'ROC AUC')\n",
    "        plt.plot(x, accs, color = 'm', ls = '-.', marker = 'x', markersize = 5, label = 'Accuracy')\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-village",
   "metadata": {},
   "source": [
    "### Multi(weighted) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(top5_nn[['z_'+str(i) for i in range(100)]].values, device = 'cuda')\n",
    "y = torch.tensor(top5_nn['epitope_label'].values, device = 'cuda').long()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, train_size = 0.7, shuffle=True)\n",
    "\n",
    "mlp = MLP(10, 300, activation = nn.ReLU(), n_output = len(y.unique()))\n",
    "mlp.to(device);\n",
    "\n",
    "nb_epochs = 1000\n",
    "batch_size = 2**9\n",
    "#total len = 27700\n",
    "weights = 1/(top5_nn.groupby('antigen_epitope').agg(count=('cdr3','count')).values/len(top5_nn))\n",
    "\n",
    "train_clf(model = mlp, data = (X_train,X_val,y_train, y_val), lr = 3e-5, wd = 1e-4,\n",
    "          nb_epochs = nb_epochs, batch_size = batch_size, loss_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    out = F.softmax(mlp(X_val), dim =1).cpu()\n",
    "    preds = torch.argmax(out,dim=1).numpy()\n",
    "print(f'roc_auc : {roc_auc_score(y_val.cpu().numpy(), out.numpy(), multi_class =\"ovo\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_val.cpu().numpy(), preds, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-softball",
   "metadata": {},
   "source": [
    "### OvR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_nn.antigen_epitope.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_nn.groupby('antigen_epitope').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-battlefield",
   "metadata": {},
   "source": [
    "##### Doing for GILGFVFTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ovr_xy_split_class(df, col = 'antigen_epitope', pos_label = 'GILGFVFTL', train_size=.7):\n",
    "    top5_nn['positive_class'] = top5_nn[col].apply(lambda x: 1 if x == pos_label else 0)\n",
    "    X = torch.tensor(top5_nn[['z_'+str(i) for i in range(100)]].values, device = 'cuda')\n",
    "    y = torch.tensor(top5_nn['positive_class'].values, device = 'cuda').long()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X,y, train_size = train_size, shuffle=True)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def get_min(array, x_epochs):\n",
    "    index = np.argmin(array)\n",
    "    value = array[index]\n",
    "    epoch = x_epochs[index]\n",
    "    return epoch, value\n",
    "\n",
    "def get_max(array, x_epochs):\n",
    "    index = np.argmax(array)\n",
    "    value = array[index]\n",
    "    epoch = x_epochs[index]\n",
    "    return epoch, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers, n_hidden, n_output, \n",
    "                 activation = nn.SELU(), p_drop = 0.5)# dropout = True):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        if p_drop >0 :\n",
    "            self.drop = nn.Dropout(p_drop)\n",
    "        else: \n",
    "            self.drop = nn.Identity()\n",
    "            \n",
    "        self.input_layers = nn.Sequential(nn.Linear(100, 200),\n",
    "                                          nn.BatchNorm1d(200),\n",
    "                                          activation,\n",
    "                                          self.drop,\n",
    "                                          nn.Linear(200,150),\n",
    "                                          nn.BatchNorm1d(150),\n",
    "                                          activation,\n",
    "                                          self.drop,\n",
    "                                          nn.Linear(150,n_hidden),\n",
    "                                          activation,\n",
    "                                          self.drop)\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "            layers.append(activation)\n",
    "            layers.append(self.drop)\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layers(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.softmax(x, dim= 1) No need because CEL takes logits and has softmax built-in\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_nn.antigen_epitope.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_ls = {}\n",
    "for epi in tqdm(top5_nn.antigen_epitope.unique()):\n",
    "    X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "    cb = len(top5_nn.query('antigen_epitope==@epi'))/len(top5_nn)\n",
    "    print(f'FOR CURRENT EPITOPE : {epi}, % of positive class = {cb:.2%}')\n",
    "    \n",
    "    mlp = MLP(2,300 , n_output = 2)\n",
    "    mlp.to(device);\n",
    "    losses = train_clf(model = mlp, data = (X_train,X_val,y_train, y_val), lr = 6.67e-5, wd = 1e-2,\n",
    "              nb_epochs = 750, batch_size = 2**10, loss_weight = None)\n",
    "    epi_ls[epi] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "epi_ls = {}\n",
    "for epi in tqdm(top5_nn.antigen_epitope.unique()):\n",
    "    X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "    cb = len(top5_nn.query('antigen_epitope==@epi'))/len(top5_nn)\n",
    "    print(f'FOR CURRENT EPITOPE : {epi}, % of positive class = {cb:.2%}')\n",
    "    \n",
    "    mlp = MLP(4, 50 , n_output = 2)# nobatchnorm, but with ReLU\n",
    "    mlp.to(device);\n",
    "    \n",
    "    losses = train_clf(model = mlp, data = (X_train,X_val,y_train, y_val), lr = 6.67e-5, wd = 1e-2,\n",
    "                  nb_epochs = 750, batch_size = 2**10, loss_weight = None)\n",
    "    epi_ls[epi] = losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-environment",
   "metadata": {},
   "source": [
    "#### gridsearch for MLP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch CNN\n",
    "nb_epochs = 500\n",
    "losses = {}\n",
    "epi = 'GILGFVFTL'\n",
    "x_epochs = np.arange(0, nb_epochs, 50)\n",
    "x_epochs = np.append(x_epochs, nb_epochs-1)\n",
    "X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "X_train.to(device)\n",
    "X_val.to(device)\n",
    "y_train.to(device)\n",
    "y_val.to(device)\n",
    "\n",
    "df_mlp1=pd.DataFrame(columns = ['name','best_roc','roc_epoch','best_val','val_epoch'])\n",
    "ls = {}\n",
    "for criterion in [nn.CrossEntropyLoss]:\n",
    "    for hidden in tqdm([25,50,75, 100],\n",
    "                   leave = False):\n",
    "        for n_layers in tqdm([1,2,3,4],\n",
    "                            leave=False):\n",
    "            for act in tqdm([nn.SELU(), nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.Softmax(dim=1)], \n",
    "                          leave = False):\n",
    "                for p_drop in tqdm([0, 0.5], \n",
    "                                  leave = False):\n",
    "                    for lr in tqdm([1e-5, 1e-4], \n",
    "                                  leave = False):\n",
    "                        for wd in tqdm([1e-2, 5e-7],\n",
    "                                      leave = False):\n",
    "                            name = '_'.join([act.__class__.__name__,\n",
    "                                             criterion().__class__.__name__,\n",
    "                                             'hidden'+str(hidden),\n",
    "                                             'layers'+str(n_layers),\n",
    "                                             'drop'+str(p_drop),\n",
    "                                             'lr'+str(lr),\n",
    "                                             'wd'+str(wd)])\n",
    "                            if criterion().__class__.__name__ == 'BCEWithLogitsLoss':\n",
    "                                outdim = 1\n",
    "                                y_val = y_val.view(-1,1)\n",
    "                                y_train = y_train.view(-1,1)\n",
    "                            else : \n",
    "                                outdim = 2\n",
    "                            mlp = MLP(n_layers, hidden, outdim, act, p_drop)\n",
    "                            print(f'FOR {name}')\n",
    "                            mlp.to(device)\n",
    "                            losses = train_clf(model = mlp, data = (X_train,X_val,y_train, y_val), \n",
    "                                               loss_fct = criterion,\n",
    "                                               lr = lr, wd = wd, nb_epochs = nb_epochs,\n",
    "                                               batch_size = 2**12, loss_weight = None)\n",
    "                            ls[name]= losses\n",
    "                            roc_epoch, best_roc = get_max(losses['roc'], x_epochs)\n",
    "                            val_epoch, best_val = get_min(losses['val'], range(nb_epochs))\n",
    "                            df_mlp1 = df_mlp1.append(pd.DataFrame(data=[[name, best_roc, roc_epoch, best_val, val_epoch]],\n",
    "                                                            columns = ['name','best_roc','roc_epoch','best_val','val_epoch']),\n",
    "                                               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlp1.sort_values('best_roc', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch CNN\n",
    "nb_epochs = 500\n",
    "losses = {}\n",
    "epi = 'GILGFVFTL'\n",
    "x_epochs = np.arange(0, nb_epochs, 50)\n",
    "x_epochs = np.append(x_epochs, nb_epochs-1)\n",
    "X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "X_train.to(device)\n",
    "X_val.to(device)\n",
    "y_train.to(device)\n",
    "y_val.to(device)\n",
    "\n",
    "ls = {}\n",
    "for criterion in [nn.CrossEntropyLoss]:\n",
    "    for hidden in tqdm([150,200,250,300],\n",
    "                   leave = False):\n",
    "        for n_layers in tqdm([2,3,4],\n",
    "                            leave=False):\n",
    "            for act in tqdm([nn.SELU(), nn.ReLU(), nn.Sigmoid()], \n",
    "                          leave = False):\n",
    "                for p_drop in tqdm([0, 0.5], \n",
    "                                  leave = False):\n",
    "                    for lr in tqdm([1e-5, 1e-4], \n",
    "                                  leave = False):\n",
    "                        for wd in tqdm([1e-2, 5e-7],\n",
    "                                      leave = False):\n",
    "                            name = '_'.join([act.__class__.__name__,\n",
    "                                             criterion().__class__.__name__,\n",
    "                                             'hidden'+str(hidden),\n",
    "                                             'layers'+str(n_layers),\n",
    "                                             'drop'+str(p_drop),\n",
    "                                             'lr'+str(lr),\n",
    "                                             'wd'+str(wd)])\n",
    "                            if criterion().__class__.__name__ == 'BCEWithLogitsLoss':\n",
    "                                outdim = 1\n",
    "                                y_val = y_val.view(-1,1)\n",
    "                                y_train = y_train.view(-1,1)\n",
    "                            else : \n",
    "                                outdim = 2\n",
    "                            mlp = MLP(n_layers, hidden, outdim, act, p_drop)\n",
    "                            print(f'FOR {name}')\n",
    "                            mlp.to(device)\n",
    "                            losses = train_clf(model = mlp, data = (X_train,X_val,y_train, y_val), \n",
    "                                               loss_fct = criterion,\n",
    "                                               lr = lr, wd = wd, nb_epochs = nb_epochs,\n",
    "                                               batch_size = 2**12, loss_weight = None)\n",
    "                            ls[name]= losses\n",
    "                            roc_epoch, best_roc = get_max(losses['roc'], x_epochs)\n",
    "                            val_epoch, best_val = get_min(losses['val'], range(nb_epochs))\n",
    "                            df_mlp1 = df_mlp1.append(pd.DataFrame(data=[[name, best_roc, roc_epoch, best_val, val_epoch]],\n",
    "                                                            columns = ['name','best_roc','roc_epoch','best_val','val_epoch']),\n",
    "                                               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, nb_epochs, 50 )\n",
    "x = np.append(x, nb_epochs-1)\n",
    "\n",
    "for k1 in df_mlp1.sort_values('best_roc', ascending = False)['name'].values[0:3]:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    k = list(ls[k1].keys())\n",
    "    plt.plot(ls[k1][k[0]], 'b-.' ,label = k[0])\n",
    "    plt.plot(ls[k1][k[1]], 'r-.' ,label = k[1])\n",
    "    \n",
    "    plt.plot(x, ls[k1][k[2]], c='g', ls = '-', lw = 1, marker = 'o' ,label = k[2])\n",
    "    plt.plot(x, ls[k1][k[3]], c='m', ls = '-', lw = 1, marker = 'o' ,label = k[3])\n",
    "    \n",
    "    plt.title(k1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlp1.sort_values('best_roc', ascending = False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-demographic",
   "metadata": {},
   "source": [
    "### 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OD_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_kernels, act = nn.SELU(), p_drop = .3, final=nn.ReLU()):\n",
    "        super(OD_CNN, self).__init__()\n",
    "        self.n_kernels = n_kernels\n",
    "        if p_drop != 0:\n",
    "            self.drop = nn.Dropout(p_drop)\n",
    "        else:\n",
    "            self.drop = nn.Identity()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, n_kernels, kernel_size = 51)\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(n_kernels, n_kernels*2, kernel_size = 22)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_kernels*4, n_kernels*2)\n",
    "        self.bn1 = nn.BatchNorm1d(n_kernels*2)\n",
    "        self.fc2 = nn.Linear(n_kernels*2, n_kernels)\n",
    "        self.bn2 = nn.BatchNorm1d(n_kernels)\n",
    "        self.fc3 = nn.Linear(n_kernels, 64)\n",
    "        self.output = nn.Linear(64, 2)\n",
    "        self.act = act\n",
    "        self.final = final\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # 1D-conv layers\n",
    "        x = self.act(self.maxpool(self.conv1(x.unsqueeze(1))))\n",
    "        x = self.act(self.maxpool(self.conv2(x)))\n",
    "        # Flattening & FC layers\n",
    "        x = x.view(-1, 4*self.n_kernels)\n",
    "        x = self.bn1(self.drop(self.act(self.fc1(x))))\n",
    "        x = self.bn2(self.drop(self.act(self.fc2(x))))\n",
    "        x = self.act(self.fc3(x))\n",
    "        x = self.final(self.output(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "cnn_losses = {}\n",
    "for epi in tqdm(top5_nn.antigen_epitope.unique()):\n",
    "    X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "    cb = len(top5_nn.query('antigen_epitope==@epi'))/len(top5_nn)\n",
    "    print(f'FOR CURRENT EPITOPE : {epi}, % of positive class = {cb:.2%}')\n",
    "    cnn = OD_CNN(128, act = nn.ReLU(), p_drop=.25, final = nn.ReLU())\n",
    "    cnn.to(device);\n",
    "    \n",
    "    losses = train_clf(model = cnn, data = (X_train,X_val,y_train, y_val), \n",
    "                       lr = 6.67e-5, wd = 1e-3, nb_epochs = 750, \n",
    "                       batch_size = 2**10, loss_weight = None)\n",
    "    cnn_losses[epi] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "cnn_losses = {}\n",
    "for epi in tqdm(top5_nn.antigen_epitope.unique()):\n",
    "    X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "    cb = len(top5_nn.query('antigen_epitope==@epi'))/len(top5_nn)\n",
    "    print(f'FOR CURRENT EPITOPE : {epi}, % of positive class = {cb:.2%}')\n",
    "    cnn = OD_CNN(128, act = nn.SELU(), p_drop=.25, final = nn.Tanh())\n",
    "    cnn.to(device);\n",
    "    \n",
    "    losses = train_clf(model = cnn, data = (X_train,X_val,y_train, y_val), \n",
    "                       lr = 1e-4, wd = 1e-3, nb_epochs = 500,\n",
    "                       batch_size = 2**10, loss_weight = None)\n",
    "    cnn_losses[epi] = losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-berry",
   "metadata": {},
   "source": [
    "##### gridsearch cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-going",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gridsearch CNN\n",
    "nb_epochs = 500\n",
    "losses = {}\n",
    "epi = 'GILGFVFTL'\n",
    "x_epochs = np.arange(0, nb_epochs, 50)\n",
    "x_epochs = np.append(x, nb_epochs-1)\n",
    "X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "X_train.to(device)\n",
    "X_val.to(device)\n",
    "y_train.to(device)\n",
    "y_val.to(device)\n",
    "\n",
    "df=pd.DataFrame(columns = ['name','best_roc','roc_epoch','best_val','val_epoch'])\n",
    "l = {}\n",
    "for nk in tqdm([128, 72],\n",
    "               leave = False):\n",
    "    for act in tqdm([nn.SELU(), nn.ReLU(), nn.Sigmoid(), nn.Tanh(), nn.Softmax(dim=1)], \n",
    "                  leave = False):\n",
    "        for p_drop in tqdm([0, 0.4], \n",
    "                          leave = False):\n",
    "            for lr in tqdm([1e-5], \n",
    "                          leave = False):\n",
    "                for wd in tqdm([1e-3, 5e-7],\n",
    "                              leave = False):\n",
    "                        name = '_'.join([act.__class__.__name__,\n",
    "                                         'nk'+str(nk),\n",
    "                                         'drop'+str(p_drop),\n",
    "                                         'lr'+str(lr),\n",
    "                                         'wd'+str(wd)])\n",
    "                        cnn = OD_CNN(nk, act = act, p_drop=p_drop, final = act)\n",
    "                        print(f'FOR {name}')\n",
    "                        cnn.to(device)\n",
    "                        losses = train_clf(model = cnn, data = (X_train,X_val,y_train, y_val), \n",
    "                                           lr = lr, wd = wd, nb_epochs = nb_epochs,\n",
    "                                           batch_size = 2**10, loss_weight = None)\n",
    "                        l[name]= losses\n",
    "                        roc_epoch, best_roc = get_max(losses['roc'], x_epochs)\n",
    "                        val_epoch, best_val = get_min(losses['val'], range(nb_epochs))\n",
    "                        df = df.append(pd.DataFrame(data=[[name, best_roc, roc_epoch, best_val, val_epoch]],\n",
    "                                                    columns = ['name','best_roc','roc_epoch','best_val','val_epoch']),\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-faith",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values('best_roc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-telephone",
   "metadata": {},
   "source": [
    "### MLP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-actress",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, n_layers, n_hidden, n_output, \n",
    "                 activation = nn.SELU(), p_drop = 0.5): # dropout = True):\n",
    "        super(MLP2, self).__init__()\n",
    "        if p_drop >0 :\n",
    "            self.drop = nn.Dropout(p_drop)\n",
    "        else: \n",
    "            self.drop = nn.Identity()\n",
    "            \n",
    "        self.input_layers = nn.Sequential(nn.Linear(100, 75),\n",
    "                                          activation,\n",
    "                                          nn.BatchNorm1d(75),\n",
    "                                          self.drop,\n",
    "                                          nn.Linear(75,50),\n",
    "                                          activation,\n",
    "                                          nn.BatchNorm1d(50),\n",
    "                                          self.drop,\n",
    "                                          nn.Linear(50, 25),\n",
    "                                          activation,\n",
    "                                          nn.BatchNorm1d(25),\n",
    "                                          self.drop)\n",
    "        # 100 -> 75\n",
    "        # 75 -> 50 \n",
    "        # 50 -> 25\n",
    "        layers = [nn.Linear(25,n_hidden),\n",
    "                  activation,]\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "            layers.append(activation)\n",
    "            layers.append(self.drop)\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layers(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output(x)\n",
    "        #x = nn.Sigmoid(x) ?\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-today",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gridsearch CNN\n",
    "nb_epochs = 500\n",
    "losses = {}\n",
    "epi = 'GILGFVFTL'\n",
    "x_epochs = np.arange(0, nb_epochs, 50)\n",
    "x_epochs = np.append(x, nb_epochs-1)\n",
    "X_train, X_val, y_train, y_val = ovr_xy_split_class(top5_nn, epi, train_size = .667)\n",
    "X_train.to(device)\n",
    "X_val.to(device)\n",
    "y_train.to(device)\n",
    "y_val.to(device)\n",
    "\n",
    "df_mlp2 = pd.DataFrame(columns = ['name','best_roc','roc_epoch','best_val','val_epoch'])\n",
    "ls2 = {}\n",
    "for hidden in tqdm([30,50,70],\n",
    "               leave = False):\n",
    "    for n_layers in tqdm([1,2,3],\n",
    "                        leave=False):\n",
    "        for act in tqdm([nn.SELU(), nn.ReLU(), nn.Sigmoid(), nn.Tanh()], \n",
    "                      leave = False):\n",
    "            for p_drop in tqdm([0, 0.5], \n",
    "                              leave = False):\n",
    "                for lr in tqdm([1e-5,3e-4], \n",
    "                              leave = False):\n",
    "                    for wd in tqdm([1e-2, 5e-7],\n",
    "                                  leave = False):\n",
    "                            \n",
    "                            name = '_'.join([act.__class__.__name__,\n",
    "                                             'hidden'+str(hidden),\n",
    "                                             'layers'+str(n_layers),\n",
    "                                             'drop'+str(p_drop),\n",
    "                                             'lr'+str(lr),\n",
    "                                             'wd'+str(wd)])\n",
    "                            \n",
    "                            mlp = MLP2(n_layers, hidden, 2, act, p_drop)\n",
    "                            print(f'FOR {name}')\n",
    "                            mlp.to(device)\n",
    "                            losses = train_clf(model = mlp, data = (X_train,X_val,y_train, y_val), \n",
    "                                               lr = lr, wd = wd, nb_epochs = nb_epochs,\n",
    "                                               batch_size = 2**12, loss_weight = None)\n",
    "                            ls2[name]= losses\n",
    "                            roc_epoch, best_roc = get_max(losses['roc'], x_epochs)\n",
    "                            val_epoch, best_val = get_min(losses['val'], range(nb_epochs))\n",
    "                            df_mlp2 = df_mlp2.append(pd.DataFrame(data=[[name, best_roc, roc_epoch, best_val, val_epoch]],\n",
    "                                                        columns = ['name','best_roc','roc_epoch','best_val','val_epoch']),\n",
    "                                           ignore_index=True)\n",
    "                            \n",
    "df_mlp2.sort_values('best_roc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../output/mlp_nn/')\n",
    "df_mlp1.to_csv('../output/mlp_nn/df_mlp1.csv', header=True,index=False)\n",
    "df_mlp2.to_csv('../output/mlp_nn/df_mlp2.csv', header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_mlp1.sort_values('best_roc', ascending=False).head(10))\n",
    "display(df_mlp2.sort_values('best_roc', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, nb_epochs, 50 )\n",
    "x = np.append(x, nb_epochs-1)\n",
    "\n",
    "for k1 in df_mlp2.sort_values('best_roc', ascending=False).head()['name'].values:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    k = list(ls2[k1].keys())\n",
    "    plt.plot(ls2[k1][k[0]], 'b-.' ,label = k[0])\n",
    "    plt.plot(ls2[k1][k[1]], 'r-.' ,label = k[1])\n",
    "    \n",
    "    plt.plot(x, ls2[k1][k[2]], c='g', ls = '-', lw = 1, marker = 'o' ,label = k[2])\n",
    "    plt.plot(x, ls2[k1][k[3]], c='m', ls = '-', lw = 1, marker = 'o' ,label = k[3])\n",
    "    plt.legend()\n",
    "    plt.title(k1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
