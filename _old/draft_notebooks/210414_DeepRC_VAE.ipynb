{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "honest-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "#Allows relative imports\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#imports from files\n",
    "from src.preprocessing import *\n",
    "from src.pickling import *\n",
    "from src.torch_util import *\n",
    "from src.repertoire_dataset import EmersonRepertoire_Dataset, load_train_test_repertoire\n",
    "from vae_cel.DeepRC_VAE import *\n",
    "from vae_cel.vae_cel import *\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import BatchSampler, RandomSampler, SequentialSampler\n",
    "#checking gpu status\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using : {}\".format(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using : {}\".format(device))\n",
    "    \n",
    "#Plot and stuff\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "viral-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('../training_data_new/emerson_raw/batch1/emerson_batch1_train_top_20k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "every-objective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here ../training_data_new/emerson_raw/batch1/emerson_batch1_train_top_20k.csv\n",
      "there 23 100\n",
      "Here??\n",
      "there???\n",
      "how about this\n",
      "here ../training_data_new/emerson_raw/batch1/emerson_batch1_test_top_20k.csv\n",
      "there 23 100\n",
      "Here??\n",
      "there???\n",
      "how about this\n",
      "Splitting into train/val\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<src.repertoire_dataset.EmersonRepertoire_Dataset at 0x195f71b08b0>,\n",
       " <src.repertoire_dataset.EmersonRepertoire_Dataset at 0x19569658e80>,\n",
       " <torch.utils.data.dataset.Subset at 0x195695e0160>,\n",
       " <torch.utils.data.dataset.Subset at 0x195695e0490>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_train_test_repertoire(datapath, 23, 100, 'B', 'B07', split_ratio = .7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intended-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here ../training_data_new/emerson_raw/batch1/emerson_batch1_train_top_20k.csv\n",
      "there 23 10000\n",
      "here ../training_data_new/emerson_raw/batch1/emerson_batch1_test_top_20k.csv\n",
      "there 23 10000\n"
     ]
    }
   ],
   "source": [
    "datapath = '../training_data_new/emerson_raw/batch1/'\n",
    "max_len = 23\n",
    "top_k = 10000; allele = 'A'; pos_class = 'A01'; split_ratio = .6\n",
    "train_dataset, test_dataset, train_subset, val_subset = load_train_test_repertoire(path = datapath,\n",
    "                                                                                   max_len = max_len, top_k = top_k, allele = allele, \n",
    "                                                                                   pos_class = pos_class, split_ratio = split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "removed-protein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.randint(0,2,(10,))\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sorted-samuel",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "civic-atlantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2567, -0.2778,  0.1939,  0.0579], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(10,1)\n",
    "x = torch.randn((4,10))\n",
    "lin(x).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>amino_acid</th>\n",
       "      <th>frequency</th>\n",
       "      <th>v_family</th>\n",
       "      <th>j_family</th>\n",
       "      <th>hla_a1</th>\n",
       "      <th>hla_a2</th>\n",
       "      <th>hla_b1</th>\n",
       "      <th>hla_b2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14386</th>\n",
       "      <td>HIP00110.tsv</td>\n",
       "      <td>CASSLTGMNTEAFF</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>A03</td>\n",
       "      <td>A24</td>\n",
       "      <td>B07</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18019</th>\n",
       "      <td>HIP00110.tsv</td>\n",
       "      <td>CASSLENRAAGSAFF</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>A03</td>\n",
       "      <td>A24</td>\n",
       "      <td>B07</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16879</th>\n",
       "      <td>HIP00110.tsv</td>\n",
       "      <td>CASSLVLGINEQFF</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A03</td>\n",
       "      <td>A24</td>\n",
       "      <td>B07</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16856</th>\n",
       "      <td>HIP00110.tsv</td>\n",
       "      <td>CASGRTGLGYEQFF</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>A03</td>\n",
       "      <td>A24</td>\n",
       "      <td>B07</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16855</th>\n",
       "      <td>HIP00110.tsv</td>\n",
       "      <td>CASSNRVGEQFF</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>A03</td>\n",
       "      <td>A24</td>\n",
       "      <td>B07</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11215188</th>\n",
       "      <td>HIP19716.tsv</td>\n",
       "      <td>CASSSWGGPKDEKLFF</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>B08</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11215182</th>\n",
       "      <td>HIP19716.tsv</td>\n",
       "      <td>CASSRLALATYNEQFF</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>B08</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211482</th>\n",
       "      <td>HIP19716.tsv</td>\n",
       "      <td>CASSQGTGTGANVLTF</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A01</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>B08</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211480</th>\n",
       "      <td>HIP19716.tsv</td>\n",
       "      <td>CASSQEPGTGYSPLHF</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>B08</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210147</th>\n",
       "      <td>HIP19716.tsv</td>\n",
       "      <td>CASSLSEILSPLHF</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>B08</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5626686 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename        amino_acid  frequency  v_family  j_family  \\\n",
       "14386     HIP00110.tsv    CASSLTGMNTEAFF   0.000011         4         0   \n",
       "18019     HIP00110.tsv   CASSLENRAAGSAFF   0.000012         4         0   \n",
       "16879     HIP00110.tsv    CASSLVLGINEQFF   0.000012         4         1   \n",
       "16856     HIP00110.tsv    CASGRTGLGYEQFF   0.000012         5         1   \n",
       "16855     HIP00110.tsv      CASSNRVGEQFF   0.000012         9         1   \n",
       "...                ...               ...        ...       ...       ...   \n",
       "11215188  HIP19716.tsv  CASSSWGGPKDEKLFF   0.000023        24         0   \n",
       "11215182  HIP19716.tsv  CASSRLALATYNEQFF   0.000023        13         1   \n",
       "11211482  HIP19716.tsv  CASSQGTGTGANVLTF   0.000340         3         1   \n",
       "11211480  HIP19716.tsv  CASSQEPGTGYSPLHF   0.000338         3         0   \n",
       "11210147  HIP19716.tsv    CASSLSEILSPLHF   0.000015        10         0   \n",
       "\n",
       "         hla_a1   hla_a2 hla_b1   hla_b2  \n",
       "14386       A03      A24    B07  Unknown  \n",
       "18019       A03      A24    B07  Unknown  \n",
       "16879       A03      A24    B07  Unknown  \n",
       "16856       A03      A24    B07  Unknown  \n",
       "16855       A03      A24    B07  Unknown  \n",
       "...         ...      ...    ...      ...  \n",
       "11215188    A01  Unknown    B08  Unknown  \n",
       "11215182    A01  Unknown    B08  Unknown  \n",
       "11211482    A01  Unknown    B08  Unknown  \n",
       "11211480    A01  Unknown    B08  Unknown  \n",
       "11210147    A01  Unknown    B08  Unknown  \n",
       "\n",
       "[5626686 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patient_topk_sequences(x, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "developed-discretion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using : cuda\n",
      "Loading data, this may take a few minutes\n",
      "Loading model\n",
      "Model succesfully loaded from ../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/BEST_VAE_tune-weighted0.5_latent100_Pad-before_Annealing-hyper-gamma7.0.pth.tar:\n",
      "\tepoch: 49\n",
      "Starting training\n",
      "HERE C:\\Users\\richi\\Documents\\EPFL\\Master\\PDM\\code\\TREE\\draft_notebooks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batch:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid Batch:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss at 0 epochs: TRAIN:6.666e-01,\t VAL:6.542e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batch:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid Batch:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss at 1 epochs: TRAIN:6.377e-01,\t VAL:6.223e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batch:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid Batch:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss at 2 epochs: TRAIN:6.115e-01,\t VAL:6.522e-01\n",
      "\n",
      "Time elapsed:\n",
      "\t0.0 minutes\n",
      "\t43.201057 seconds\n"
     ]
    }
   ],
   "source": [
    "from vae_cel.VAE_attention_trainers import *\n",
    "pipeline_attention('../training_data_new/emerson_raw/batch1/',\n",
    "                   10000, 'A', 'A01', .7,\n",
    "         30, 1e-3, 4, name = 'testA01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sixth-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [x for x in os.listdir('../training_data_new/emerson_raw/batch1/') if 'emerson' in x and 'csv' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-mandate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "equipped-motor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emerson_batch1_test', '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0].split('.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "norman-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = '../training_data_new/emerson_raw/batch1/'\n",
    "for x in f :\n",
    "    \n",
    "    tmp = get_patient_topk_sequences(pd.read_csv(dir_+x),\n",
    "                               20000)\n",
    "    fn = x.split('.csv')[0]+'top_20k.csv'\n",
    "    tmp.to_csv(os.path.join(dir_, fn), header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-gothic",
   "metadata": {},
   "source": [
    "### check dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "saved-method",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564, 62, 394, 170)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset, train, val = load_train_test_repertoire(path = '../training_data_new/emerson_raw/batch1/',\n",
    "                                                                     max_len = 23, top_k = 10000, allele = 'A', \n",
    "                                                                     pos_class = 'A01', split_ratio = .7)\n",
    "\n",
    "len(train_dataset), len(test_dataset), len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "systematic-photograph",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.indices[4])\n",
    "train_dataset[460][0] == train[4][0]\n",
    "# getting Dataset[460] is equal to getting train[4] \n",
    "# because train[4] gives the index 460"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-gates",
   "metadata": {},
   "source": [
    "### check empty grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "impossible-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "class combined(nn.Module):\n",
    "    \n",
    "    def __init__(self, model1, model2):\n",
    "        super(combined, self).__init__()\n",
    "        self.mod1 = model1\n",
    "        #self.mod1.requires_grad = False\n",
    "        for p in self.mod1.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.mod2 = model2\n",
    "    def forward(self, x):\n",
    "        x = self.mod1(x)\n",
    "        x = self.mod2(x)\n",
    "        return x \n",
    "mod1 = nn.Sequential(nn.Linear(10,20),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(20,20),\n",
    "                     nn.SELU())\n",
    "\n",
    "mod2 = nn.Sequential(nn.Linear(20,5), \n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(5,2))\n",
    "\n",
    "comb = combined(mod1,mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "saved-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.randn((100,10))\n",
    "yy = torch.randn((100,2))\n",
    "crit = nn.MSELoss()\n",
    "opt = torch.optim.SGD(comb.parameters(), lr = 1e-3)\n",
    "out = comb(xx)\n",
    "\n",
    "loss = crit(out,yy)\n",
    "comb.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "three-tradition",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.2112e-02,  6.6641e-03, -1.4971e-02,  5.8938e-03, -5.8211e-03,\n",
       "          -1.5945e-02, -3.7710e-03, -6.7012e-04,  4.6982e-03, -1.3781e-02,\n",
       "          -2.1570e-02, -5.9170e-03, -1.1678e-02, -2.9458e-03,  4.0377e-03,\n",
       "          -1.5892e-02,  1.3305e-02,  9.4698e-03, -7.4193e-03,  8.1752e-04],\n",
       "         [ 2.4865e-03,  7.5362e-04,  3.5638e-05, -6.5563e-04, -1.3543e-03,\n",
       "           1.6707e-03, -7.4163e-05, -1.9239e-04, -7.9940e-04,  1.8660e-03,\n",
       "           4.2817e-04, -1.0861e-03,  3.0768e-04, -5.2907e-04,  1.0264e-03,\n",
       "           4.8657e-04,  2.2745e-03, -2.9416e-04, -1.6178e-03,  1.5655e-03],\n",
       "         [ 2.7908e-03, -8.1720e-03,  2.9362e-03, -4.1228e-03,  4.1611e-03,\n",
       "           5.5693e-04, -5.8695e-03,  4.3369e-03,  1.2786e-03, -6.4265e-03,\n",
       "           1.8050e-03,  4.2103e-03,  2.9229e-03,  8.9660e-03, -4.9600e-04,\n",
       "           4.6514e-03, -9.5777e-03, -6.9866e-03,  2.7985e-03, -7.0377e-03],\n",
       "         [ 4.2767e-03, -5.3302e-03,  5.9997e-03,  1.6744e-03,  4.2696e-03,\n",
       "          -3.9434e-03, -3.7154e-03,  1.8736e-03, -5.2325e-04, -9.0050e-03,\n",
       "          -1.2926e-03, -3.4486e-04,  2.6989e-04,  6.9047e-03, -3.5873e-03,\n",
       "          -3.9651e-03, -1.2564e-02, -1.3865e-03,  3.6623e-03, -4.1472e-03],\n",
       "         [-2.6166e-03,  9.9038e-04, -3.3002e-03,  2.7343e-05, -4.3598e-04,\n",
       "           1.6391e-04,  1.5299e-03,  2.3390e-03,  1.0725e-03, -3.0053e-03,\n",
       "          -2.2372e-03, -2.2028e-03, -7.9753e-04,  9.4355e-04,  3.3219e-04,\n",
       "          -1.2442e-03,  1.1131e-03,  4.9300e-04, -1.2407e-03, -1.1991e-03]]),\n",
       " None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb.mod2[0].weight.grad, comb.mod1[2].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-butler",
   "metadata": {},
   "source": [
    "### train/val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-scottish",
   "metadata": {},
   "source": [
    "#### VAE as embedder, with frozen VAE layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "governing-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model succesfully loaded from ../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/BEST_VAE_tune-weighted0.5_latent100_Pad-before_Annealing-hyper-gamma7.0.pth.tar:\n",
      "\tepoch: 49\n"
     ]
    }
   ],
   "source": [
    "from src.torch_util import *\n",
    "VAE = VAE_cel(latent_dim = 100, aa_dim = 25)\n",
    "path = '../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/'\n",
    "VAE = load_model(VAE, os.path.join(path, [x for x in os.listdir(path) if 'best' in x.lower()][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "genetic-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataset, train_subset, batch_size, \n",
    "                max_len, weighted, pad, positional, device):\n",
    "    \"\"\"trains for one full epoch (all batches)\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for b in tqdm(BatchSampler(RandomSampler(train_subset), \n",
    "                          batch_size = batch_size, drop_last = False),\n",
    "                  desc = 'Train Batch',\n",
    "                  leave = False, position = 3):\n",
    "        \n",
    "        values, n_per_bag, target = dataset[b]\n",
    "        target = torch.Tensor(target).to(device).long()\n",
    "        x_tuple = batch_aa_vj(values, max_len, weighted, pad,\n",
    "                              positional = True, atchley = False, device = device)\n",
    "        \n",
    "        output = model(x_tuple, n_per_bag)\n",
    "        loss = criterion(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= math.floor(len(train_subset)/ batch_size)\n",
    "    return train_loss\n",
    "\n",
    "def eval_model(model, criterion, dataset, val_subset, batch_size, \n",
    "                max_len, weighted, pad, positional, device):\n",
    "    \"\"\"trains for one full epoch (all batches)\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for b in tqdm(BatchSampler(RandomSampler(val_subset), \n",
    "                          batch_size = batch_size, drop_last = False),\n",
    "                  desc = 'Valid Batch',\n",
    "                  leave = False, position=4):\n",
    "        \n",
    "        values, n_per_bag, target = dataset[b]\n",
    "        target = torch.Tensor(target).to(device).long()\n",
    "        x_tuple = batch_aa_vj(values, max_len, weighted, pad,\n",
    "                              positional = True, atchley = False, device = device)\n",
    "        \n",
    "        output = model(x_tuple, n_per_bag)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= math.floor(len(val_subset)/ batch_size)\n",
    "    return val_loss\n",
    "\n",
    "def train_attention(model, nb_epochs, lr, batch_size,\n",
    "                    train_dataset, train_subset, val_subset, \n",
    "                    max_len=23, weighted=0.5, pad = 'before',\n",
    "                    positional = True, device = 'cuda', name = ''):\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = lr())\n",
    "    train_losses = []\n",
    "    val_losses = [] \n",
    "    best_val = 100\n",
    "    \n",
    "    model.to(device) \n",
    "    \n",
    "    for e in tqdm(range(nb_epochs),\n",
    "                  position=0, leave = False):\n",
    "        train_loss = train_model(model,criterion, optimizer, train_dataset, train_subset,\n",
    "                                 batch_size, max_len, weighted, pad, \n",
    "                                 positional, device)\n",
    "        \n",
    "        val_loss = eval_model(model, criterion, train_dataset, val_subset, batch_size, \n",
    "                              max_len, weighted, pad, positional, device)\n",
    "        \n",
    "        if e != 0 and val < best_val:\n",
    "            torch.save({'state_dict':model.state_dict(), 'epoch':e, 'loss':val_loss}, \n",
    "                           f'./VAE_attention_{name}.pth.tar')\n",
    "            best_val = val\n",
    "            \n",
    "        val_losses.append(val_losses)\n",
    "        train_losses.append(train_losses)\n",
    "        tqdm.write(f' loss at {e} epochs: TRAIN:{train_loss},\\t VAL:{val_loss}')\n",
    "        \n",
    "    losses = {'train': train_losses,\n",
    "              'val' : val_losses}\n",
    "    return losses\n",
    "\n",
    "def pipeline_attention(datapath, top_k, allele, pos_class, split_ratio, nb_epochs, lr, batch_size,\n",
    "             max_len=23, weighted=0.5, pad = 'before',\n",
    "             positional = True, device = 'cuda', name = ''):\n",
    "    \n",
    "    start_time = dt.now()\n",
    "    print('Loading data, this may take a few minutes')\n",
    "    train_dataset, test_dataset, train_subset, val_subset = load_train_test_repertoire(path = datapath,\n",
    "                                                                                   max_len = max_len, top_k = top_k, allele = allele, \n",
    "                                                                                   pos_class = pos_class, split_ratio = split_ratio)\n",
    "    #fixed model for now\n",
    "    VAE = VAE_cel(latent_dim = 100, aa_dim = 25)\n",
    "    path = '../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/'\n",
    "    VAE = load_model(VAE, os.path.join(path, [x for x in os.listdir(path) if 'best' in x.lower()][0]))\n",
    "    attention = AttentionNetwork(n_input_features = 100, n_layers = 3, n_units = 50)\n",
    "    output_net = OutputNetwork(n_input_features = 100, n_output_features = 2,\n",
    "                               n_layers = 2, n_units = 50)\n",
    "    \n",
    "    model = DeepRC_VAE(VAE,\n",
    "                       attention,\n",
    "                       output_net)\n",
    "    \n",
    "    losses = train_attention(model, nb_epochs, lr, batch_size,\n",
    "                    train_dataset, train_subset, val_subset, \n",
    "                    max_len, weighted, pad,\n",
    "                    positional, device, name)\n",
    "    \n",
    "    end_time = dt.now()       \n",
    "    elapsed = divmod((end_time-start_time).total_seconds(), 60)\n",
    "    print(f\"\\nTime elapsed:\\n\\t{elapsed[0]} minutes\\n\\t{elapsed[1]} seconds\")\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "insured-sheffield",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3533db050385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pipeline_attention('../training_data_new/emerson_raw/batch1/', 10000, 'A', 'A01', .7,\n\u001b[0m\u001b[0;32m      2\u001b[0m          30, 1e-3, 4, name = 'testA01')\n",
      "\u001b[1;32m<ipython-input-21-57c71b5ea7c9>\u001b[0m in \u001b[0;36mpipeline_attention\u001b[1;34m(datapath, top_k, allele, pos_class, split_ratio, nb_epochs, lr, batch_size, max_len, weighted, pad, positional, device, name)\u001b[0m\n\u001b[0;32m     84\u001b[0m              positional = True, device = 'cuda', name = ''):\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     train_dataset, test_dataset, train_subset, val_subset = load_train_test_repertoire(path = datapath,\n\u001b[0m\u001b[0;32m     87\u001b[0m                                                                                    \u001b[0mmax_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallele\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallele\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                                                                                    pos_class = pos_class, split_ratio = split_ratio)\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\PDM\\code\\TREE\\src\\repertoire_dataset.py\u001b[0m in \u001b[0;36mload_train_test_repertoire\u001b[1;34m(path, max_len, top_k, allele, pos_class, split_ratio)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mthen\u001b[0m \u001b[0msplits\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0minto\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmersonRepertoire_Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallele\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmersonRepertoire_Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallele\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;31m#Splitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Master\\PDM\\code\\TREE\\src\\repertoire_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, max_len, istrain, top_k, allele, pos_class)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m#Reading the top_K most frequent (grouped by patient) values from DF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         df = get_patient_topk_sequences(pd.read_csv(os.path.join(path, fn))\\\n\u001b[0m\u001b[0;32m     69\u001b[0m                                           \u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'amino_acid.str.len() <= @max_len'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                                         top_k)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdm\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdm\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdm\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdm\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2054\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pdm\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m     \"\"\"\n\u001b[0;32m    539\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline_attention('../training_data_new/emerson_raw/batch1/', 10000, 'A', 'A01', .7,\n",
    "         30, 1e-3, 4, name = 'testA01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "apparent-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, train_subset, val_subset = load_train_test_repertoire(path = '../training_data_new/emerson_raw/batch1/',\n",
    "                                                                                   max_len = 23, top_k = 10000, allele = 'A', \n",
    "                                                                                   pos_class = 'A01', split_ratio = .7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extended-surprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model succesfully loaded from ../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/BEST_VAE_tune-weighted0.5_latent100_Pad-before_Annealing-hyper-gamma7.0.pth.tar:\n",
      "\tepoch: 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef39a9d79814851b94c380288d2cae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batch:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid Batch:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5807a123596d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf' loss at {e} epochs: TRAIN:{train_loss},\\t VAL:{val}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m losses = {'train':losses,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "VAE = VAE_cel(latent_dim = 100, aa_dim = 25)\n",
    "path = '../output/HyperbolicTuning/LOWERBETA_gamma7_wd1e-3_ad3097/'\n",
    "VAE = load_model(VAE, os.path.join(path, [x for x in os.listdir(path) if 'best' in x.lower()][0]))\n",
    "attention = AttentionNetwork(n_input_features = 100, n_layers = 3, n_units = 50)\n",
    "output_net = OutputNetwork(n_input_features = 100, n_output_features = 2,\n",
    "                           n_layers = 3, n_units = 50)\n",
    "\n",
    "model = DeepRC_VAE(VAE,\n",
    "                   attention,\n",
    "                   output_net)\n",
    "nb_epochs = 30\n",
    "batch_size = 4 \n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = lr)\n",
    "train_losses = []\n",
    "val_losses = [] \n",
    "best_val = 100\n",
    "max_len = 23\n",
    "weighted = 0.5\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "for e in tqdm(range(nb_epochs),position=0, leave = False):\n",
    "    train_loss = train_model(model,criterion, optimizer, train_dataset, train_subset,\n",
    "                             batch_size, max_len, weighted, pad='before', \n",
    "                             positional = True, device='cuda')\n",
    "    \n",
    "    val_loss = eval_model(model, criterion, train_dataset, val_subset, batch_size, \n",
    "                          max_len, weighted, 'before', True, device)\n",
    "    \n",
    "    if e != 0 and val < best_val:\n",
    "        torch.save({'state_dict':model.state_dict(), 'epoch':e, 'loss':val_loss}, \n",
    "                       './deepRC_VAE_A01.pth.tar')\n",
    "        best_val = val\n",
    "        \n",
    "    val_losses.append(val_losses)\n",
    "    train_losses.append(train_losses)\n",
    "    tqdm.write(f' loss at {e} epochs: TRAIN:{train_loss},\\t VAL:{val_loss}')\n",
    "    \n",
    "losses = {'train': train_losses,\n",
    "          'val' : val_losses}\n",
    "#save_pkl('./withVAE_frozen_losses.pkl', losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
